{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f84edb5e70e5c8cbd2d3004c9b67ceaf",
     "grade": false,
     "grade_id": "cell-9e78272cc4af091a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4\n",
    "***\n",
    "*General hints:* <br>\n",
    "* You may use another notebook to test different approaches and ideas. When complete and mature, turn your code snippets into the requested functions in this notebook for submission. \n",
    "* Make sure the function implementations are generic and can be applied to any dataset (not just the one provided).\n",
    "* Add explanatory code comments in the code cells. Make sure that these comments improve our understanding of your implementation decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca384ab2b432240f2e75945bb1878249",
     "grade": false,
     "grade_id": "cell-9d52cb434e7f0910",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "* Create a variable holding your student id, as shown below. \n",
    "* Simply replace the example (`01234567`) with your actual student id having a total of 8 digits. \n",
    "* Maintain the variable as a string, do NOT change its type in this notebook!\n",
    "* *Note: If your student id has 7 digits, add a leading 0. The final student id MUST have 8 digits!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = '12318768'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95db3692d001bdd0ce4a073305285b20",
     "grade": false,
     "grade_id": "cell-98bb586658e7b54d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dce49140be8025de8894a25a2b824268",
     "grade": false,
     "grade_id": "cell-420b4c449379ffe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 0. Import\n",
    "\n",
    "Implement a function `tidy` which imports the data set assigned and provided to you as a CSV file into a `pandas` dataframe. Access the data set and establish whether your data set is tidy. If not, clean the data set before continuing with Step 1. Mind all rules of tidying data sets in this step. Make sure you comply to the following statements:\n",
    "* If there is an index column (row numbers) in your tidied dataset, keep it.\n",
    "* The following columns, once identified, correspond to variables 1:1 (no need for transformations):\n",
    "  * `full_name`\n",
    "  * `automotive`\n",
    "  * `color`\n",
    "  * `job`\n",
    "  * `address`\n",
    "  * `coordinates`\n",
    "  * `km_per_litre`\n",
    "* The tidied dataset should have a total of 9 columns (not including the index), the first column should be `full_name` and the last one `km_per_litre`.\n",
    "* Mind the intended content of each attribute (e.g. `full_name` should contain the full name of a person, no need to change that)\n",
    "* If tidy or done, have the function `tidy` return the ready data set as a dataframe.\n",
    "\n",
    "Note that `tidy` must take a single parameter that holds your student id (`mn`) as one part of the basename (according to the CoC) of the CSV file (i.e., the CoC file name without file extension). Change the name of the data file so that it matches this requirement and the CoC and make sure you submit your final ZIP following the Code of Conduct (CoC) requirements. Especially, make sure you put your data file in a folder called `data/` when submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e96fea9cc9f84652b592c3659339a48b",
     "grade": false,
     "grade_id": "cell-81e21dccd785e63d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported tidied data to 'tidied_data_12318768.csv'\n",
      "T0.1 Passed: Result is a DataFrame.\n",
      "T0.2 Passed: Correct number of columns.\n",
      "T0.3 Passed: First column is 'full_name'.\n",
      "T0.4 Passed: Last column is 'km_per_litre'.\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import re # Not needed\n",
    "# import os # Not needed\n",
    "\n",
    "# Make sure 'mn' is defined before calling tidy\n",
    "mn = '12318768'\n",
    "\n",
    "def tidy(x):\n",
    "    \"\"\"\n",
    "    Imports and tidies the dataset from a CSV file using only pandas and numpy.\n",
    "\n",
    "    Args:\n",
    "        x (str): The student ID, used to construct the filename (e.g., 'data/12318768.csv').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The tidied dataframe, or None if the file cannot be read.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Define potential file paths using string formatting ---\n",
    "    filepath_data = f'data/{x}.csv'\n",
    "    filepath_current = f'{x}.csv'\n",
    "    df = None # Initialize df to None\n",
    "\n",
    "    # --- Step 2: Try reading the file from specified paths ---\n",
    "    try:\n",
    "        df = pd.read_csv(filepath_data, header=None)\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath_current, header=None)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Data file not found at {filepath_data} or {filepath_current}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "             print(f\"Error reading {filepath_current}: {e}\")\n",
    "             return None\n",
    "    except Exception as e:\n",
    "         print(f\"Error reading {filepath_data}: {e}\")\n",
    "         return None\n",
    "\n",
    "    # --- Step 3: Proceed with tidying ONLY if df was loaded successfully ---\n",
    "    if df is None:\n",
    "        print(\"Error: DataFrame was not loaded.\")\n",
    "        return None\n",
    "\n",
    "    # Set the first column (variable names) as the index\n",
    "    try:\n",
    "        df = df.set_index(0)\n",
    "    except KeyError:\n",
    "        print(\"Error: Cannot set index. Column 0 might be missing or dataframe structure is unexpected.\")\n",
    "        return None\n",
    "\n",
    "    # Transpose the dataframe\n",
    "    df = df.T\n",
    "\n",
    "    # Clean index and column names\n",
    "    df.index.name = None\n",
    "    df.columns.name = None\n",
    "\n",
    "    # Define and replace non-standard missing values\n",
    "    na_list = ['NaN', 'nan', 'NA', 'N/A', 'n/a', '--', '-inf', 'inf', 'None', '']\n",
    "    df = df.replace(na_list, np.nan)\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True) # Handle whitespace-only strings\n",
    "\n",
    "    # Split the combined 'date_time/full_company_name' column using string slicing\n",
    "    combined_col = 'date_time/full_company_name'\n",
    "    datetime_len = 26 # Length of 'YYYY-MM-DD HH:MM:SS.ffffff'\n",
    "\n",
    "    if combined_col in df.columns:\n",
    "        df[combined_col] = df[combined_col].astype(str)\n",
    "        df.loc[:, 'date_time'] = df[combined_col].str[:datetime_len]\n",
    "        df.loc[:, 'date_time'] = pd.to_datetime(df['date_time'], errors='coerce')\n",
    "        df.loc[:, 'full_company_name'] = df[combined_col].str[datetime_len:].str.strip()\n",
    "        df.loc[df['full_company_name'] == '', 'full_company_name'] = np.nan\n",
    "        df = df.drop(columns=[combined_col])\n",
    "    else:\n",
    "        if 'date_time' not in df.columns: df['date_time'] = pd.NaT\n",
    "        if 'full_company_name' not in df.columns: df['full_company_name'] = np.nan\n",
    "        print(f\"Warning: Column '{combined_col}' not found for splitting.\")\n",
    "\n",
    "    # Convert 'km_per_litre' to numeric\n",
    "    if 'km_per_litre' in df.columns:\n",
    "        df.loc[:, 'km_per_litre'] = pd.to_numeric(df['km_per_litre'], errors='coerce')\n",
    "    else:\n",
    "        df['km_per_litre'] = np.nan\n",
    "        print(\"Warning: Column 'km_per_litre' not found.\")\n",
    "\n",
    "    # Ensure all required columns exist and set the final order\n",
    "    final_cols_order = ['full_name', 'automotive', 'color', 'job', 'address', 'coordinates', 'date_time', 'full_company_name', 'km_per_litre']\n",
    "    for col in final_cols_order:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Expected column '{col}' missing. Adding as NaN/NaT.\")\n",
    "            if col == 'date_time':\n",
    "                df[col] = pd.NaT\n",
    "            else:\n",
    "                df[col] = np.nan\n",
    "\n",
    "    # Reorder columns\n",
    "    try:\n",
    "        df = df[final_cols_order]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error reordering columns. Missing expected columns: {e}\")\n",
    "        available_cols = [col for col in final_cols_order if col in df.columns]\n",
    "        df = df[available_cols]\n",
    "\n",
    "    # Reset index to standard integer index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Call tidy function ---\n",
    "tidied_df = tidy(mn)\n",
    "\n",
    "# --- Export the tidied DataFrame to CSV ---\n",
    "if tidied_df is not None:\n",
    "    output_filename = f\"tidied_data_{mn}.csv\"\n",
    "    try:\n",
    "        # index=False prevents pandas from writing the DataFrame index as a column\n",
    "        tidied_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Successfully exported tidied data to '{output_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting DataFrame to CSV: {e}\")\n",
    "\n",
    "    # --- Run Assertions (as before) ---\n",
    "    assert type(tidied_df) == pd.core.frame.DataFrame, \"T0.1 Failed: Result is not a DataFrame\"\n",
    "    print(\"T0.1 Passed: Result is a DataFrame.\")\n",
    "    assert len(tidied_df.columns) == 9, f\"T0.2 Failed: Expected 9 columns, got {len(tidied_df.columns)}\"\n",
    "    print(\"T0.2 Passed: Correct number of columns.\")\n",
    "    assert list(tidied_df.columns)[0] == \"full_name\", f\"T0.3 Failed: First column is {list(tidied_df.columns)[0]}, expected 'full_name'\"\n",
    "    print(\"T0.3 Passed: First column is 'full_name'.\")\n",
    "    assert list(tidied_df.columns)[-1] == \"km_per_litre\", f\"T0.4 Failed: Last column is {list(tidied_df.columns)[-1]}, expected 'km_per_litre'\"\n",
    "    print(\"T0.4 Passed: Last column is 'km_per_litre'.\")\n",
    "else:\n",
    "    print(\"Tidying failed, cannot run assertions or export.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be6c2c8b745391ec6bf4e4482a6fa632",
     "grade": false,
     "grade_id": "cell-d538705e1bdde279",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(tidy(mn)) == pd.core.frame.DataFrame, \"T0.1\"\n",
    "assert len((tidy(mn)).columns) == 9, \"T0.2\"\n",
    "assert list((tidy(mn)).columns)[0] == \"full_name\", \"T0.3\"\n",
    "assert list((tidy(mn)).columns)[len((tidy(mn)).columns)-1] == \"km_per_litre\", \"T0.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb930bb6bf9a82301ea0e43b2e24ad91",
     "grade": true,
     "grade_id": "cell-9a75a2763c7bbe5d",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! üòº‚ö°Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1a8d139471d249eef3a881ab9502e3d",
     "grade": false,
     "grade_id": "cell-72c2573051ab8535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-------\n",
    "## 1. Missing values\n",
    "\n",
    "### 1.1 Code part\n",
    "Implement a function called `missing_values` which takes as an input a dataframe and check if there are any missing values in the dataset. Record the row positions (*not* the row labels!) of the observations containing missing values as a list of numbers and make sure that the function returns the recorded list in the end, sorted in ascending order. If there are no missing values, `missing_values` should return an empty list.\n",
    "\n",
    "**NOTE:** You shall find out how missing values are encoded in your datasest and which missing values occur in your dataset, you will ***need manual inspection*** by applying Python helpers. For instance, missing values could be encoded as: `\"nan\"`,`\"(+/-)inf\"` but also other values or empty fields or fields containing only white spaces are conceivable to encode missing values in your dataset. Do *not* rely on built-in Python or pandas functions alone!\n",
    "\n",
    "Important: Mind the difference between row positions and row labels. `.index` of a dataframe returns row labels. `.iloc` takes row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c924e80de70e8169cc282686502fb094",
     "grade": false,
     "grade_id": "cell-6d4fb7f2e44e7cfb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row positions with missing values: [57, 110, 131, 159, 165, 201, 286, 312, 328, 368, 404, 453, 543, 571, 594, 607, 648, 653, 673, 698]... (showing first 20)\n",
      "Total number of rows with missing values: 45\n",
      "T1.1 Passed: Result is a list.\n",
      "T1.2 Passed: List contains only integers.\n",
      "T1.3 Passed: List is sorted.\n"
     ]
    }
   ],
   "source": [
    "def missing_values(x):\n",
    "    \"\"\"\n",
    "    Finds the row positions containing missing values (NaN or NaT) in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        x (pandas.DataFrame): The input dataframe (assumed to be tidied).\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of integer row positions containing missing values.\n",
    "              Returns an empty list if no missing values are found or input is invalid.\n",
    "    \"\"\"\n",
    "    # Check if the input is a DataFrame\n",
    "    if not isinstance(x, pd.DataFrame):\n",
    "        print(\"Error: Input must be a pandas DataFrame.\")\n",
    "        return []\n",
    "\n",
    "    # Identify rows containing any NaN/NaT values across columns\n",
    "    # .isnull() detects both np.nan and pd.NaT\n",
    "    rows_with_nan_mask = x.isnull().any(axis=1)\n",
    "\n",
    "    # Get the index labels of these rows. Since tidy() ends with reset_index(drop=True),\n",
    "    # the index labels directly correspond to the 0-based row positions.\n",
    "    nan_index_labels = x[rows_with_nan_mask].index\n",
    "\n",
    "    # Convert the index labels (positions) to a list\n",
    "    nan_positions = nan_index_labels.tolist()\n",
    "\n",
    "    # Sort the positions in ascending order (although RangeIndex is usually sorted)\n",
    "    nan_positions.sort()\n",
    "\n",
    "    return nan_positions\n",
    "\n",
    "# --- Example Call and Assertion ---\n",
    "# Ensure the necessary imports (pandas, numpy) and the tidy function are defined\n",
    "# and the tidied_df is created.\n",
    "\n",
    "# Create tidied_df first\n",
    "tidied_df = tidy(mn)\n",
    "\n",
    "if tidied_df is not None:\n",
    "    mv_indices = missing_values(tidied_df)\n",
    "    print(f\"Row positions with missing values: {mv_indices[:20]}... (showing first 20)\") # Show only first few for brevity\n",
    "    print(f\"Total number of rows with missing values: {len(mv_indices)}\")\n",
    "\n",
    "    # Run Assertions\n",
    "    assert type(mv_indices) == list, \"T1.1 Failed: Result is not a list\"\n",
    "    print(\"T1.1 Passed: Result is a list.\")\n",
    "    assert all(isinstance(i, int) for i in mv_indices), \"T1.2 Failed: List does not contain only integers\"\n",
    "    print(\"T1.2 Passed: List contains only integers.\")\n",
    "    # Additional check for sorting\n",
    "    assert all(mv_indices[i] <= mv_indices[i+1] for i in range(len(mv_indices)-1)), \"T1.3 Failed: List is not sorted\"\n",
    "    print(\"T1.3 Passed: List is sorted.\")\n",
    "else:\n",
    "    print(\"Cannot run missing_values tests because tidying failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25ac8efba1efdd23e9985baa4cf78d40",
     "grade": false,
     "grade_id": "cell-49e8fe2aeb51324f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(missing_values(tidy(mn))) == list, \"T1.1\"\n",
    "assert all(isinstance(i, int) for i in missing_values(tidy(mn))), \"T1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce5f9cfea1f57588c27bc8d2a33bc37e",
     "grade": true,
     "grade_id": "cell-4c692eab5b638fc7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! üòº‚ö°Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1835150c80672823f016567532fe9856",
     "grade": false,
     "grade_id": "cell-82f316abfa9423e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2. Analytical part\n",
    "\n",
    "* Does the dataset contain missing values?\n",
    "* Explain your manual-inspection procedure and the Python helpers used!\n",
    "* If no, explain how you proved that this is actually the case. \n",
    "* If yes, describe the discovered missing values. What could be an explanation for their missingness?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "229a22e8b0e33f07eb7e3e7a6a652bf2",
     "grade": true,
     "grade_id": "cell-3ad38c6f2d1998f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "919ec0c1fbf3d46cfeacebf3789dc7b3",
     "grade": false,
     "grade_id": "cell-87126fc406d941ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## 2. Handling missing values\n",
    "### 2.1. Code part\n",
    "Apply a (simple) function called *handling_missing_values* for handling missing values using an adequate single-imputation technique (or, one of the alternatives to single imputation) of your choice per type of missing values. Make use of the techniques learned in Unit 4. The function should take as an input a dataframe and return the updated dataframe. Mind the following:\n",
    "- The objective is to apply single imputation on these synthetic data. Do not make up a background story (at this point)!\n",
    "- Do NOT simply drop the missing values. This is not an option.\n",
    "- The imputation technique must be adequate for a given variable type (quantitative, qualitative).\n",
    "- To establish whether a variable is quantitative or qualitative, it is *not* sufficient to only inspect on data types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0226044c2b66de952ca20bc64edae12b",
     "grade": false,
     "grade_id": "cell-5ba2dc8b5cbe1f8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def handling_missing_values(x):\n",
    "    \"\"\"\n",
    "    Handles missing values in the dataframe using single imputation.\n",
    "    Uses median for quantitative columns ('km_per_litre') and mode for qualitative columns.\n",
    "\n",
    "    Args:\n",
    "        x (pandas.DataFrame): The input dataframe (assumed tidied), potentially containing missing values.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The dataframe with missing values imputed.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, pd.DataFrame):\n",
    "        print(\"Input is not a pandas DataFrame.\")\n",
    "        return x # Return original input\n",
    "    if x.empty:\n",
    "       return x # Return empty DataFrame if input is empty\n",
    "\n",
    "    df = x.copy() # Work on a copy\n",
    "\n",
    "    # 1. Ensure missing values identified in Step 1 are represented as NaN\n",
    "    #    (Re-apply detection logic here to be self-contained, though it assumes Step 1 identified them correctly)\n",
    "    na_strings = [\n",
    "        'nan', 'NaN', 'NA', 'N/A', '#N/A', 'null', 'Null', '', 'none', 'None',\n",
    "        'missing', 'Missing', ' ', '?', '-', '--',\n",
    "        'inf', '-inf', 'Infinity', '-Infinity'\n",
    "    ]\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "             try:\n",
    "                is_string = df[col].apply(lambda item: isinstance(item, str))\n",
    "                df.loc[is_string, col] = df.loc[is_string, col].str.strip()\n",
    "                df[col].replace(na_strings, np.nan, inplace=True)\n",
    "             except AttributeError:\n",
    "                 # This column might contain non-strings even if dtype is object\n",
    "                 pass # Ignore if strip fails for non-strings\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        # Add specific placeholder replacements if identified in Step 1 (e.g., 0 for km_per_litre)\n",
    "        # if col == 'km_per_litre' and pd.api.types.is_numeric_dtype(df[col]):\n",
    "        #     df[col].replace(0, np.nan, inplace=True) # Be cautious if 0 is valid\n",
    "\n",
    "    # 2. Identify quantitative vs. qualitative columns from the 9 tidied columns\n",
    "    # 'km_per_litre' is quantitative.\n",
    "    # 'Id' is numeric but acts as an identifier; treating as qualitative for mode imputation is safer than median.\n",
    "    # Others are qualitative or complex strings/coordinates.\n",
    "    quantitative_cols = ['km_per_litre']\n",
    "    qualitative_cols = [\n",
    "        'full_name', 'Id', 'automotive', 'color', 'job',\n",
    "        'address', 'coordinates', 'datetime_company_details'\n",
    "    ]\n",
    "\n",
    "    # Ensure all columns are classified\n",
    "    all_cols = quantitative_cols + qualitative_cols\n",
    "    if len(all_cols) != len(df.columns) or set(all_cols) != set(df.columns):\n",
    "         print(\"Warning: Column classification doesn't match DataFrame columns. Check lists.\")\n",
    "         # Fallback or error handling could be added here\n",
    "\n",
    "    # 3. Impute missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any(): # Only impute columns with missing values\n",
    "            if col in quantitative_cols:\n",
    "                # Impute quantitative with median (robust to outliers)\n",
    "                # Ensure column is numeric for median calculation\n",
    "                numeric_col = pd.to_numeric(df[col], errors='coerce')\n",
    "                if numeric_col.isna().all(): # Handle case where column becomes all NaN\n",
    "                    median_val = 0 # Or some other default, maybe np.nan still? Use 0 for example.\n",
    "                    print(f\"Warning: Column '{col}' is all NaN after coercion. Filling with {median_val}.\")\n",
    "                else:\n",
    "                    median_val = numeric_col.median()\n",
    "\n",
    "                # Fill NaN in the original DataFrame column\n",
    "                df[col].fillna(median_val, inplace=True)\n",
    "                print(f\"Imputed {col} (quantitative) with median: {median_val}\")\n",
    "\n",
    "\n",
    "            elif col in qualitative_cols:\n",
    "                # Impute qualitative with mode\n",
    "                # Mode might return multiple values; use the first one ([0])\n",
    "                mode_val = df[col].mode()\n",
    "                if not mode_val.empty:\n",
    "                    fill_value = mode_val[0]\n",
    "                    df[col].fillna(fill_value, inplace=True)\n",
    "                    print(f\"Imputed {col} (qualitative) with mode: {fill_value}\")\n",
    "                else:\n",
    "                    # Handle cases where the column is entirely NaN or mode fails\n",
    "                    fill_value = \"Unknown\" # Or another suitable placeholder\n",
    "                    df[col].fillna(fill_value, inplace=True)\n",
    "                    print(f\"Imputed {col} (qualitative) with placeholder: {fill_value} (mode empty)\")\n",
    "            else:\n",
    "                # This case should not happen if lists cover all columns\n",
    "                 print(f\"Warning: Column '{col}' was not classified for imputation.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f4b2e3d0f0820ea95c3c5ffb9efe951",
     "grade": true,
     "grade_id": "cell-0edce052e98e2e95",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(missing_values(handling_missing_values(tidy(mn)))) == 0, \"T2.1\"\n",
    "assert handling_missing_values(tidy(mn)).shape == tidy(mn).shape, \"T2.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "379b1e101131334e3262e96f723fe8e6",
     "grade": false,
     "grade_id": "cell-c697641b9d5a1c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2. Analytical part\n",
    "Discuss the implications. Answer the following:\n",
    "\n",
    "- How would you qualify the data-generating processes leading to different types of missing values, provided that the data was not synthetic?\n",
    "- What are the benefits and disadvantages of the chosen single-imputation technique?\n",
    "- How would you apply a multiple-imputation technique to one type of missing values, if applicable at all?\n",
    "- We asked you to test for/treat as missing values by checking certain field values, as well as empty fields or fields containing the numeric value 0... what are potential problems of this heuristics?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34f9ca681c722ebe151d6fac42d71b25",
     "grade": true,
     "grade_id": "cell-5c05456587f2ff17",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21398d4af7c97ce06acb2f77675ce4d8",
     "grade": false,
     "grade_id": "cell-573d56d6699b84eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 3. Detecting duplicate entries\n",
    "Implement a function called `duplicates` that takes as an input a (tidy) dataframe `x` and a list of column labels (`VARIABLES`). Assume that `duplicates` receives a dataframe as returned from your Step 0 implementation of `tidy`. It then checks whether there are any duplicates in the dataset. Record the row positions of the second and any later observations being duplicates and have `duplicates` return the list of rows positions, sorted in asending order, in the end. An empty list indicates the absence of duplicated observations.\n",
    "\n",
    "Important:\n",
    "* The first observation that belongs to the detected duplicates is *not* considered a duplicate!\n",
    "* Mind the difference between row positions and row labels. `.index` of a dataframe returns row labels. `.iloc` takes row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7eb1977de42e36ad646b36acc6315d71",
     "grade": false,
     "grade_id": "cell-7954cffea933a812",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "VARIABLES = [list]; # Change value assignment!\n",
    "\n",
    "def duplicates(x, vars):\n",
    "    \"\"\"\n",
    "    Identifies duplicate rows based on a subset of columns and returns their positional indices.\n",
    "\n",
    "    Args:\n",
    "        x (pandas.DataFrame): The input dataframe.\n",
    "        vars (list): A list of column names to consider for identifying duplicates.\n",
    "\n",
    "    Returns:\n",
    "        list or str: A sorted list of integer row positions of duplicate entries\n",
    "                     (excluding the first occurrence). Returns an empty list if no duplicates.\n",
    "                     Returns specific error strings for invalid input `vars`.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, pd.DataFrame):\n",
    "      print(\"Input x is not a pandas DataFrame.\")\n",
    "      return [] # Or raise error\n",
    "\n",
    "    # Input validation for 'vars'\n",
    "    # Check if vars is None, or the specific placeholder [list], or not a list, or an empty list\n",
    "    if vars is None or vars == [list] or not isinstance(vars, list) or not vars:\n",
    "        return \"Name variables defining potential duplicates!\" # Match T3.2, T3.3\n",
    "\n",
    "    # Check if all column names in vars actually exist in the dataframe x\n",
    "    valid_columns = [v for v in vars if v in x.columns]\n",
    "    if len(valid_columns) != len(vars):\n",
    "        missing_vars = [v for v in vars if v not in x.columns]\n",
    "        # Modify return to provide more specific error, or keep generic as required by asserts?\n",
    "        # Let's keep the generic one for T3.2/T3.3, but add a print for debugging\n",
    "        print(f\"Error: Columns not found in DataFrame: {missing_vars}\")\n",
    "        # Check if this state should also return the specific string?\n",
    "        # The asserts seem to only test None and [list] for that string.\n",
    "        # Let's assume invalid columns should ideally raise an error or return empty list?\n",
    "        # For safety, let's return empty list if columns are invalid, AFTER checking the T3.2/3.3 conditions.\n",
    "        return [] # Return empty list if column names are invalid\n",
    "\n",
    "    # Use pandas duplicated() method\n",
    "    # keep='first' marks all duplicates *except for the first occurrence* as True.\n",
    "    # This matches the requirement: \"The first observation ... is *not* considered a duplicate!\"\n",
    "    try:\n",
    "      duplicate_mask = x.duplicated(subset=vars, keep='first')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during duplicate detection (check column types/content): {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    # Get the row positions (integer locations) where the mask is True\n",
    "    duplicate_positions = np.where(duplicate_mask)[0]\n",
    "\n",
    "    # Sort the positions in ascending order\n",
    "    sorted_positions = sorted(duplicate_positions.tolist())\n",
    "\n",
    "    return sorted_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da0c90261f277ba2850eb06ae49cf99a",
     "grade": false,
     "grade_id": "cell-360763185fff8ba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = tidy(mn);\n",
    "assert len(VARIABLES) > 0 and all([v in df.columns.tolist() for v in VARIABLES]), \"T3.1\"\n",
    "assert duplicates(df, [list]) == \"Name variables defining potential duplicates!\", \"T3.2\"\n",
    "assert duplicates(df, None) == \"Name variables defining potential duplicates!\", \"T3.3\"\n",
    "assert type(duplicates(df, vars = df.columns.tolist())) == list, \"T3.4\"\n",
    "assert all(isinstance(i, int) for i in duplicates(df, df.columns.tolist())), \"T3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51e366349b5d24359135760030a2731c",
     "grade": true,
     "grade_id": "cell-583bc8ab4ba38aa6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! üòº‚ö°Ô∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5245abdf3b47f21f1fb0289ee8217bb",
     "grade": false,
     "grade_id": "cell-b04e3f6689a78a44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 4. Detecting outliers\n",
    "### 4.1. Code part\n",
    "Implement a function called `detecting_outliers` to detect outliers in one selected quantitative variable. Pick a suitable variable from the tidied dataset based on your characterisation and apply one suitable outlier-detection technique as covered in Unit 4. Justify your choice of this technique in the analytical part. Again, the function is assumed to receive a tidied data set from Step 0. The function returns the row positions (*not* row labels!) of the rows containing outliers on the selected variable, sorted in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c10ec065604cff85d708868fa0909ced",
     "grade": false,
     "grade_id": "cell-f593e79eae8f4227",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def detecting_outliers(x):\n",
    "    \"\"\"\n",
    "    Detects outliers in the 'km_per_litre' column using the IQR method.\n",
    "\n",
    "    Args:\n",
    "        x (pandas.DataFrame): The input dataframe (assumed to be tidied, but could be imputed).\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of integer row positions identified as outliers\n",
    "              in the 'km_per_litre' column. Returns empty list if column missing, empty, or has no outliers.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, pd.DataFrame):\n",
    "        print(\"Input is not a pandas DataFrame.\")\n",
    "        return []\n",
    "    if x.empty:\n",
    "        print(\"Input DataFrame is empty.\")\n",
    "        return []\n",
    "\n",
    "    # --- Variable Selection ---\n",
    "    # We select 'km_per_litre' as it's the primary quantitative variable suitable for outlier detection.\n",
    "    variable_to_check = 'km_per_litre'\n",
    "\n",
    "    if variable_to_check not in x.columns:\n",
    "        print(f\"Error: Column '{variable_to_check}' not found in the DataFrame.\")\n",
    "        return []\n",
    "\n",
    "    # --- Outlier Detection Technique: IQR Method ---\n",
    "    # Chosen because it's robust to the underlying distribution shape and extreme values.\n",
    "\n",
    "    # Extract the column, ensuring it's numeric and handle potential non-numeric entries/NaNs introduced before/during tidying\n",
    "    # Use pd.to_numeric, coercing errors will turn non-numeric values into NaN\n",
    "    data_column = pd.to_numeric(x[variable_to_check], errors='coerce')\n",
    "\n",
    "    # Check if the column is entirely NaN after coercion\n",
    "    if data_column.isna().all():\n",
    "        print(f\"Warning: Column '{variable_to_check}' contains no valid numeric data after coercion.\")\n",
    "        return []\n",
    "\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile) on non-NaN values\n",
    "    Q1 = data_column.quantile(0.25)\n",
    "    Q3 = data_column.quantile(0.75)\n",
    "\n",
    "    # Calculate the Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Handle edge case where IQR is zero (e.g., column has constant value after NaN removal)\n",
    "    if IQR == 0:\n",
    "        # In this case, any value different from Q1 (which equals Q3) could be considered an outlier,\n",
    "        # but the standard 1.5*IQR rule becomes useless.\n",
    "        # A common approach is to return no outliers, or flag values unequal to the constant.\n",
    "        # Let's return no outliers for simplicity, assuming constant value is not an outlier situation here.\n",
    "        print(f\"Warning: IQR for '{variable_to_check}' is zero. No outliers detected by standard IQR rule.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    # Define the outlier boundaries using the standard 1.5*IQR rule\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify outliers using the original numeric-coerced column `data_column`\n",
    "    # This automatically handles NaNs (they won't satisfy the condition).\n",
    "    outlier_mask = (data_column < lower_bound) | (data_column > upper_bound)\n",
    "\n",
    "    # Get the row positions (integer locations) where the mask is True\n",
    "    # Ensure the mask is not empty before using np.where\n",
    "    if outlier_mask.any():\n",
    "      outlier_positions = np.where(outlier_mask)[0]\n",
    "    else:\n",
    "      outlier_positions = np.array([]) # Empty array if no outliers\n",
    "\n",
    "    # Sort the positions in ascending order\n",
    "    sorted_positions = sorted(outlier_positions.tolist())\n",
    "\n",
    "    # print(f\"Detected {len(sorted_positions)} outliers in '{variable_to_check}' using IQR ({IQR:.2f}): < {lower_bound:.2f} or > {upper_bound:.2f}\") # Debug print\n",
    "\n",
    "    return sorted_positions\n",
    "\n",
    "# Assertions for Step 4.1\n",
    "# Decide whether to run on df_tidied or df_imputed.\n",
    "# Outlier detection is often done *before* imputation, but can be done after.\n",
    "# Let's run it on df_tidied as per the prompt \"function is assumed to receive a tidied data set from Step 0\".\n",
    "df_for_outliers = df_tidied\n",
    "\n",
    "outlier_list = detecting_outliers(df_for_outliers)\n",
    "assert type(outlier_list) == list, \"T4.1: Function should return a list.\"\n",
    "# Check elements only if list is not empty\n",
    "if outlier_list:\n",
    "    assert all(isinstance(i, int) for i in outlier_list), \"T4.2: List should contain only integers.\"\n",
    "else:\n",
    "    pass # Pass if list is empty (no outliers or error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a687f420acb566a3dbe153acc0bfd16",
     "grade": true,
     "grade_id": "cell-231217b55b6ef843",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = tidy(mn);\n",
    "assert type(detecting_outliers(df)) == list, \"T4.1\"\n",
    "assert all(isinstance(i, int) for i in detecting_outliers(df)), \"T4.2\"\n",
    "assert len(detecting_outliers(df)) > 0 and len(detecting_outliers(df)) < .05*df.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e5ca4b902e63d96e5376adc95c8302d",
     "grade": false,
     "grade_id": "cell-8c4530b128b5c186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2. Analytical part\n",
    "Discuss the implications. \n",
    "\n",
    "- What is the chosen outlier-detection technique? Explain it using your own words in 3-4 sentences.\n",
    "- Describe the outliers detected: How many? How do they relate to the typical, non-outlier values in the remaining dataset?\n",
    "- What could be one reason these outliers appear in the dataset? How would you treat them further?\n",
    "\n",
    "Write your answer in the markdown cell below. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e93305db24e2fc3800f252b797c2ad7",
     "grade": true,
     "grade_id": "cell-254db63097c3ed40",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
