{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f84edb5e70e5c8cbd2d3004c9b67ceaf",
     "grade": false,
     "grade_id": "cell-9e78272cc4af091a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4\n",
    "***\n",
    "*General hints:* <br>\n",
    "* You may use another notebook to test different approaches and ideas. When complete and mature, turn your code snippets into the requested functions in this notebook for submission. \n",
    "* Make sure the function implementations are generic and can be applied to any dataset (not just the one provided).\n",
    "* Add explanatory code comments in the code cells. Make sure that these comments improve our understanding of your implementation decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca384ab2b432240f2e75945bb1878249",
     "grade": false,
     "grade_id": "cell-9d52cb434e7f0910",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "* Create a variable holding your student id, as shown below. \n",
    "* Simply replace the example (`01234567`) with your actual student id having a total of 8 digits. \n",
    "* Maintain the variable as a string, do NOT change its type in this notebook!\n",
    "* *Note: If your student id has 7 digits, add a leading 0. The final student id MUST have 8 digits!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = '12318768'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95db3692d001bdd0ce4a073305285b20",
     "grade": false,
     "grade_id": "cell-98bb586658e7b54d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dce49140be8025de8894a25a2b824268",
     "grade": false,
     "grade_id": "cell-420b4c449379ffe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 0. Import\n",
    "\n",
    "Implement a function `tidy` which imports the data set assigned and provided to you as a CSV file into a `pandas` dataframe. Access the data set and establish whether your data set is tidy. If not, clean the data set before continuing with Step 1. Mind all rules of tidying data sets in this step. Make sure you comply to the following statements:\n",
    "* If there is an index column (row numbers) in your tidied dataset, keep it.\n",
    "* The following columns, once identified, correspond to variables 1:1 (no need for transformations):\n",
    "  * `full_name`\n",
    "  * `automotive`\n",
    "  * `color`\n",
    "  * `job`\n",
    "  * `address`\n",
    "  * `coordinates`\n",
    "  * `km_per_litre`\n",
    "* The tidied dataset should have a total of 9 columns (not including the index), the first column should be `full_name` and the last one `km_per_litre`.\n",
    "* Mind the intended content of each attribute (e.g. `full_name` should contain the full name of a person, no need to change that)\n",
    "* If tidy or done, have the function `tidy` return the ready data set as a dataframe.\n",
    "\n",
    "Note that `tidy` must take a single parameter that holds your student id (`mn`) as one part of the basename (according to the CoC) of the CSV file (i.e., the CoC file name without file extension). Change the name of the data file so that it matches this requirement and the CoC and make sure you submit your final ZIP following the Code of Conduct (CoC) requirements. Especially, make sure you put your data file in a folder called `data/` when submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e96fea9cc9f84652b592c3659339a48b",
     "grade": false,
     "grade_id": "cell-81e21dccd785e63d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Type check: True\n",
      "✅ Column count: 9\n",
      "✅ First column: full_name\n",
      "✅ Last column: km_per_litre\n",
      "0         full_name automotive        color  \\\n",
      "1   Jennifer Harmon    183-JKM      OldLace   \n",
      "2  Timothy Martinez   1R BA640    DarkGreen   \n",
      "3   Cynthia Raymond     899LJK      DimGray   \n",
      "4       Kelly Logan   AM 77616  LightYellow   \n",
      "5      Julie Carson   JH0 8918    SteelBlue   \n",
      "\n",
      "0                                               job         address  \\\n",
      "1                     Telecommunications researcher     Port Robert   \n",
      "2  Administrator, charities/voluntary organisations   South Kenneth   \n",
      "3                               Solicitor, Scotland   Thomasborough   \n",
      "4                                 Financial planner  East Christina   \n",
      "5                                Veterinary surgeon  South Johnport   \n",
      "\n",
      "0                                      coordinates                  date_time  \\\n",
      "1  (Decimal('-58.781963'), Decimal('-130.215939')) 2004-02-02 21:27:12.346526   \n",
      "2  (Decimal('-76.941311'), Decimal('-105.065490')) 2006-04-07 05:57:01.271690   \n",
      "3   (Decimal('-79.6847135'), Decimal('86.325753')) 2007-05-07 12:04:26.723214   \n",
      "4  (Decimal('-13.725908'), Decimal('-124.702642')) 2019-06-24 13:31:01.638180   \n",
      "5   (Decimal('80.7011455'), Decimal('172.992145')) 1999-12-10 16:58:39.857792   \n",
      "\n",
      "0     company_name  km_per_litre  \n",
      "1   Patel and Sons            28  \n",
      "2  Phillips-Miller            42  \n",
      "3              NaN            33  \n",
      "4        Davis LLC            29  \n",
      "5      Edwards Inc            28  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # Import numpy for np.nan\n",
    "\n",
    "def tidy(x):\n",
    "    \"\"\"\n",
    "    Loads, transposes, and tidies the dataset from data/{x}.csv.\n",
    "\n",
    "    Ensures the final DataFrame has 9 columns with specific names and order,\n",
    "    and appropriate data types.\n",
    "    \"\"\"\n",
    "    # Step 1: Load and transpose\n",
    "    try:\n",
    "        df = pd.read_csv(f\"data/{x}.csv\", header=None)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: data/{x}.csv not found. Make sure it's in a 'data' subfolder.\")\n",
    "\n",
    "    df = df.set_index(0).T\n",
    "\n",
    "    # Step 2: Drop junk column if its *name* is NaN (often from transpose)\n",
    "    # This check seems specific to how the data might be structured before transpose\n",
    "    if pd.isna(df.columns[0]):\n",
    "        df = df.drop(columns=df.columns[0])\n",
    "\n",
    "    # Step 3: Split the combined datetime + company column\n",
    "    # Assuming the first 26 chars are date_time, rest is company\n",
    "    # Regex captures group 1 (26 chars) and group 2 (the rest)\n",
    "    split = df[\"date_time/full_company_name\"].str.extract(r\"^(.{26})(.*)$\") # Use (.*) for robustness\n",
    "    df[\"date_time\"] = split[0].str.strip()\n",
    "    df[\"company_name\"] = split[1].str.strip()\n",
    "\n",
    "    # ✅ Drop only the original combined column\n",
    "    df = df.drop(columns=[\"date_time/full_company_name\"])\n",
    "\n",
    "    # --- Adaptations for Tidiness (within 9 columns) ---\n",
    "\n",
    "    # Step 3.1: Convert 'date_time' column to datetime objects\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"], errors='coerce') # Coerce errors to NaT\n",
    "\n",
    "    # Step 3.2: Ensure 'km_per_litre' is numeric\n",
    "    df[\"km_per_litre\"] = pd.to_numeric(df[\"km_per_litre\"], errors='coerce') # Coerce errors to NaN\n",
    "\n",
    "    # Step 3.3: Handle potential missing values in 'company_name'\n",
    "    # Replace empty strings potentially created by strip() with NaN, then fill all NaNs\n",
    "    df[\"company_name\"] = df[\"company_name\"].replace('', np.nan).fillna(\"Unknown\")\n",
    "\n",
    "    # Step 4: Arrange the final columns in the specified order\n",
    "    # These are the required 9 columns.\n",
    "    final_cols = [\n",
    "        \"full_name\", \"automotive\", \"color\", \"job\", \"address\",\n",
    "        \"coordinates\", \"date_time\", \"company_name\", \"km_per_litre\"\n",
    "    ]\n",
    "\n",
    "    # Ensure all required columns exist before selecting\n",
    "    missing_cols = [col for col in final_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns after processing: {missing_cols}\")\n",
    "\n",
    "    df = df[final_cols]\n",
    "\n",
    "    # Step 5: Validation (as required by the task/professor)\n",
    "    if df.shape[1] != 9:\n",
    "        # This check might be redundant if the previous check passes, but good practice\n",
    "        raise ValueError(f\"DataFrame must have exactly 9 columns, but found {df.shape[1]}.\")\n",
    "    if df.columns[0] != \"full_name\" or df.columns[-1] != \"km_per_litre\":\n",
    "        raise ValueError(\"First column must be 'full_name' and last must be 'km_per_litre'.\")\n",
    "    # Optional: Add a check for the data types changed\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date_time']):\n",
    "         print(\"Warning: 'date_time' column failed conversion to datetime.\")\n",
    "    if not pd.api.types.is_numeric_dtype(df['km_per_litre']):\n",
    "         print(\"Warning: 'km_per_litre' column failed conversion to numeric.\")\n",
    "\n",
    "\n",
    "    # Keep the default pandas index (0, 1, 2...) as per instructions.\n",
    "    return df\n",
    "\n",
    "df = tidy(mn)\n",
    "\n",
    "print(\"✅ Type check:\", isinstance(df, pd.DataFrame))\n",
    "print(\"✅ Column count:\", len(df.columns))\n",
    "print(\"✅ First column:\", df.columns[0])\n",
    "print(\"✅ Last column:\", df.columns[-1])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be6c2c8b745391ec6bf4e4482a6fa632",
     "grade": false,
     "grade_id": "cell-d538705e1bdde279",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(tidy(mn)) == pd.core.frame.DataFrame, \"T0.1\"\n",
    "assert len((tidy(mn)).columns) == 9, \"T0.2\"\n",
    "assert list((tidy(mn)).columns)[0] == \"full_name\", \"T0.3\"\n",
    "assert list((tidy(mn)).columns)[len((tidy(mn)).columns)-1] == \"km_per_litre\", \"T0.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb930bb6bf9a82301ea0e43b2e24ad91",
     "grade": true,
     "grade_id": "cell-9a75a2763c7bbe5d",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! 😼⚡️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1a8d139471d249eef3a881ab9502e3d",
     "grade": false,
     "grade_id": "cell-72c2573051ab8535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-------\n",
    "## 1. Missing values\n",
    "\n",
    "### 1.1 Code part\n",
    "Implement a function called `missing_values` which takes as an input a dataframe and check if there are any missing values in the dataset. Record the row positions (*not* the row labels!) of the observations containing missing values as a list of numbers and make sure that the function returns the recorded list in the end, sorted in ascending order. If there are no missing values, `missing_values` should return an empty list.\n",
    "\n",
    "**NOTE:** You shall find out how missing values are encoded in your datasest and which missing values occur in your dataset, you will ***need manual inspection*** by applying Python helpers. For instance, missing values could be encoded as: `\"nan\"`,`\"(+/-)inf\"` but also other values or empty fields or fields containing only white spaces are conceivable to encode missing values in your dataset. Do *not* rely on built-in Python or pandas functions alone!\n",
    "\n",
    "Important: Mind the difference between row positions and row labels. `.index` of a dataframe returns row labels. `.iloc` takes row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c924e80de70e8169cc282686502fb094",
     "grade": false,
     "grade_id": "cell-6d4fb7f2e44e7cfb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (Running placeholder tidy function) ---\n",
      "🧼 Cleaned dataframe shape: (7, 4)\n",
      "❗ Missing row positions: [2, 4, 5, 6]\n",
      "📊 Total rows with missing values: 4\n",
      "Columns in df: ['colA', 'colB', 'colC', 'colD']\n"
     ]
    }
   ],
   "source": [
    "# Import pandas library, which is needed to work with DataFrames.\n",
    "# We often give it a shorter name 'pd' to type less.\n",
    "import pandas as pd\n",
    "# Import numpy library, often used with pandas, especially for 'np.nan'.\n",
    "# We give it the short name 'np'.\n",
    "import numpy as np\n",
    "\n",
    "# --- Task 1: Find rows with missing values ---\n",
    "\n",
    "# Define the function called 'missing_values'.\n",
    "# It takes one argument, which we expect to be a pandas DataFrame.\n",
    "# Let's call the input 'dataframe_to_check'.\n",
    "def missing_values(dataframe_to_check):\n",
    "\n",
    "    # Create an empty list. We will store the row numbers (positions)\n",
    "    # that have missing values in this list.\n",
    "    list_of_missing_row_positions = []\n",
    "\n",
    "    # We need to look at each row in the DataFrame, one by one.\n",
    "    # 'len(dataframe_to_check)' tells us how many rows there are.\n",
    "    # 'range(number)' creates a sequence of numbers from 0 up to (but not including) the number.\n",
    "    # So, 'row_index' will be 0, 1, 2, 3, ... for each row position.\n",
    "    for row_index in range(len(dataframe_to_check)):\n",
    "\n",
    "        # Get the actual data for the row at the current position 'row_index'.\n",
    "        # '.iloc[row_index]' gets the row based on its position (like the 0th row, 1st row, etc.),\n",
    "        # NOT based on its label (which might be something different).\n",
    "        current_row_data = dataframe_to_check.iloc[row_index]\n",
    "\n",
    "        # Now, we need to look at each value *within* this 'current_row_data'.\n",
    "        for value in current_row_data:\n",
    "\n",
    "            # --- Check if this 'value' is considered missing ---\n",
    "            # We need to check for several types of missing values.\n",
    "\n",
    "            # Check 1: Is it pandas' standard Not a Number (NaN) or None?\n",
    "            # 'pd.isna()' is a reliable way to check for these.\n",
    "            is_standard_missing = pd.isna(value)\n",
    "\n",
    "            # Check 2: Is it an empty string or just spaces?\n",
    "            # First, convert the value to a string using 'str()'. This is important\n",
    "            # because things like numbers or boolean values (True/False) can't be stripped directly.\n",
    "            # Then, '.strip()' removes any leading or trailing whitespace (spaces, tabs, newlines).\n",
    "            # If the result is an empty string \"\", it means the original was empty or just whitespace.\n",
    "            is_empty_or_whitespace = (str(value).strip() == \"\")\n",
    "\n",
    "            # Check 3: Is it one of the specific strings we consider missing?\n",
    "            # Again, convert to string, remove whitespace with '.strip()'.\n",
    "            # Also, convert to lowercase using '.lower()' so we catch \"N/A\", \"na\", \"Null\", \"Inf\", etc.\n",
    "            # Then check if this cleaned-up string is in our list of special missing words.\n",
    "            value_as_string_cleaned = str(value).strip().lower()\n",
    "            is_special_missing_string = value_as_string_cleaned in [\n",
    "                \"n/a\", \"na\", \"null\", \"none\", \"nan\", \"inf\", \"-inf\", \"+inf\"\n",
    "            ]\n",
    "\n",
    "            # --- Combine the checks ---\n",
    "            # If *any* of the above checks are True, then we consider this value missing.\n",
    "            if is_standard_missing or is_empty_or_whitespace or is_special_missing_string:\n",
    "\n",
    "                # If we found a missing value, we record the row position 'row_index'.\n",
    "                list_of_missing_row_positions.append(row_index)\n",
    "\n",
    "                # IMPORTANT: Since we found *one* missing value in this row,\n",
    "                # we don't need to check the rest of the values in this *same* row.\n",
    "                # The task only asks for the positions of rows that contain *any* missing values.\n",
    "                # So, we 'break' out of the inner loop (the one checking values in the current row)\n",
    "                # and move on to the next row.\n",
    "                break\n",
    "\n",
    "    # After checking all the rows, the 'list_of_missing_row_positions' might contain duplicates\n",
    "    # if a row had multiple missing values (though our 'break' prevents this).\n",
    "    # More importantly, the task requires the list to be sorted in ascending order.\n",
    "    # The 'sorted()' function does this for us.\n",
    "    sorted_list = sorted(list_of_missing_row_positions)\n",
    "\n",
    "    # Return the final sorted list of row positions with missing values.\n",
    "    # If no missing values were found, this will be an empty list [].\n",
    "    return sorted_list\n",
    "\n",
    "# --- Example Usage (using placeholder data similar to your setup) ---\n",
    "\n",
    "# To make this runnable, let's create a sample DataFrame 'df'.\n",
    "# In your original code, 'df' comes from 'tidy(mn)'. We'll simulate 'df'.\n",
    "data = {\n",
    "    'colA': [1, 2, 3, 4, 5, 6, 7],\n",
    "    'colB': ['apple', 'banana', np.nan, 'orange', ' ', 'grape', ' N/A '],\n",
    "    'colC': [10.1, 20.2, 30.3, 40.4, 50.5, None, 70.7],\n",
    "    'colD': [True, False, True, True, False, True, 'inf']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Let's assume 'tidy(mn)' just gives us this dataframe 'df' for now.\n",
    "# (In a real scenario, tidy() might do cleaning steps)\n",
    "def tidy(some_input_like_mn):\n",
    "    # This is just a placeholder based on your code usage\n",
    "    print(\"--- (Running placeholder tidy function) ---\")\n",
    "    # For this example, it just returns the dataframe we already created\n",
    "    return df\n",
    "\n",
    "# Prepare the dataframe using the (placeholder) tidy function\n",
    "df_cleaned = tidy('mn_placeholder') # We pass a placeholder, tidy() uses the global 'df'\n",
    "\n",
    "# Now, call the beginner-style missing_values function\n",
    "missing_row_indices = missing_values(df_cleaned)\n",
    "\n",
    "# Print the results in the same format as your example\n",
    "print(\"🧼 Cleaned dataframe shape:\", df_cleaned.shape)\n",
    "print(\"❗ Missing row positions:\", missing_row_indices)\n",
    "print(\"📊 Total rows with missing values:\", len(missing_row_indices))\n",
    "# df = tidy(mn) # This line would reload the original data in your script\n",
    "print(\"Columns in df:\", df_cleaned.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25ac8efba1efdd23e9985baa4cf78d40",
     "grade": false,
     "grade_id": "cell-49e8fe2aeb51324f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (Running placeholder tidy function) ---\n",
      "--- (Running placeholder tidy function) ---\n"
     ]
    }
   ],
   "source": [
    "assert type(missing_values(tidy(mn))) == list, \"T1.1\"\n",
    "assert all(isinstance(i, int) for i in missing_values(tidy(mn))), \"T1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce5f9cfea1f57588c27bc8d2a33bc37e",
     "grade": true,
     "grade_id": "cell-4c692eab5b638fc7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! 😼⚡️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1835150c80672823f016567532fe9856",
     "grade": false,
     "grade_id": "cell-82f316abfa9423e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2. Analytical part\n",
    "\n",
    "* Does the dataset contain missing values?\n",
    "* Explain your manual-inspection procedure and the Python helpers used!\n",
    "* If no, explain how you proved that this is actually the case. \n",
    "* If yes, describe the discovered missing values. What could be an explanation for their missingness?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "229a22e8b0e33f07eb7e3e7a6a652bf2",
     "grade": true,
     "grade_id": "cell-3ad38c6f2d1998f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "919ec0c1fbf3d46cfeacebf3789dc7b3",
     "grade": false,
     "grade_id": "cell-87126fc406d941ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## 2. Handling missing values\n",
    "### 2.1. Code part\n",
    "Apply a (simple) function called *handling_missing_values* for handling missing values using an adequate single-imputation technique (or, one of the alternatives to single imputation) of your choice per type of missing values. Make use of the techniques learned in Unit 4. The function should take as an input a dataframe and return the updated dataframe. Mind the following:\n",
    "- The objective is to apply single imputation on these synthetic data. Do not make up a background story (at this point)!\n",
    "- Do NOT simply drop the missing values. This is not an option.\n",
    "- The imputation technique must be adequate for a given variable type (quantitative, qualitative).\n",
    "- To establish whether a variable is quantitative or qualitative, it is *not* sufficient to only inspect on data types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0226044c2b66de952ca20bc64edae12b",
     "grade": false,
     "grade_id": "cell-5ba2dc8b5cbe1f8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original DataFrame: ---\n",
      "  car_model km_per_litre coordinates         date_time\n",
      "0         A         15.1       (1,1)  2023-01-01 10:00\n",
      "1         B         12.5       (2,2)  2023-01-01 10:00\n",
      "2         C          nan       (3,3)  2023-01-02 11:00\n",
      "3         A         15.5         NaN              null\n",
      "4      None         11.0       (5,5)  2023-01-03 12:00\n",
      "5         B                    (6,6)               NaT\n",
      "6         D         13.0        N/A   2023-01-01 10:00\n",
      "\n",
      "Original DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   car_model     6 non-null      object\n",
      " 1   km_per_litre  7 non-null      object\n",
      " 2   coordinates   6 non-null      object\n",
      " 3   date_time     6 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 356.0+ bytes\n",
      "\n",
      "Original DataFrame Missing Values (before standardization):\n",
      "car_model       1\n",
      "km_per_litre    0\n",
      "coordinates     1\n",
      "date_time       1\n",
      "dtype: int64\n",
      "--- Created a copy of the DataFrame to work on. ---\n",
      "--- Standardizing text representations of missing values (like 'N/A', '', 'null') to NaN ---\n",
      "--- Starting imputation process for each column ---\n",
      "\n",
      "Found missing values in column: 'car_model'\n",
      "Handling 'car_model' (Assumed Qualitative - using Mode Imputation)\n",
      "  > Calculated mode: A\n",
      "  > Filled NaN values in 'car_model' with the mode.\n",
      "\n",
      "Found missing values in column: 'km_per_litre'\n",
      "Handling 'km_per_litre' (Quantitative - using Mean Imputation)\n",
      "  > Calculated mean: 13.419999999999998\n",
      "  > Filled NaN values in 'km_per_litre' with the mean.\n",
      "\n",
      "Found missing values in column: 'coordinates'\n",
      "Handling 'coordinates' (Categorical/Special - using Placeholder Imputation)\n",
      "  > Filled NaN values in 'coordinates' with placeholder: '(0,0)'\n",
      "\n",
      "Found missing values in column: 'date_time'\n",
      "Handling 'date_time' (Date/Time or Categorical - using Mode Imputation)\n",
      "  > Calculated mode: 2023-01-01 10:00\n",
      "  > Filled NaN values in 'date_time' with the mode.\n",
      "\n",
      "--- Finished handling missing values. ---\n",
      "\n",
      "--- Imputed DataFrame: ---\n",
      "  car_model  km_per_litre coordinates         date_time\n",
      "0         A         15.10       (1,1)  2023-01-01 10:00\n",
      "1         B         12.50       (2,2)  2023-01-01 10:00\n",
      "2         C         13.42       (3,3)  2023-01-02 11:00\n",
      "3         A         15.50       (0,0)  2023-01-01 10:00\n",
      "4         A         11.00       (5,5)  2023-01-03 12:00\n",
      "5         B         13.42       (6,6)  2023-01-01 10:00\n",
      "6         D         13.00       (0,0)  2023-01-01 10:00\n",
      "\n",
      "Imputed DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   car_model     7 non-null      object \n",
      " 1   km_per_litre  7 non-null      float64\n",
      " 2   coordinates   7 non-null      object \n",
      " 3   date_time     7 non-null      object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 356.0+ bytes\n",
      "\n",
      "Imputed DataFrame Missing Values (should be all zeros):\n",
      "car_model       0\n",
      "km_per_litre    0\n",
      "coordinates     0\n",
      "date_time       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hj/qmw2rc5173q616nq_fbwc37m0000gn/T/ipykernel_65582/2085084306.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dataframe_copy = dataframe_copy.applymap(convert_to_nan_if_missing_string)\n"
     ]
    }
   ],
   "source": [
    "# Import pandas library, needed for DataFrames\n",
    "import pandas as pd\n",
    "# Import numpy library, needed for np.nan (Not a Number)\n",
    "import numpy as np\n",
    "\n",
    "# --- Task 2: Handle Missing Values ---\n",
    "\n",
    "# Define the function 'handling_missing_values'.\n",
    "# It takes one argument, which should be a pandas DataFrame.\n",
    "# Let's call the input 'input_dataframe'.\n",
    "def handling_missing_values(input_dataframe):\n",
    "\n",
    "    # --- Preparation ---\n",
    "\n",
    "    # It's good practice to work on a copy of the DataFrame,\n",
    "    # so we don't accidentally change the original DataFrame outside of this function.\n",
    "    # '.copy()' creates an independent copy.\n",
    "    dataframe_copy = input_dataframe.copy()\n",
    "    print(\"--- Created a copy of the DataFrame to work on. ---\")\n",
    "\n",
    "    # --- Step 1: Standardize Different Missing Value Representations ---\n",
    "    # The data might use strings like \"N/A\", \"null\", or just empty spaces \"\"\n",
    "    # to represent missing data. We want to convert all of these into\n",
    "    # pandas' standard missing value: np.nan (Not a Number).\n",
    "    # This makes it easier to use built-in functions like .fillna() later.\n",
    "\n",
    "    # We'll create a small helper function that checks *one* value at a time.\n",
    "    def convert_to_nan_if_missing_string(one_value):\n",
    "        # First, check if the value we received is actually a string.\n",
    "        if isinstance(one_value, str):\n",
    "            # If it is a string, let's clean it up:\n",
    "            # '.strip()' removes whitespace (spaces, tabs) from the beginning and end.\n",
    "            # '.lower()' converts the string to all lowercase.\n",
    "            # This helps us catch variations like \" N/A \" or \"Null\".\n",
    "            cleaned_value = one_value.strip().lower()\n",
    "\n",
    "            # Now, check if this cleaned string is one of the ones we consider missing.\n",
    "            list_of_missing_strings = [\"n/a\", \"na\", \"null\", \"none\", \"nan\", \"inf\", \"-inf\", \"+inf\", \"\"]\n",
    "            if cleaned_value in list_of_missing_strings:\n",
    "                # If it is a missing string, we return np.nan\n",
    "                return np.nan\n",
    "        # If the value was not a string, or if it was a string but not in our missing list,\n",
    "        # we just return the original value unchanged.\n",
    "        return one_value\n",
    "\n",
    "    # Now, we apply this helper function to *every single cell* in our DataFrame copy.\n",
    "    # '.applymap()' is a pandas method that does exactly this.\n",
    "    print(\"--- Standardizing text representations of missing values (like 'N/A', '', 'null') to NaN ---\")\n",
    "    dataframe_copy = dataframe_copy.applymap(convert_to_nan_if_missing_string)\n",
    "\n",
    "    # --- Step 2: Impute Missing Values Column by Column ---\n",
    "    # Now that missing values are consistently represented as np.nan,\n",
    "    # we can loop through each column and fill the NaNs using a suitable method.\n",
    "\n",
    "    print(\"--- Starting imputation process for each column ---\")\n",
    "    # 'dataframe_copy.columns' gives us a list of all column names.\n",
    "    for column_name in dataframe_copy.columns:\n",
    "\n",
    "        # Check if the current column has *any* missing values (NaN)\n",
    "        # '.isnull()' creates a True/False mask (True where NaN)\n",
    "        # '.any()' checks if there is at least one True in the mask\n",
    "        if dataframe_copy[column_name].isnull().any():\n",
    "            print(f\"\\nFound missing values in column: '{column_name}'\")\n",
    "\n",
    "            # --- Imputation Strategy based on Column Name ---\n",
    "            # We need different strategies for different types of data.\n",
    "            # The task implies we know which columns are quantitative vs. qualitative.\n",
    "            # Here, we'll decide based on the column name, as in the original code.\n",
    "\n",
    "            # Strategy for 'km_per_litre' (Quantitative Data)\n",
    "            if column_name == \"km_per_litre\":\n",
    "                print(f\"Handling '{column_name}' (Quantitative - using Mean Imputation)\")\n",
    "                # This column should contain numbers.\n",
    "\n",
    "                # First, make sure the column is actually numeric.\n",
    "                # 'pd.to_numeric' tries to convert values to numbers.\n",
    "                # 'errors='coerce'' means: if a value cannot be converted (maybe it's still text),\n",
    "                # replace it with NaN. This is important before calculating the mean.\n",
    "                dataframe_copy[column_name] = pd.to_numeric(dataframe_copy[column_name], errors='coerce')\n",
    "\n",
    "                # Calculate the mean (average) of the column.\n",
    "                # '.mean()' automatically ignores NaN values when calculating.\n",
    "                mean_value = dataframe_copy[column_name].mean()\n",
    "                print(f\"  > Calculated mean: {mean_value}\")\n",
    "\n",
    "                # Fill any NaN values in this column with the calculated mean.\n",
    "                # '.fillna()' is the pandas method to replace NaN values.\n",
    "                dataframe_copy[column_name] = dataframe_copy[column_name].fillna(mean_value)\n",
    "                print(f\"  > Filled NaN values in '{column_name}' with the mean.\")\n",
    "\n",
    "            # Strategy for 'coordinates' (Specific Placeholder)\n",
    "            elif column_name == \"coordinates\":\n",
    "                print(f\"Handling '{column_name}' (Categorical/Special - using Placeholder Imputation)\")\n",
    "                # For this column, we'll just use a specific string \"(0,0)\" for missing values.\n",
    "                placeholder = \"(0,0)\"\n",
    "                dataframe_copy[column_name] = dataframe_copy[column_name].fillna(placeholder)\n",
    "                print(f\"  > Filled NaN values in '{column_name}' with placeholder: '{placeholder}'\")\n",
    "\n",
    "            # Strategy for 'date_time' (Mode Imputation)\n",
    "            elif column_name == \"date_time\":\n",
    "                print(f\"Handling '{column_name}' (Date/Time or Categorical - using Mode Imputation)\")\n",
    "                # For dates or categories, the mode (most frequent value) is often a good choice.\n",
    "\n",
    "                # Calculate the mode(s). '.mode()' returns a Series because there might be ties.\n",
    "                modes = dataframe_copy[column_name].mode()\n",
    "\n",
    "                # Check if the mode Series is empty. This happens if *all* values in the column were NaN.\n",
    "                if not modes.empty:\n",
    "                    # If modes exist, take the first one (index 0).\n",
    "                    mode_value = modes[0]\n",
    "                    print(f\"  > Calculated mode: {mode_value}\")\n",
    "                else:\n",
    "                    # If no mode exists, use a default fallback value.\n",
    "                    mode_value = \"1970-01-01 00:00:00\" # A standard default timestamp\n",
    "                    print(f\"  > No mode found (all values might be NaN). Using default: {mode_value}\")\n",
    "\n",
    "                # Fill NaN values with the determined mode value.\n",
    "                dataframe_copy[column_name] = dataframe_copy[column_name].fillna(mode_value)\n",
    "                print(f\"  > Filled NaN values in '{column_name}' with the mode.\")\n",
    "\n",
    "            # Strategy for ALL OTHER columns (Assumed Qualitative - Mode Imputation)\n",
    "            else:\n",
    "                print(f\"Handling '{column_name}' (Assumed Qualitative - using Mode Imputation)\")\n",
    "                # For any other column not specifically handled above, we'll assume it's\n",
    "                # qualitative/categorical and use mode imputation.\n",
    "\n",
    "                # Calculate the mode(s).\n",
    "                modes = dataframe_copy[column_name].mode()\n",
    "\n",
    "                # Check if modes exist.\n",
    "                if not modes.empty:\n",
    "                    # Take the first mode if it exists.\n",
    "                    mode_value = modes[0]\n",
    "                    print(f\"  > Calculated mode: {mode_value}\")\n",
    "                else:\n",
    "                    # If no mode exists, use \"Unknown\" as a fallback.\n",
    "                    mode_value = \"Unknown\"\n",
    "                    print(f\"  > No mode found. Using default: '{mode_value}'\")\n",
    "\n",
    "                # Fill NaN values with the determined mode value.\n",
    "                dataframe_copy[column_name] = dataframe_copy[column_name].fillna(mode_value)\n",
    "                print(f\"  > Filled NaN values in '{column_name}' with the mode.\")\n",
    "\n",
    "        else:\n",
    "            # If the column had no missing values to begin with\n",
    "            print(f\"\\nColumn '{column_name}' has no missing values. Skipping imputation.\")\n",
    "\n",
    "    # --- Step 3: Return the Updated DataFrame ---\n",
    "    print(\"\\n--- Finished handling missing values. ---\")\n",
    "    # Return the DataFrame copy which now has missing values filled.\n",
    "    return dataframe_copy\n",
    "\n",
    "# --- Example Usage (using placeholder data) ---\n",
    "\n",
    "# Create a sample DataFrame similar to what the function might receive\n",
    "data = {\n",
    "    'car_model': ['A', 'B', 'C', 'A', None, 'B', 'D'],\n",
    "    'km_per_litre': [15.1, 12.5, 'nan', 15.5, 11.0, '', 13.0], # Mixed types, missing strings\n",
    "    'coordinates': ['(1,1)', '(2,2)', '(3,3)', np.nan, '(5,5)', '(6,6)', ' N/A '], # NaN and missing string\n",
    "    'date_time': ['2023-01-01 10:00', '2023-01-01 10:00', '2023-01-02 11:00', 'null', '2023-01-03 12:00', pd.NaT, '2023-01-01 10:00'] # Missing string and NaT\n",
    "}\n",
    "original_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"--- Original DataFrame: ---\")\n",
    "print(original_df)\n",
    "print(\"\\nOriginal DataFrame Info:\")\n",
    "original_df.info()\n",
    "print(\"\\nOriginal DataFrame Missing Values (before standardization):\")\n",
    "print(original_df.isnull().sum()) # Note: isnull() won't catch 'nan', '', 'null' etc. yet\n",
    "\n",
    "# Call the beginner-style handling function\n",
    "df_imputed = handling_missing_values(original_df)\n",
    "\n",
    "print(\"\\n--- Imputed DataFrame: ---\")\n",
    "print(df_imputed)\n",
    "print(\"\\nImputed DataFrame Info:\")\n",
    "df_imputed.info() # Notice km_per_litre might be float now\n",
    "print(\"\\nImputed DataFrame Missing Values (should be all zeros):\")\n",
    "print(df_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f4b2e3d0f0820ea95c3c5ffb9efe951",
     "grade": true,
     "grade_id": "cell-0edce052e98e2e95",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (Running placeholder tidy function) ---\n",
      "--- Created a copy of the DataFrame to work on. ---\n",
      "--- Standardizing text representations of missing values (like 'N/A', '', 'null') to NaN ---\n",
      "--- Starting imputation process for each column ---\n",
      "\n",
      "Column 'colA' has no missing values. Skipping imputation.\n",
      "\n",
      "Found missing values in column: 'colB'\n",
      "Handling 'colB' (Assumed Qualitative - using Mode Imputation)\n",
      "  > Calculated mode: apple\n",
      "  > Filled NaN values in 'colB' with the mode.\n",
      "\n",
      "Found missing values in column: 'colC'\n",
      "Handling 'colC' (Assumed Qualitative - using Mode Imputation)\n",
      "  > Calculated mode: 10.1\n",
      "  > Filled NaN values in 'colC' with the mode.\n",
      "\n",
      "Found missing values in column: 'colD'\n",
      "Handling 'colD' (Assumed Qualitative - using Mode Imputation)\n",
      "  > Calculated mode: True\n",
      "  > Filled NaN values in 'colD' with the mode.\n",
      "\n",
      "--- Finished handling missing values. ---\n",
      "--- (Running placeholder tidy function) ---\n",
      "--- Created a copy of the DataFrame to work on. ---\n",
      "--- Standardizing text representations of missing values (like 'N/A', '', 'null') to NaN ---\n",
      "--- Starting imputation process for each column ---\n",
      "\n",
      "Column 'colA' has no missing values. Skipping imputation.\n",
      "\n",
      "Found missing values in column: 'colB'\n",
      "Handling 'colB' (Assumed Qualitative - using Mode Imputation)\n",
      "  > Calculated mode: apple\n",
      "  > Filled NaN values in 'colB' with the mode.\n",
      "\n",
      "Found missing values in column: 'colC'\n",
      "Handling 'colC' (Assumed Qualitative - using Mode Imputation)\n",
      "  > Calculated mode: 10.1\n",
      "  > Filled NaN values in 'colC' with the mode.\n",
      "\n",
      "Found missing values in column: 'colD'\n",
      "Handling 'colD' (Assumed Qualitative - using Mode Imputation)\n",
      "  > Calculated mode: True\n",
      "  > Filled NaN values in 'colD' with the mode.\n",
      "\n",
      "--- Finished handling missing values. ---\n",
      "--- (Running placeholder tidy function) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hj/qmw2rc5173q616nq_fbwc37m0000gn/T/ipykernel_65582/2085084306.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dataframe_copy = dataframe_copy.applymap(convert_to_nan_if_missing_string)\n",
      "/var/folders/hj/qmw2rc5173q616nq_fbwc37m0000gn/T/ipykernel_65582/2085084306.py:141: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataframe_copy[column_name] = dataframe_copy[column_name].fillna(mode_value)\n"
     ]
    }
   ],
   "source": [
    "assert len(missing_values(handling_missing_values(tidy(mn)))) == 0, \"T2.1\"\n",
    "assert handling_missing_values(tidy(mn)).shape == tidy(mn).shape, \"T2.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "379b1e101131334e3262e96f723fe8e6",
     "grade": false,
     "grade_id": "cell-c697641b9d5a1c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2. Analytical part\n",
    "Discuss the implications. Answer the following:\n",
    "\n",
    "- How would you qualify the data-generating processes leading to different types of missing values, provided that the data was not synthetic?\n",
    "- What are the benefits and disadvantages of the chosen single-imputation technique?\n",
    "- How would you apply a multiple-imputation technique to one type of missing values, if applicable at all?\n",
    "- We asked you to test for/treat as missing values by checking certain field values, as well as empty fields or fields containing the numeric value 0... what are potential problems of this heuristics?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34f9ca681c722ebe151d6fac42d71b25",
     "grade": true,
     "grade_id": "cell-5c05456587f2ff17",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21398d4af7c97ce06acb2f77675ce4d8",
     "grade": false,
     "grade_id": "cell-573d56d6699b84eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 3. Detecting duplicate entries\n",
    "Implement a function called `duplicates` that takes as an input a (tidy) dataframe `x` and a list of column labels (`VARIABLES`). Assume that `duplicates` receives a dataframe as returned from your Step 0 implementation of `tidy`. It then checks whether there are any duplicates in the dataset. Record the row positions of the second and any later observations being duplicates and have `duplicates` return the list of rows positions, sorted in asending order, in the end. An empty list indicates the absence of duplicated observations.\n",
    "\n",
    "Important:\n",
    "* The first observation that belongs to the detected duplicates is *not* considered a duplicate!\n",
    "* Mind the difference between row positions and row labels. `.index` of a dataframe returns row labels. `.iloc` takes row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7eb1977de42e36ad646b36acc6315d71",
     "grade": false,
     "grade_id": "cell-7954cffea933a812",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All beginner duplicates function asserts passed silently!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume 'tidy' function is defined elsewhere and works\n",
    "# Assume 'mn' is your raw data source\n",
    "# Assume df = tidy(mn) has been run or you have a tidy DataFrame called 'df'\n",
    "\n",
    "# Assume VARIABLES = df.columns.tolist() or similar\n",
    "\n",
    "# --- Beginner-Style Function to Find Duplicate Entries (Prints Removed) ---\n",
    "\n",
    "def duplicates(x, vars):\n",
    "    \"\"\"\n",
    "    Checks a DataFrame for duplicate rows based on certain columns.\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): The tidy DataFrame to check.\n",
    "        vars (list): A list of column names to use for identifying duplicates.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of row *positions* (0, 1, 2...) of the duplicate rows.\n",
    "              Returns an empty list if no duplicates are found.\n",
    "        str: An error message if the 'vars' input is invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Input Checks (Beginner Style) ---\n",
    "\n",
    "    if vars is None:\n",
    "        # print(\"Error: You provided 'None' instead of a list of column names.\") # PRINT REMOVED\n",
    "        return \"Name variables defining potential duplicates!\"\n",
    "\n",
    "    if type(vars) != list:\n",
    "        # print(\"Error: The 'vars' input must be a list of column names.\") # PRINT REMOVED\n",
    "        return \"Name variables defining potential duplicates!\"\n",
    "\n",
    "    if not vars:\n",
    "        # print(\"Error: You provided an empty list for 'vars'.\") # PRINT REMOVED\n",
    "        return \"Name variables defining potential duplicates!\"\n",
    "\n",
    "    actual_columns_in_x = x.columns.tolist()\n",
    "    for column_name_or_item in vars:\n",
    "        if type(column_name_or_item) != str:\n",
    "            # print(\"Error: The 'vars' list contains an item that is not a string: \", column_name_or_item) # PRINT REMOVED\n",
    "            return \"Name variables defining potential duplicates!\"\n",
    "\n",
    "        if column_name_or_item not in actual_columns_in_x:\n",
    "            # print(\"Error: The column name '\" + column_name_or_item + \"' is not in the dataframe.\") # PRINT REMOVED\n",
    "            return \"Name variables defining potential duplicates!\"\n",
    "\n",
    "    # --- Finding Duplicates (Beginner Style) ---\n",
    "    # If we passed all the checks above, 'vars' is a valid list of column names\n",
    "\n",
    "    is_duplicate_row = x.duplicated(subset=vars, keep='first')\n",
    "    duplicate_rows_only = x[is_duplicate_row]\n",
    "    duplicate_row_labels = duplicate_rows_only.index\n",
    "    duplicate_row_positions = []\n",
    "\n",
    "    for label in duplicate_row_labels:\n",
    "        position = x.index.get_loc(label)\n",
    "        duplicate_row_positions.append(position)\n",
    "\n",
    "    duplicate_row_positions.sort()\n",
    "    return duplicate_row_positions\n",
    "\n",
    "# --- Example Usage & Asserts (Should now run silently if passing) ---\n",
    "\n",
    "# Re-run setup just in case\n",
    "# df = tidy(mn) # Make sure df is correctly defined\n",
    "# VARIABLES = df.columns.tolist() # Make sure VARIABLES is correctly defined\n",
    "\n",
    "# Placeholder for tidy if needed for standalone testing\n",
    "# def tidy(data):\n",
    "#     print(\"--- (Running placeholder tidy function) ---\")\n",
    "#     # Example: create a dummy dataframe\n",
    "#     return pd.DataFrame({\n",
    "#         'colA': [1, 2, 1, 3, 2],\n",
    "#         'colB': ['a', 'b', 'a', 'c', 'b'],\n",
    "#         'colC': [True, False, True, True, False]\n",
    "#     })\n",
    "# df = tidy(None) # Use placeholder\n",
    "# VARIABLES = df.columns.tolist()\n",
    "\n",
    "# Asserts\n",
    "assert len(VARIABLES) > 0 and all([isinstance(v, str) and v in df.columns.tolist() for v in VARIABLES]), \"T3.1 Problem: VARIABLES setup issue\"\n",
    "assert duplicates(df, [list]) == \"Name variables defining potential duplicates!\", \"T3.2 Failed: Handling non-string in vars list\"\n",
    "assert duplicates(df, None) == \"Name variables defining potential duplicates!\", \"T3.3 Failed: Handling None input\"\n",
    "assert type(duplicates(df, vars = df.columns.tolist())) == list, \"T3.4 Failed: Did not return a list for valid input\"\n",
    "result_list = duplicates(df, df.columns.tolist())\n",
    "assert isinstance(result_list, list) and all(isinstance(i, int) for i in result_list), \"T3.5 Failed: Return list does not contain all integers\"\n",
    "\n",
    "# If the script reaches here without an AssertionError, all tests passed.\n",
    "print(\"✅ All beginner duplicates function asserts passed silently!\")\n",
    "\n",
    "# Optional: You can still call the function outside asserts to see prints/results\n",
    "# print(\"\\n--- Manual Test ---\")\n",
    "# duplicate_rows = duplicates(df, VARIABLES)\n",
    "# print(\"Duplicate rows found:\", duplicate_rows)\n",
    "# duplicate_rows_invalid = duplicates(df, ['colA', 'non_existent_col'])\n",
    "# print(\"Result for invalid column:\", duplicate_rows_invalid) # This will now just print the return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da0c90261f277ba2850eb06ae49cf99a",
     "grade": false,
     "grade_id": "cell-360763185fff8ba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (Running placeholder tidy function) ---\n"
     ]
    }
   ],
   "source": [
    "df = tidy(mn);\n",
    "assert len(VARIABLES) > 0 and all([v in df.columns.tolist() for v in VARIABLES]), \"T3.1\"\n",
    "assert duplicates(df, [list]) == \"Name variables defining potential duplicates!\", \"T3.2\"\n",
    "assert duplicates(df, None) == \"Name variables defining potential duplicates!\", \"T3.3\"\n",
    "assert type(duplicates(df, vars = df.columns.tolist())) == list, \"T3.4\"\n",
    "assert all(isinstance(i, int) for i in duplicates(df, df.columns.tolist())), \"T3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51e366349b5d24359135760030a2731c",
     "grade": true,
     "grade_id": "cell-583bc8ab4ba38aa6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! 😼⚡️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5245abdf3b47f21f1fb0289ee8217bb",
     "grade": false,
     "grade_id": "cell-b04e3f6689a78a44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 4. Detecting outliers\n",
    "### 4.1. Code part\n",
    "Implement a function called `detecting_outliers` to detect outliers in one selected quantitative variable. Pick a suitable variable from the tidied dataset based on your characterisation and apply one suitable outlier-detection technique as covered in Unit 4. Justify your choice of this technique in the analytical part. Again, the function is assumed to receive a tidied data set from Step 0. The function returns the row positions (*not* row labels!) of the rows containing outliers on the selected variable, sorted in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c10ec065604cff85d708868fa0909ced",
     "grade": false,
     "grade_id": "cell-f593e79eae8f4227",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Main Tidy Function ---\n",
      "--- (Running placeholder tidy function) ---\n",
      "--- Tidy Function Completed ---\n",
      "Columns in tidied DataFrame: ['colA', 'colB', 'colC', 'colD']\n",
      "\n",
      "❌ FATAL: 'km_per_litre' column is missing even after running tidy(). Check the tidy() function implementation in cell 55.\n"
     ]
    }
   ],
   "source": [
    "# --- Task 4.1: Detecting Outliers ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume the 'tidy' function defined in cell 55 is available and correct.\n",
    "\n",
    "def detecting_outliers(input_df):\n",
    "    \"\"\"\n",
    "    Detects outliers in the 'km_per_litre' column using the IQR method.\n",
    "\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): The tidied DataFrame (expected to have 'km_per_litre').\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of row *positions* (0-based integers) containing outliers.\n",
    "              Returns an empty list if no outliers are found or the column is unsuitable.\n",
    "    \"\"\"\n",
    "    # --- Constants and Input Validation ---\n",
    "    COLUMN_TO_CHECK = \"km_per_litre\" # The quantitative variable chosen\n",
    "\n",
    "    # Work on a copy to avoid modifying the original DataFrame unexpectedly\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # Check if the chosen column exists in the dataframe\n",
    "    if COLUMN_TO_CHECK not in df.columns:\n",
    "        print(f\"❌ Error: Column '{COLUMN_TO_CHECK}' not found in the DataFrame!\")\n",
    "        print(f\"Available columns are: {df.columns.tolist()}\")\n",
    "        # Return an empty list as we cannot proceed\n",
    "        return []\n",
    "\n",
    "    print(f\"🔎 Checking for outliers in column: '{COLUMN_TO_CHECK}'\")\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    # Ensure the column is numeric. 'coerce' turns non-numeric values into NaN.\n",
    "    # It's crucial that this happens *before* calculating quantiles.\n",
    "    numeric_series = pd.to_numeric(df[COLUMN_TO_CHECK], errors='coerce')\n",
    "\n",
    "    # Check if the column became all NaNs after coercion (e.g., if it was all text)\n",
    "    if numeric_series.isnull().all():\n",
    "        print(f\"⚠️ Warning: Column '{COLUMN_TO_CHECK}' contains no valid numeric data after coercion. Cannot detect outliers.\")\n",
    "        return []\n",
    "\n",
    "    # --- IQR Calculation ---\n",
    "    # Calculate Q1 (25th percentile), Q3 (75th percentile)\n",
    "    # pandas' quantile method automatically handles NaNs present in the numeric_series\n",
    "    Q1 = numeric_series.quantile(0.25)\n",
    "    Q3 = numeric_series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Handle the edge case where IQR is 0 (e.g., many data points have the same value)\n",
    "    if IQR == 0:\n",
    "        print(f\"⚠️ Warning: IQR for '{COLUMN_TO_CHECK}' is zero. Outlier detection using 1.5*IQR rule might not be suitable.\")\n",
    "        # Outliers would only be values strictly outside Q1/Q3 if IQR is 0.\n",
    "        # We can proceed, but the bounds will just be Q1 and Q3.\n",
    "        # Alternatively, one might return [] or use a different method here.\n",
    "\n",
    "    # Define the outlier boundaries\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    print(f\"   - Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"   - Lower Bound: {lower_bound:.2f}, Upper Bound: {upper_bound:.2f}\")\n",
    "\n",
    "    # --- Identify Outliers ---\n",
    "    # Create a boolean mask (Series) indicating outlier status.\n",
    "    # A value is an outlier if it's below the lower bound OR above the upper bound.\n",
    "    # Crucially, ensure we only evaluate non-NaN values from the numeric series.\n",
    "    outlier_mask = ((numeric_series < lower_bound) | (numeric_series > upper_bound)) & numeric_series.notna()\n",
    "\n",
    "    # --- Get Row Positions (Corrected Method) ---\n",
    "    # Use np.where() on the boolean mask. It returns the indices (positions)\n",
    "    # where the condition (mask == True) is met.\n",
    "    # np.where returns a tuple of arrays (one for each dimension); we need the first element.\n",
    "    outlier_positions = np.where(outlier_mask)[0].tolist()\n",
    "\n",
    "    print(f\"📊 Found {len(outlier_positions)} outlier row positions.\")\n",
    "\n",
    "    # --- Return Sorted Positions ---\n",
    "    # np.where returns sorted positions, so no extra sorting needed.\n",
    "    return outlier_positions\n",
    "\n",
    "# --- Execution and Testing ---\n",
    "# IMPORTANT: Ensure the CORRECT 'tidy' function (from cell 55) is run before this cell!\n",
    "# Re-run cell 55 if needed.\n",
    "try:\n",
    "    print(\"--- Running Main Tidy Function ---\")\n",
    "    # Explicitly call the main tidy function again here to ensure we have the right df\n",
    "    df_tidied = tidy(mn)\n",
    "    print(\"--- Tidy Function Completed ---\")\n",
    "    print(\"Columns in tidied DataFrame:\", df_tidied.columns.tolist()) # Verify columns\n",
    "\n",
    "    # Check if 'km_per_litre' is actually present now\n",
    "    if 'km_per_litre' in df_tidied.columns:\n",
    "        print(f\"Column 'km_per_litre' dtype: {df_tidied['km_per_litre'].dtype}\")\n",
    "        print(\"Sample data from 'km_per_litre':\\n\", df_tidied['km_per_litre'].head())\n",
    "\n",
    "        # Call the outlier detection function with the correctly tidied DataFrame\n",
    "        print(\"\\n--- Running Outlier Detection ---\")\n",
    "        outliers = detecting_outliers(df_tidied)\n",
    "        print(\"--- Outlier Detection Completed ---\")\n",
    "\n",
    "        print(\"\\n--- Results ---\")\n",
    "        print(\"Detected outlier row positions:\", outliers)\n",
    "\n",
    "        # --- Assertions for Validation ---\n",
    "        print(\"\\n--- Running Assertions ---\")\n",
    "        assert isinstance(outliers, list), \"T4.1 Failed: Result is not a list.\"\n",
    "        print(\"✅ T4.1 Passed: Result type is list.\")\n",
    "\n",
    "        assert all(isinstance(i, int) for i in outliers), \"T4.2 Failed: Not all elements in the list are integers.\"\n",
    "        print(\"✅ T4.2 Passed: All elements are integers.\")\n",
    "\n",
    "        # Check the problematic assertion with more context\n",
    "        num_outliers = len(outliers)\n",
    "        data_size = df_tidied.shape[0]\n",
    "        min_expected = 1 # Assert requires at least one outlier\n",
    "        max_allowed_fraction = 0.05\n",
    "        max_expected = int(max_allowed_fraction * data_size) # Integer value for comparison\n",
    "\n",
    "        print(f\"Checking assertion: {min_expected} <= num_outliers ({num_outliers}) < {max_expected} ({max_allowed_fraction*100}% of {data_size})\")\n",
    "\n",
    "        assert num_outliers >= min_expected, f\"T4.3 Failed: Expected > 0 outliers, found {num_outliers}. Data might not have outliers by IQR rule.\"\n",
    "        print(f\"✅ T4.3 Passed: Found {num_outliers} outliers (>= {min_expected}).\")\n",
    "\n",
    "        assert num_outliers < data_size * max_allowed_fraction, f\"T4.4 Failed: Found {num_outliers} outliers, which is >= 5% ({max_allowed_fraction*100:.1f}%) of the data ({data_size}). Limit is {max_expected}.\"\n",
    "        print(f\"✅ T4.4 Passed: Number of outliers ({num_outliers}) is less than 5% of data size ({max_expected}).\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n❌ FATAL: 'km_per_litre' column is missing even after running tidy(). Check the tidy() function implementation in cell 55.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n❌ Error during tidy setup: {e}\")\n",
    "    print(\"   Please ensure the data file 'data/{mn}.csv' exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An unexpected error occurred during execution: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a687f420acb566a3dbe153acc0bfd16",
     "grade": true,
     "grade_id": "cell-231217b55b6ef843",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (Running placeholder tidy function) ---\n",
      "🔎 Checking for outliers in column: 'YOUR_COLUMN_NAME_HERE'\n",
      "❌ Error: Column 'YOUR_COLUMN_NAME_HERE' not found in the DataFrame!\n",
      "Available columns are: ['colA', 'colB', 'colC', 'colD']\n",
      "🔎 Checking for outliers in column: 'YOUR_COLUMN_NAME_HERE'\n",
      "❌ Error: Column 'YOUR_COLUMN_NAME_HERE' not found in the DataFrame!\n",
      "Available columns are: ['colA', 'colB', 'colC', 'colD']\n",
      "🔎 Checking for outliers in column: 'YOUR_COLUMN_NAME_HERE'\n",
      "❌ Error: Column 'YOUR_COLUMN_NAME_HERE' not found in the DataFrame!\n",
      "Available columns are: ['colA', 'colB', 'colC', 'colD']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(detecting_outliers(df)) == \u001b[38;5;28mlist\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mT4.1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m detecting_outliers(df)), \u001b[33m\"\u001b[39m\u001b[33mT4.2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(detecting_outliers(df)) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(detecting_outliers(df)) < \u001b[32m.05\u001b[39m*df.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df = tidy(mn);\n",
    "assert type(detecting_outliers(df)) == list, \"T4.1\"\n",
    "assert all(isinstance(i, int) for i in detecting_outliers(df)), \"T4.2\"\n",
    "assert len(detecting_outliers(df)) > 0 and len(detecting_outliers(df)) < .05*df.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e5ca4b902e63d96e5376adc95c8302d",
     "grade": false,
     "grade_id": "cell-8c4530b128b5c186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2. Analytical part\n",
    "Discuss the implications. \n",
    "\n",
    "- What is the chosen outlier-detection technique? Explain it using your own words in 3-4 sentences.\n",
    "- Describe the outliers detected: How many? How do they relate to the typical, non-outlier values in the remaining dataset?\n",
    "- What could be one reason these outliers appear in the dataset? How would you treat them further?\n",
    "\n",
    "Write your answer in the markdown cell below. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e93305db24e2fc3800f252b797c2ad7",
     "grade": true,
     "grade_id": "cell-254db63097c3ed40",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
