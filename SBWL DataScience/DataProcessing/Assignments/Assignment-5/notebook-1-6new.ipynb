{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08fecd06cada05dc2b74c825e68b8924",
     "grade": false,
     "grade_id": "cell-e37bf3469fc1e9d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). Do NOT add any cells to the notebook!\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or _YOUR ANSWER HERE_ , as well as your name and group below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "STUDENTID = \"\"\n",
    "GROUPID = \"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4a2a4b93859de036464fc9d6382039f",
     "grade": false,
     "grade_id": "cell-547da1b1777ed8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 5 (Group)\n",
    "In Assignment 2, as a group, you trained yourselves in accessing and characterising two data sources. You also sketched out a data-science project based on these data sources. In this assignment, based on this project idea, you should select, implement, and describe 3 appropriate visualisations.\n",
    "\n",
    "The following materials provide the necessary background:\n",
    "* the slide deck on visualisations (Unit 5) and the corresponding notebook;\n",
    "* Chapter 3 of \"Data Science from Scratch\"\n",
    "* the mandatory read on \"Task-Based Effectiveness of Basic Visualizations\" available from MyLearn: _B. Saket, A. Endert and Ã‡. Demiralp (2019), \"Task-Based Effectiveness of Basic Visualizations,\" in IEEE Transactions on Visualization and Computer Graphics, vol. 25, no. 7, pp. 2505-2512, DOI: 10.1109/TVCG.2018.2829750_\n",
    "\n",
    "Requirements:\n",
    "* Required reading: Study the paper by Saket et al. (2019) and explicitly refer to the task types, pros & cons of different visualisations as identified by these authors when answering the questions on this assignment.\n",
    "* The visualisation should be appropriate the chosen tasks on the data sets.\n",
    "* You should use at least two different types of visualisations. Even if two tasks in two steps below were identical (e.g., two aggregation tasks), you would be expected to choose a different visualisation for each. \n",
    "* As contrast to Assignment 2, you will be expected to use `pandas` to represent and to prepare the data sets for visualisation.\n",
    "* As for the data sets collected during Assignment 2, to avoid confusion:\n",
    "  * Use the genuine ones, not the manipulated ones (having anomalies introduced). \n",
    "  * If you have worked with excerpts (samples) from the original and genuine datasets, you can continue to use these. You are also free to use the complete datasets, but this is not expected.\n",
    "  * Please stick to your project description in Assignment 2 when choosing tasks and corresponding visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4e1ae2e0b2b37eae2abbc0f9d3f714d",
     "grade": false,
     "grade_id": "cell-41258b830f38673b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## Step 1 (6 points)\n",
    "\n",
    "Select, implement and describe one visualisation for data source 1 (in isolation from data source 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85e303e23b787c6ffa49284b473073db",
     "grade": true,
     "grade_id": "cell-f5c21aa256cedf80",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"data/data_notebook-notebook-1_dataset1.csv\")  # adjust filename if needed\n",
    "\n",
    "# Filter for 2014 and 2019\n",
    "df_2014 = df[df['year'] == 2014][['eli_pct', 'abmi_pct']].dropna()\n",
    "df_2019 = df[df['year'] == 2019][['eli_pct', 'abmi_pct']].dropna()\n",
    "\n",
    "# Manually compute Pearson correlation\n",
    "r_2014 = df_2014['eli_pct'].corr(df_2014['abmi_pct'])\n",
    "r_2019 = df_2019['eli_pct'].corr(df_2019['abmi_pct'])\n",
    "\n",
    "# Compute regression lines (y = mx + b)\n",
    "m_2014, b_2014 = np.polyfit(df_2014['eli_pct'], df_2014['abmi_pct'], 1)\n",
    "m_2019, b_2019 = np.polyfit(df_2019['eli_pct'], df_2019['abmi_pct'], 1)\n",
    "\n",
    "# Set up side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# 2014 plot\n",
    "axes[0].scatter(df_2014['eli_pct'], df_2014['abmi_pct'], alpha=0.5)\n",
    "axes[0].plot(df_2014['eli_pct'], m_2014 * df_2014['eli_pct'] + b_2014, color='red')\n",
    "axes[0].set_title(f'ELI % vs ABMI % (2014)\\nPearson r = {r_2014:.2f}')\n",
    "axes[0].set_xlabel('Extremely Low Income % (ELI)')\n",
    "axes[0].set_ylabel('Above Moderate Income % (ABMI)')\n",
    "axes[0].set_xlim(left=0)\n",
    "axes[0].set_ylim(bottom=0)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 2019 plot\n",
    "axes[1].scatter(df_2019['eli_pct'], df_2019['abmi_pct'], alpha=0.5, color='orange')\n",
    "axes[1].plot(df_2019['eli_pct'], m_2019 * df_2019['eli_pct'] + b_2019, color='red')\n",
    "axes[1].set_title(f'ELI % vs ABMI % (2019)\\nPearson r = {r_2019:.2f}')\n",
    "axes[1].set_xlabel('Extremely Low Income % (ELI)')\n",
    "axes[1].set_xlim(left=0)\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "#raise NotImplementedError()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and merge data\n",
    "df = pd.read_csv(\"final_processed_data.csv\")\n",
    "mapping_df = pd.read_csv(\"census_tract_incomes.csv\")\n",
    "\n",
    "mapping_df['geo_id2'] = mapping_df['geo_id2'].astype(str)\n",
    "df['geoid20'] = df['geoid20'].astype(str)\n",
    "df = df.merge(mapping_df, left_on='geoid20', right_on='geo_id2', how='left')\n",
    "df['county'] = df['name'].str.extract(r',\\s*(.*?)\\s*County', expand=False) + \" County\"\n",
    "\n",
    "# Extract short tract ID\n",
    "df['tract_id'] = df['geoid20'].str[-6:]\n",
    "\n",
    "# Calculate exclusive income group percentages\n",
    "df['ex_eli_pct'] = df['eli_pct']\n",
    "df['ex_vli_pct'] = df['vli_pct'] - df['eli_pct']\n",
    "df['ex_li_pct'] = df['li_pct'] - df['vli_pct']\n",
    "df['ex_mi_pct'] = df['mi_pct'] - df['li_pct']\n",
    "df['ex_abmi_pct'] = df['abmi_pct']  # assumed to be already exclusive\n",
    "\n",
    "# Filter for one specific tract (e.g., 128102)\n",
    "tract_df = df[df['tract_id'] == '128102'].sort_values('year')\n",
    "\n",
    "# Plot exclusive income groups over time as separate lines\n",
    "exclusive_cols = ['ex_eli_pct', 'ex_vli_pct', 'ex_li_pct', 'ex_mi_pct', 'ex_abmi_pct']\n",
    "labels = ['ELI', 'VLI', 'LI', 'MI', 'ABMI']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for col, label in zip(exclusive_cols, labels):\n",
    "    plt.plot(tract_df['year'], tract_df[col], marker='o', label=label)\n",
    "\n",
    "plt.title('Exclusive Income Group Trends Over Time\\nCensus Tract 1281.02, Los Angeles County')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Exclusive Share of Households (%)')\n",
    "plt.legend(title='Income Group')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "165b41745efd30d40fe52c446b458b2b",
     "grade": false,
     "grade_id": "cell-c98a5a86ee4b41f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Document your decision and describe the resulting visualisation. In your answer, cover the following aspects by referring explicitly to Saket et al. (2019):\n",
    "\n",
    "* What is the task according to Saket et al. (2019) on the data source supported by the chosen visualisation?\n",
    "* Why is the chosen visualisation effective for the given task?\n",
    "* What does the visualisation show exactly?\n",
    "* What does the visualisation contribute to answering your project's questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d12d6f369e868dd946a69ce1cc7ac7c",
     "grade": true,
     "grade_id": "cell-62681c4240d2770f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af76f135495844516d647c09c36e5d69",
     "grade": false,
     "grade_id": "cell-716d568e128ba2a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## Step 2 (6 points)\n",
    "\n",
    "Select, implement and describe one visualisation for data source 2 (in isolation from data source 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37fcd41ffd943e01cfe101fb587471af",
     "grade": true,
     "grade_id": "cell-06384a01f3b8fd45",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "import traceback # Import traceback for detailed error printing\n",
    "\n",
    "# --- Define File Path ---\n",
    "data_folder = 'data'\n",
    "# Make sure this filename matches exactly, including case sensitivity if needed\n",
    "file_name = 'data_notebook-notebook-1_dataset2.json'\n",
    "file_path = os.path.join(data_folder, file_name)\n",
    "\n",
    "try:\n",
    "    # --- Data Loading ---\n",
    "    print(f\"Attempting to load data from: {file_path}\")\n",
    "    df = pd.read_json(file_path)\n",
    "    print(f\"Successfully loaded {len(df)} records.\")\n",
    "\n",
    "    # --- Data Processing ---\n",
    "    required_cols = ['crm_cd_desc', 'date_occ']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # --- Date Conversion ---\n",
    "    print(\"\\n--- Converting Dates ---\")\n",
    "    df['date_occ'] = pd.to_datetime(df['date_occ'], errors='coerce')\n",
    "    nat_count = df['date_occ'].isna().sum()\n",
    "    if nat_count > 0:\n",
    "        print(f\"Warning: {nat_count} rows had invalid date formats and were coerced to NaT.\")\n",
    "    # Keep a copy before dropping NaNs for full date range calculation later\n",
    "    df_with_dates = df.dropna(subset=['date_occ']).copy()\n",
    "    print(f\"{len(df_with_dates)} records remaining after removing NaT dates.\")\n",
    "    if df_with_dates.empty: raise ValueError(\"No valid dates found after conversion.\")\n",
    "    min_date, max_date = df_with_dates['date_occ'].min(), df_with_dates['date_occ'].max()\n",
    "    print(f\"Full date range after conversion: {min_date} to {max_date}\")\n",
    "\n",
    "    # --- DIAGNOSTIC: Check 2013 data AFTER date cleaning ---\n",
    "    print(\"\\n--- Checking 2013 Data After Date Cleaning ---\")\n",
    "    df_2013_after_date_clean = df_with_dates[df_with_dates['date_occ'].dt.year == 2013].copy() # Use .copy()\n",
    "    print(f\"Number of records dated 2013 after date cleaning: {len(df_2013_after_date_clean)}\")\n",
    "    if not df_2013_after_date_clean.empty:\n",
    "        print(\"Sample 2013 crime descriptions (before top 5 filter):\")\n",
    "        print(df_2013_after_date_clean['crm_cd_desc'].value_counts().head(10))\n",
    "    # --- END DIAGNOSTIC ---\n",
    "\n",
    "    # --- Top 5 Crimes (Calculated on the *entire* valid date range) ---\n",
    "    # Drop NaNs in crime description *before* calculating top 5\n",
    "    df_cleaned = df_with_dates.dropna(subset=['crm_cd_desc']).copy()\n",
    "    print(f\"\\n{len(df_cleaned)} records remaining after removing NaN crime descriptions.\")\n",
    "    crime_counts = df_cleaned['crm_cd_desc'].value_counts()\n",
    "    if crime_counts.empty: raise ValueError(\"No crime descriptions found.\")\n",
    "    top_5_crime_list = crime_counts.head(5).index.tolist()\n",
    "    print(f\"\\nOverall Top 5 Crime Types (based on {min_date.year}-{max_date.year} dataset): {top_5_crime_list}\")\n",
    "\n",
    "    # --- Filter DataFrame for Top 5 Crimes ---\n",
    "    df_top5 = df_cleaned[df_cleaned['crm_cd_desc'].isin(top_5_crime_list)].copy()\n",
    "    print(f\"\\n{len(df_top5)} records belong to the overall top 5 crime types.\")\n",
    "    if df_top5.empty: raise ValueError(\"No records found matching the top 5 crimes.\")\n",
    "\n",
    "    # --- DIAGNOSTIC: Check 2013 data AFTER top 5 filter ---\n",
    "    print(\"\\n--- Checking 2013 Data After Top 5 Filter ---\")\n",
    "    df_top5_2013 = df_top5[df_top5['date_occ'].dt.year == 2013]\n",
    "    print(f\"Number of Top 5 crime records dated 2013: {len(df_top5_2013)}\")\n",
    "    if not df_top5_2013.empty:\n",
    "         print(\"Crime descriptions in 2013 (Top 5 Filtered):\")\n",
    "         print(df_top5_2013['crm_cd_desc'].value_counts())\n",
    "    else:\n",
    "         print(\"Conclusion: No records from 2013 matched the overall Top 5 crime list.\")\n",
    "         if not df_2013_after_date_clean.empty:\n",
    "             print(\"This means the crimes that *did* occur in 2013 were not among the overall top 5.\")\n",
    "    # --- END DIAGNOSTIC ---\n",
    "\n",
    "    # --- Aggregate Yearly ---\n",
    "    df_top5.set_index('date_occ', inplace=True)\n",
    "    if not isinstance(df_top5.index, pd.DatetimeIndex):\n",
    "         raise TypeError(\"Index is not DatetimeIndex before resampling.\")\n",
    "\n",
    "    # --- DIAGNOSTIC: Check index before resampling ---\n",
    "    print(\"\\n--- Index Check Before Resampling ---\")\n",
    "    if not df_top5.empty:\n",
    "        print(f\"Index min: {df_top5.index.min()}, Index max: {df_top5.index.max()}\")\n",
    "        print(f\"Is 2013 present in the index of df_top5? {'Yes' if any(df_top5.index.year == 2013) else 'No'}\")\n",
    "    else:\n",
    "        print(\"df_top5 is empty before resampling.\")\n",
    "    # --- END DIAGNOSTIC ---\n",
    "\n",
    "    crime_over_time = df_top5.groupby([pd.Grouper(freq='YE'), 'crm_cd_desc']).size()\n",
    "\n",
    "    # --- Restructure ---\n",
    "    crime_pivot = crime_over_time.unstack(level='crm_cd_desc', fill_value=0)\n",
    "    for crime in top_5_crime_list:\n",
    "        if crime not in crime_pivot.columns: crime_pivot[crime] = 0\n",
    "\n",
    "    # --- REINDEX TO FULL DATE RANGE ---\n",
    "    # Use the min/max dates calculated earlier from df_with_dates\n",
    "    print(\"\\n--- Reindexing Pivot Table to Full Date Range ---\")\n",
    "    if not df_with_dates.empty:\n",
    "        # Create a complete DatetimeIndex for every year end in the full range\n",
    "        all_years_index = pd.date_range(start=min_date, end=max_date, freq='YE')\n",
    "\n",
    "        print(f\"Original pivot index range: {crime_pivot.index.min().year if not crime_pivot.empty else 'N/A'} to {crime_pivot.index.max().year if not crime_pivot.empty else 'N/A'}\")\n",
    "        print(f\"Expected full range based on data: {all_years_index.min().year} to {all_years_index.max().year}\")\n",
    "\n",
    "        # Reindex the pivot table using the complete yearly index.\n",
    "        # Missing years will be added with the specified fill_value (0).\n",
    "        crime_pivot = crime_pivot.reindex(all_years_index, fill_value=0)\n",
    "\n",
    "        print(\"Pivot table index after reindexing:\")\n",
    "        print(crime_pivot.index)\n",
    "        print(\"\\nFirst few rows after reindexing (should include earliest years):\")\n",
    "        print(crime_pivot.head())\n",
    "    else:\n",
    "        print(\"Skipping reindexing because initial date processing failed.\")\n",
    "    # --- END REINDEX ---\n",
    "\n",
    "\n",
    "    # --- DIAGNOSTIC: Check Pivot Table for 2013 ---\n",
    "    print(\"\\n--- Checking Pivot Table for 2013 (After Reindex) ---\")\n",
    "    year_end_2013 = pd.Timestamp('2013-12-31')\n",
    "    if year_end_2013 in crime_pivot.index:\n",
    "        print(\"Row for 2013-12-31 in final pivot table:\")\n",
    "        print(crime_pivot.loc[year_end_2013])\n",
    "    else:\n",
    "        # This should only happen now if 2013 was outside the min/max date range\n",
    "        print(\"No index entry found for 2013-12-31 in the final pivot table.\")\n",
    "    # --- END DIAGNOSTIC ---\n",
    "\n",
    "\n",
    "    # --- Pre-Plot Checks ---\n",
    "    print(\"\\n--- Pre-Plot Data Check ---\")\n",
    "    print(\"Pivot table info:\")\n",
    "    crime_pivot.info()\n",
    "    print(\"\\nFirst 5 rows of pivot table:\")\n",
    "    print(crime_pivot.head())\n",
    "    print(f\"\\nPivot index type: {type(crime_pivot.index)}, dtype: {crime_pivot.index.dtype}\")\n",
    "    if crime_pivot.empty: raise ValueError(\"Pivot table is empty.\")\n",
    "    if not isinstance(crime_pivot.index, pd.DatetimeIndex): print(\"Warning: Pivot index not DatetimeIndex.\")\n",
    "    # Check if all plotted values are zero AFTER reindexing\n",
    "    if not crime_pivot.empty and (crime_pivot[top_5_crime_list].values == 0).all():\n",
    "        print(\"Warning: All data values in the pivot table for top 5 crimes are zero (possibly after reindexing).\")\n",
    "\n",
    "\n",
    "    # --- Visualization ---\n",
    "    print(\"\\n--- Plotting ---\")\n",
    "    if crime_pivot.empty:\n",
    "        print(\"Cannot plot, pivot table is empty.\")\n",
    "    else:\n",
    "        # Get range from the potentially reindexed pivot table\n",
    "        plot_min_year = crime_pivot.index.min().year\n",
    "        plot_max_year = crime_pivot.index.max().year\n",
    "        print(f\"Plotting data from {plot_min_year} to {plot_max_year}\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "        # Plot using ax.plot directly\n",
    "        markers = ['o', 's', '^', 'D', 'v'] # Different marker for each line\n",
    "        for i, crime in enumerate(top_5_crime_list):\n",
    "            if crime in crime_pivot.columns:\n",
    "                 # Ensure index and values are passed correctly\n",
    "                 ax.plot(crime_pivot.index, crime_pivot[crime], marker=markers[i % len(markers)], linestyle='-', label=crime)\n",
    "            else:\n",
    "                 print(f\"Warning: Column '{crime}' not found in pivot table for plotting.\")\n",
    "\n",
    "        # --- Customize Plot ---\n",
    "        ax.set_title('Yearly Frequency of Top 5 Crime Types Over Time')\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.set_ylabel('Number of Reported Incidents')\n",
    "        ax.legend(title='Crime Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax.grid(True, linestyle='--', which='major', color='grey', alpha=.25)\n",
    "        ax.yaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "        ax.set_ylim(bottom=0) # Keep Y starting at 0\n",
    "\n",
    "        # --- X-axis Limit and Tick Handling ---\n",
    "        # Set the locator and formatter *after* plotting\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator(base=1)) # Ensure ticks are on the year\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "        # Optional: Explicitly set limits if needed, using padding\n",
    "        # plot_start_date = crime_pivot.index.min() - pd.Timedelta(days=180)\n",
    "        # plot_end_date = crime_pivot.index.max() + pd.Timedelta(days=180)\n",
    "        # ax.set_xlim(plot_start_date, plot_end_date)\n",
    "        # # Re-apply locator/formatter if xlim changed things significantly\n",
    "        # ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "        # ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "        plt.xticks(rotation=0, ha='center') # Horizontal labels for years\n",
    "        # Adjust layout to prevent labels/legend overlapping\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1]) # Shrink plot area slightly for legend\n",
    "        plt.show()\n",
    "\n",
    "# --- Error Handling ---\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at '{file_path}'\")\n",
    "    print(\"Please ensure the 'data' folder exists in the same directory and the filename is correct.\")\n",
    "except ValueError as ve:\n",
    "    print(f\"Data Processing Error: {ve}\")\n",
    "    traceback.print_exc() # Show traceback for ValueErrors\n",
    "except TypeError as te:\n",
    "     print(f\"Type Error: {te}\")\n",
    "     traceback.print_exc() # Show traceback for TypeErrors\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    traceback.print_exc() # Print detailed traceback for any other errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b92605cf904f338b840d4e92fb0ab93",
     "grade": false,
     "grade_id": "cell-773b0383bfb8ea05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Document your decision and describe the resulting visualisation. In your answer, cover the following aspects by referring explicitly to Saket et al. (2019):\n",
    "\n",
    "* What is the task according to Saket et al. (2019) on the data source supported by the chosen visualisation?\n",
    "* Why is the chosen visualisation effective for the given task?\n",
    "* What does the visualisation show exactly?\n",
    "* What does the visualisation contribute to answering your project's questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70f9c93b079b85b8ff73115f2d84bc55",
     "grade": true,
     "grade_id": "cell-55569ce66deb1db2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed58851b8a05d2f85ce175ca5557eb21",
     "grade": false,
     "grade_id": "cell-41258b830f38673baa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## Step 3  (7 points)\n",
    "\n",
    "Merge the two data sets (or, relevant subsets thereof) based on your project idea from Assignment 2. Select, implement and describe one visualisation on the combined data set. Make sure you visualize variables taken from both original data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3aaf3a45eb8d36949f9e02fcb3dfea97",
     "grade": true,
     "grade_id": "cell-b485c10ffce5955c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Optional, for potentially nicer plots\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "print(\"Starting streamlined script...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Configuration & Setup\n",
    "# ==============================================================================\n",
    "try:\n",
    "    CWD = pathlib.Path(os.getcwd())\n",
    "    DATA_DIR = CWD / \"data\"\n",
    "    if not DATA_DIR.is_dir():\n",
    "        raise FileNotFoundError(f\"Data directory '{DATA_DIR}' not found in '{CWD}'.\")\n",
    "\n",
    "    CRIME_JSON_PATH = DATA_DIR / \"data_notebook-notebook-1_dataset2.json\"\n",
    "    CENTROIDS_CSV_PATH = DATA_DIR / \"CA_tract_centroids_2020.csv.txt\"\n",
    "    INCOME_CSV_PATH = DATA_DIR / \"data_notebook-notebook-1_dataset1.csv\"\n",
    "    PROJECTED_CRS = \"EPSG:3310\" # NAD83 / California Albers\n",
    "    NUM_TOP_TRACTS = 5\n",
    "    NUM_TOP_CRIMES_TO_PLOT = 5\n",
    "    YEARS_TO_PLOT = range(2013, 2019 + 1) # Define the years for plotting\n",
    "\n",
    "    print(f\"Using data directory: {DATA_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during setup: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: Load Data & Calculate Top Financially Changing Tracts\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Processing Financial & Geographic Data ---\")\n",
    "\n",
    "top_geoids = []\n",
    "top_changers_lookup = pd.DataFrame() # Initialize empty\n",
    "\n",
    "try:\n",
    "    # --- Load Centroids ---\n",
    "    print(f\"Loading centroids: {CENTROIDS_CSV_PATH}\")\n",
    "    centroids_df = pd.read_csv(CENTROIDS_CSV_PATH)\n",
    "    required_centroid_cols = ['STATEFP', 'COUNTYFP', 'TRACTCE', 'LATITUDE', 'LONGITUDE']\n",
    "    if not all(col in centroids_df.columns for col in required_centroid_cols):\n",
    "        missing_cols = [col for col in required_centroid_cols if col not in centroids_df.columns]\n",
    "        raise ValueError(f\"Centroids CSV missing required columns: {missing_cols}\")\n",
    "\n",
    "    centroids_df['geoid'] = (\n",
    "        centroids_df['STATEFP'].astype(str).str.zfill(2) +\n",
    "        centroids_df['COUNTYFP'].astype(str).str.zfill(3) +\n",
    "        centroids_df['TRACTCE'].astype(str).str.zfill(6)\n",
    "    )\n",
    "    # Keep only necessary centroid info for later join and financial part\n",
    "    centroids_lookup_df = centroids_df[['geoid', 'LATITUDE', 'LONGITUDE']].copy()\n",
    "    print(f\"Loaded and prepared {len(centroids_lookup_df)} centroid records.\")\n",
    "\n",
    "    # --- Load Income Data & Calculate Financial Score Change ---\n",
    "    print(f\"Loading income data: {INCOME_CSV_PATH}\")\n",
    "    df_income = pd.read_csv(INCOME_CSV_PATH)\n",
    "    required_income_cols = ['eli_pct', 'vli_pct', 'li_pct', 'mi_pct', 'abmi_pct', 'geoid', 'year']\n",
    "    if not all(col in df_income.columns for col in required_income_cols):\n",
    "        missing = [col for col in required_income_cols if col not in df_income.columns]\n",
    "        raise ValueError(f\"Income CSV missing required columns: {missing}\")\n",
    "    print(f\"Loaded {len(df_income)} income records.\")\n",
    "\n",
    "    # Calculate exclusive percentages\n",
    "    df_income['ex_eli_pct'] = df_income['eli_pct']\n",
    "    df_income['ex_vli_pct'] = df_income['vli_pct'] - df_income['eli_pct']\n",
    "    df_income['ex_li_pct']  = df_income['li_pct']  - df_income['vli_pct']\n",
    "    df_income['ex_mi_pct']  = df_income['mi_pct']  - df_income['li_pct']\n",
    "    df_income['ex_abmi_pct'] = df_income['abmi_pct']\n",
    "\n",
    "    # Define weights\n",
    "    score_map = {'ex_eli_pct': -1.0, 'ex_vli_pct': -0.5, 'ex_li_pct': 0.0, 'ex_mi_pct': 0.5, 'ex_abmi_pct': 1.0}\n",
    "\n",
    "    # Calculate financial score\n",
    "    df_income['financial_score'] = sum(df_income[col] * weight for col, weight in score_map.items()) / 100\n",
    "\n",
    "    # Compute score change\n",
    "    df_income['year'] = pd.to_numeric(df_income['year'], errors='coerce').dropna().astype(int)\n",
    "    df_sorted = df_income.sort_values(['geoid', 'year'])\n",
    "    first_scores = df_sorted.loc[df_sorted.groupby('geoid')['year'].idxmin()]\n",
    "    last_scores = df_sorted.loc[df_sorted.groupby('geoid')['year'].idxmax()]\n",
    "\n",
    "    score_change = pd.merge(\n",
    "        first_scores[['geoid', 'financial_score', 'year']].rename(columns={'financial_score': 'first_score', 'year': 'first_year'}),\n",
    "        last_scores[['geoid', 'financial_score', 'year']].rename(columns={'financial_score': 'last_score', 'year': 'last_year'}),\n",
    "        on='geoid', how='inner'\n",
    "    )\n",
    "    score_change = score_change[score_change['first_year'] != score_change['last_year']].copy() # Ensure we have >1 year\n",
    "    score_change['change'] = score_change['last_score'] - score_change['first_score']\n",
    "\n",
    "    # Get top N changing tracts\n",
    "    top_changers = score_change.reindex(score_change['change'].abs().sort_values(ascending=False).index).head(NUM_TOP_TRACTS)\n",
    "\n",
    "    # Extract clean 11-digit geoid if necessary (handle different formats)\n",
    "    top_changers['geoid_clean'] = top_changers['geoid'].astype(str).str.extract(r'(\\d{11})').fillna(top_changers['geoid'])\n",
    "    centroids_lookup_df['geoid'] = centroids_lookup_df['geoid'].astype(str) # Ensure string for merge\n",
    "\n",
    "    # Create the lookup table for plotting titles (geoid -> change)\n",
    "    top_changers_lookup = top_changers.set_index('geoid_clean')[['change', 'first_year', 'last_year']] # Use clean geoid as index\n",
    "    top_geoids = top_changers_lookup.index.tolist() # List of the top clean geoids\n",
    "\n",
    "    if not top_geoids:\n",
    "         print(\"Warning: Could not identify any tracts with score changes over multiple years.\")\n",
    "    else:\n",
    "        print(f\"\\nIdentified Top {len(top_geoids)} Tracts by Financial Change:\")\n",
    "        # Optional: Print the top changers summary if needed for context\n",
    "        # print(top_changers[['geoid_clean', 'change', 'first_year', 'last_year']].round(4))\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"Data Error: {e}\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing expected column - {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during financial/geographic data processing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: Load Crime Data, Perform Spatial Join, and Filter\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Processing Crime Data & Performing Spatial Join ---\")\n",
    "\n",
    "crimes_top_tracts = pd.DataFrame() # Initialize empty\n",
    "\n",
    "# Only proceed if we found top geoids\n",
    "if top_geoids:\n",
    "    try:\n",
    "        # --- Load Crime Data ---\n",
    "        print(f\"Loading crime data: {CRIME_JSON_PATH}\")\n",
    "        try:\n",
    "            with open(CRIME_JSON_PATH, 'r') as f: crime_data_list = json.load(f)\n",
    "        except json.JSONDecodeError: # Try JSON Lines format\n",
    "            print(\"Attempting to read crime data as JSON Lines.\")\n",
    "            crime_data_list = []\n",
    "            with open(CRIME_JSON_PATH, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        try: crime_data_list.append(json.loads(line.strip()))\n",
    "                        except json.JSONDecodeError as e_line: print(f\"Skipping invalid JSON line: {e_line}\")\n",
    "        if not crime_data_list: raise ValueError(\"No valid crime data loaded from JSON.\")\n",
    "        crime_df = pd.DataFrame(crime_data_list)\n",
    "        print(f\"Loaded {len(crime_df)} potential crime records.\")\n",
    "\n",
    "        # --- Prepare Crime Coordinates ---\n",
    "        lat_col = next((col for col in crime_df.columns if col.lower() == 'lat'), 'lat')\n",
    "        lon_col = next((col for col in crime_df.columns if col.lower() == 'lon'), 'lon')\n",
    "        if lat_col not in crime_df.columns or lon_col not in crime_df.columns:\n",
    "             raise ValueError(\"Crime data missing latitude/longitude columns.\")\n",
    "        crime_df.rename(columns={lat_col: 'lat', lon_col: 'lon'}, inplace=True)\n",
    "\n",
    "        crime_df['lat'] = pd.to_numeric(crime_df['lat'].astype(str).str.replace('\"', ''), errors='coerce')\n",
    "        crime_df['lon'] = pd.to_numeric(crime_df['lon'].astype(str).str.replace('\"', ''), errors='coerce')\n",
    "        crime_df.dropna(subset=['lat', 'lon'], inplace=True)\n",
    "        if crime_df.empty: raise ValueError(\"No crime records with valid coordinates.\")\n",
    "        print(f\"Prepared coordinates for {len(crime_df)} crime records.\")\n",
    "\n",
    "        # --- Create GeoDataFrames & Reproject ---\n",
    "        print(\"Creating GeoDataFrames and reprojecting...\")\n",
    "        crime_gdf = gpd.GeoDataFrame(\n",
    "            crime_df, geometry=gpd.points_from_xy(crime_df.lon, crime_df.lat), crs=\"EPSG:4326\"\n",
    "        )\n",
    "        centroids_gdf = gpd.GeoDataFrame(\n",
    "            centroids_lookup_df, geometry=gpd.points_from_xy(centroids_lookup_df.LONGITUDE, centroids_lookup_df.LATITUDE), crs=\"EPSG:4326\"\n",
    "        )\n",
    "\n",
    "        crime_gdf_proj = crime_gdf.to_crs(PROJECTED_CRS)\n",
    "        centroids_gdf_proj = centroids_gdf.to_crs(PROJECTED_CRS)\n",
    "        print(\"Reprojection complete.\")\n",
    "\n",
    "        # --- Perform Spatial Join ---\n",
    "        print(\"Performing spatial join (assigning geoid to crimes)...\")\n",
    "        # Ensure centroids_gdf has the correct index corresponding to centroids_gdf_proj\n",
    "        centroids_gdf = centroids_gdf.reset_index(drop=True)\n",
    "        crimes_joined_proj = gpd.sjoin_nearest(\n",
    "            crime_gdf_proj,\n",
    "            centroids_gdf_proj[['geometry']], # Join only on geometry\n",
    "            how='left'\n",
    "            # distance_col=\"distance\" # Distance calculation removed as it's not needed for plots\n",
    "        )\n",
    "\n",
    "        # Merge the geoid back using the index from the join\n",
    "        # Note: 'index_right' refers to the index of centroids_gdf_proj, which matches centroids_gdf after reset_index\n",
    "        crimes_with_geoids_gdf = crimes_joined_proj.merge(\n",
    "            centroids_gdf[['geoid']], # Only need the geoid column\n",
    "            left_on='index_right',\n",
    "            right_index=True,\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"Spatial join complete. Assigned geoids to {len(crimes_with_geoids_gdf)} crimes.\")\n",
    "\n",
    "        # --- Prepare Joined Data for Plotting ---\n",
    "        # Keep only essential columns\n",
    "        essential_cols = ['geoid', 'date_occ', 'crm_cd_desc']\n",
    "        missing_essential = [col for col in essential_cols if col not in crimes_with_geoids_gdf.columns]\n",
    "        if missing_essential:\n",
    "            raise ValueError(f\"Joined crime data missing essential columns: {missing_essential}\")\n",
    "\n",
    "        crimes_plotting_df = crimes_with_geoids_gdf[essential_cols].copy()\n",
    "\n",
    "        # Convert date and extract year\n",
    "        crimes_plotting_df['date_occ'] = pd.to_datetime(crimes_plotting_df['date_occ'], errors='coerce')\n",
    "        crimes_plotting_df.dropna(subset=['date_occ'], inplace=True)\n",
    "        crimes_plotting_df['year'] = crimes_plotting_df['date_occ'].dt.year\n",
    "\n",
    "        # Clean crime description\n",
    "        crimes_plotting_df['crm_cd_desc'] = crimes_plotting_df['crm_cd_desc'].fillna('UNKNOWN').str.strip()\n",
    "\n",
    "        # Filter for the top geoids identified earlier\n",
    "        crimes_top_tracts = crimes_plotting_df[crimes_plotting_df['geoid'].isin(top_geoids)].copy()\n",
    "        print(f\"Filtered down to {len(crimes_top_tracts)} crime records in the top {len(top_geoids)} tracts.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        # No exit here, plotting loop will just find no data\n",
    "    except ValueError as e:\n",
    "        print(f\"Data Error: {e}\")\n",
    "        # No exit, plotting loop might be skipped\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing expected column during crime processing - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during crime data processing or spatial join: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Let script continue to plotting part, which might show nothing\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: Analyze and Plot Crime Trends for Top Tracts\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- Generating Crime Trend Plots for Top {len(top_geoids)} Tracts ---\")\n",
    "\n",
    "if crimes_top_tracts.empty:\n",
    "    if not top_geoids:\n",
    "        print(\"No top geoids identified in Part 1. Cannot generate plots.\")\n",
    "    else:\n",
    "        print(f\"No crime records found matching the top {len(top_geoids)} geoids. Cannot generate plots.\")\n",
    "else:\n",
    "    # Generate Plot for Each Top Geoid\n",
    "    for geoid in top_geoids:\n",
    "        print(f\"\\nAnalyzing GEOID: {geoid}\")\n",
    "        tract_crimes = crimes_top_tracts[crimes_top_tracts['geoid'] == geoid].copy()\n",
    "\n",
    "        if tract_crimes.empty:\n",
    "            print(f\"  No crime data found for GEOID {geoid}. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        # --- Get financial score change info for the title ---\n",
    "        change_text = \"Financial Score Change: N/A\" # Default\n",
    "        if not top_changers_lookup.empty and geoid in top_changers_lookup.index:\n",
    "            change_info = top_changers_lookup.loc[geoid]\n",
    "            change_text = f\"Financial Score Change ({int(change_info['first_year'])}-{int(change_info['last_year'])}): {change_info['change']:+.2f}\"\n",
    "        else:\n",
    "             print(f\"  Warning: Could not find financial score change for GEOID {geoid} in lookup.\")\n",
    "\n",
    "        # --- Aggregate Crime Data ---\n",
    "        # Find the overall top N crime types *for this specific tract*\n",
    "        top_crimes_in_tract = tract_crimes['crm_cd_desc'].value_counts().head(NUM_TOP_CRIMES_TO_PLOT).index.tolist()\n",
    "        print(f\"  Top {len(top_crimes_in_tract)} crime types in this tract: {top_crimes_in_tract}\")\n",
    "\n",
    "        # Filter tract data for these top crimes\n",
    "        tract_top_crimes_df = tract_crimes[tract_crimes['crm_cd_desc'].isin(top_crimes_in_tract)]\n",
    "\n",
    "        # Group by year and crime description, count occurrences\n",
    "        crime_counts_yearly = tract_top_crimes_df.groupby(['year', 'crm_cd_desc']).size().reset_index(name='count')\n",
    "\n",
    "        # Pivot for plotting\n",
    "        try:\n",
    "            crime_pivot = crime_counts_yearly.pivot_table(\n",
    "                index='year', columns='crm_cd_desc', values='count', fill_value=0\n",
    "            )\n",
    "            # Ensure all years in the desired range are present\n",
    "            crime_pivot = crime_pivot.reindex(YEARS_TO_PLOT, fill_value=0).sort_index()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not pivot/reindex data for GEOID {geoid}. Error: {e}. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        if crime_pivot.empty:\n",
    "             print(f\"  Pivoted crime data is empty for GEOID {geoid}. Skipping plot.\")\n",
    "             continue\n",
    "\n",
    "        # --- Plotting ---\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        colors = sns.color_palette('tab10', n_colors=len(crime_pivot.columns)) # Use seaborn palettes if available\n",
    "        for i, crime_type in enumerate(crime_pivot.columns):\n",
    "            plt.plot(crime_pivot.index, crime_pivot[crime_type], marker='o', linestyle='-', label=crime_type, color=colors[i])\n",
    "\n",
    "        plt.title(\n",
    "            f\"Top {len(crime_pivot.columns)} Crime Trends ({YEARS_TO_PLOT.start}-{YEARS_TO_PLOT.stop-1}) - GEOID: {geoid}\\n{change_text}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        plt.xlabel(\"Year\", fontsize=12)\n",
    "        plt.ylabel(\"Number of Reported Incidents\", fontsize=12)\n",
    "        plt.legend(title='Crime Type', loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "        plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "        # Dynamically set x-axis limits based on YEARS_TO_PLOT\n",
    "        plt.xlim(YEARS_TO_PLOT.start - 0.5, YEARS_TO_PLOT.stop - 1 + 0.5)\n",
    "        plt.xticks(list(YEARS_TO_PLOT)) # Ensure ticks cover the full range plotted\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
    "        sns.despine() # Optional: remove top/right plot spines\n",
    "\n",
    "        # Save plot (optional)\n",
    "        # plot_filename = DATA_DIR / f\"crime_trends_geoid_{geoid}_{YEARS_TO_PLOT.start}-{YEARS_TO_PLOT.stop-1}_score.png\"\n",
    "        # try:\n",
    "        #     plt.savefig(plot_filename, bbox_inches='tight')\n",
    "        #     print(f\"  Saved plot to {plot_filename}\")\n",
    "        # except Exception as e_save:\n",
    "        #     print(f\"  Error saving plot: {e_save}\")\n",
    "\n",
    "        plt.show() # Display the plot\n",
    "\n",
    "print(\"\\n--- Script finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce0ff1f313f756be08f5e6f6c4a9f356",
     "grade": false,
     "grade_id": "cell-3c0ae5fb02e6f352",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Document your decision and describe the resulting visualisation. In your answer, cover the following aspects by referring explicitly to Saket et al. (2019):\n",
    "\n",
    "* What is the task according to Saket et al. (2019) on the combined data set supported by the chosen visualisation?\n",
    "* Why is the chosen visualisation effective for the given task?\n",
    "* What does the visualisation show exactly?\n",
    "* What does the visualisation contribute to answering your project's questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93cf6e7530dfb8ecc91c0f6b7496d1d6",
     "grade": true,
     "grade_id": "cell-bde7e30e877919a8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c527eeae484320ff379c8835a02686a",
     "grade": false,
     "grade_id": "cell-1c3320479f4749e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## Step 4  (1 points)\n",
    "\n",
    "Persist the merged dataset from Step 3 as a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cafab2e2a0caec0b47775cd216004fc",
     "grade": true,
     "grade_id": "cell-7df195f7e47ef8d0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
