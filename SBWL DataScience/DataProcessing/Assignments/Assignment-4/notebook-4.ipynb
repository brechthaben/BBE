{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f84edb5e70e5c8cbd2d3004c9b67ceaf",
     "grade": false,
     "grade_id": "cell-9e78272cc4af091a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4\n",
    "***\n",
    "*General hints:* <br>\n",
    "* You may use another notebook to test different approaches and ideas. When complete and mature, turn your code snippets into the requested functions in this notebook for submission. \n",
    "* Make sure the function implementations are generic and can be applied to any dataset (not just the one provided).\n",
    "* Add explanatory code comments in the code cells. Make sure that these comments improve our understanding of your implementation decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca384ab2b432240f2e75945bb1878249",
     "grade": false,
     "grade_id": "cell-9d52cb434e7f0910",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "* Create a variable holding your student id, as shown below. \n",
    "* Simply replace the example (`01234567`) with your actual student id having a total of 8 digits. \n",
    "* Maintain the variable as a string, do NOT change its type in this notebook!\n",
    "* *Note: If your student id has 7 digits, add a leading 0. The final student id MUST have 8 digits!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = '12318768'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95db3692d001bdd0ce4a073305285b20",
     "grade": false,
     "grade_id": "cell-98bb586658e7b54d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dce49140be8025de8894a25a2b824268",
     "grade": false,
     "grade_id": "cell-420b4c449379ffe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 0. Import\n",
    "\n",
    "Implement a function `tidy` which imports the data set assigned and provided to you as a CSV file into a `pandas` dataframe. Access the data set and establish whether your data set is tidy. If not, clean the data set before continuing with Step 1. Mind all rules of tidying data sets in this step. Make sure you comply to the following statements:\n",
    "* If there is an index column (row numbers) in your tidied dataset, keep it.\n",
    "* The following columns, once identified, correspond to variables 1:1 (no need for transformations):\n",
    "  * `full_name`\n",
    "  * `automotive`\n",
    "  * `color`\n",
    "  * `job`\n",
    "  * `address`\n",
    "  * `coordinates`\n",
    "  * `km_per_litre`\n",
    "* The tidied dataset should have a total of 9 columns (not including the index), the first column should be `full_name` and the last one `km_per_litre`.\n",
    "* Mind the intended content of each attribute (e.g. `full_name` should contain the full name of a person, no need to change that)\n",
    "* If tidy or done, have the function `tidy` return the ready data set as a dataframe.\n",
    "\n",
    "Note that `tidy` must take a single parameter that holds your student id (`mn`) as one part of the basename (according to the CoC) of the CSV file (i.e., the CoC file name without file extension). Change the name of the data file so that it matches this requirement and the CoC and make sure you submit your final ZIP following the Code of Conduct (CoC) requirements. Especially, make sure you put your data file in a folder called `data/` when submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e96fea9cc9f84652b592c3659339a48b",
     "grade": false,
     "grade_id": "cell-81e21dccd785e63d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tidy(x): #this function will tidy the data\n",
    "    file_path = f\"data/{x}.csv\" #first we fetch the file path\n",
    "    my_data_frame = pd.read_csv(file_path, header=None) #then we read the file\n",
    "\n",
    "    my_data_frame_indexed = my_data_frame.set_index(0) #we set the first column as the index\n",
    "    df = my_data_frame_indexed.T # and after looking at the data it seems to be transposed, so we transpose it again \n",
    "\n",
    "    first_col_name = df.columns[0] #now we can see that the first column is the full name of the person\n",
    "    if pd.isna(first_col_name): #if the first column name is missing, we drop it\n",
    "        df = df.drop(columns=[first_col_name])\n",
    "\n",
    "    combined_column = df[\"date_time/full_company_name\"] #now lets start splitting the name and date column\n",
    "\n",
    "    date_time_part = combined_column.str[:26] # after looking at the data we can see that the date and time are the first 26 characters, so we can split it after 26 characters\n",
    "    company_name_part = combined_column.str[26:] #and the rest is the company name\n",
    "\n",
    "    df[\"date_time\"] = date_time_part.str.strip() #now we can create the new columns, that are stripped\n",
    "    df[\"company_name\"] = company_name_part.str.strip() #and once again\n",
    "\n",
    "    df = df.drop(columns=[\"date_time/full_company_name\"]) #and now we remove the original column\n",
    "\n",
    "    desired_column_order = [ #lets reorder our headers, so they match the asignment\n",
    "        \"full_name\",\n",
    "        \"automotive\",\n",
    "        \"color\",\n",
    "        \"job\",\n",
    "        \"address\",\n",
    "        \"coordinates\",\n",
    "        \"date_time\",      # New column\n",
    "        \"company_name\",   # New column\n",
    "        \"km_per_litre\"    # and lets Ensure this is last\n",
    "    ]\n",
    "    df = df[desired_column_order] # and now we reorder the columns\n",
    "\n",
    "    # and finally we return the dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be6c2c8b745391ec6bf4e4482a6fa632",
     "grade": false,
     "grade_id": "cell-d538705e1bdde279",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(tidy(mn)) == pd.core.frame.DataFrame, \"T0.1\"\n",
    "assert len((tidy(mn)).columns) == 9, \"T0.2\"\n",
    "assert list((tidy(mn)).columns)[0] == \"full_name\", \"T0.3\"\n",
    "assert list((tidy(mn)).columns)[len((tidy(mn)).columns)-1] == \"km_per_litre\", \"T0.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb930bb6bf9a82301ea0e43b2e24ad91",
     "grade": true,
     "grade_id": "cell-9a75a2763c7bbe5d",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! üòº‚ö°Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1a8d139471d249eef3a881ab9502e3d",
     "grade": false,
     "grade_id": "cell-72c2573051ab8535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-------\n",
    "## 1. Missing values\n",
    "\n",
    "### 1.1 Code part\n",
    "Implement a function called `missing_values` which takes as an input a dataframe and check if there are any missing values in the dataset. Record the row positions (*not* the row labels!) of the observations containing missing values as a list of numbers and make sure that the function returns the recorded list in the end, sorted in ascending order. If there are no missing values, `missing_values` should return an empty list.\n",
    "\n",
    "**NOTE:** You shall find out how missing values are encoded in your datasest and which missing values occur in your dataset, you will ***need manual inspection*** by applying Python helpers. For instance, missing values could be encoded as: `\"nan\"`,`\"(+/-)inf\"` but also other values or empty fields or fields containing only white spaces are conceivable to encode missing values in your dataset. Do *not* rely on built-in Python or pandas functions alone!\n",
    "\n",
    "Important: Mind the difference between row positions and row labels. `.index` of a dataframe returns row labels. `.iloc` takes row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c924e80de70e8169cc282686502fb094",
     "grade": false,
     "grade_id": "cell-6d4fb7f2e44e7cfb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Make sure pandas is imported\n",
    "\n",
    "def missing_values(x): #now lets find some missing values\n",
    "    missing_row_indices = [] #here we store the indices of the rows with missing values\n",
    "\n",
    "    missing_value_strings = [\"n/a\", \"na\", \"null\", \"none\", \"nan\", \"inf\", \"-inf\", \"+inf\", \"\"] #first we define the strings that we want to check for. Lets keep this generalized, so it also works for other dfs\n",
    "\n",
    "    print(f\"Checking for standard missing values (NaN, None) and specific strings: {missing_value_strings}\") #lets get a little help to answer the questions\n",
    "\n",
    "    # and now we start with a stroll through the dataframe\n",
    "    for i in range(len(x)):\n",
    "        row = x.iloc[i] #now we for each row\n",
    "        # Use .items() to get both column name and value, helpful for reporting\n",
    "        for col_name, item in row.items(): # we check each item\n",
    "\n",
    "            if pd.isna(item): #and we start with the standard pandas check for NaN/None\n",
    "                print(f\"Info: Row position {i}, Column '{col_name}': Found standard missing value (pd.isna=True). Original value: {item}\") #hen we define where we found a missing value\n",
    "                missing_row_indices.append(i) #if we find a missing value we add the index to the list\n",
    "                break # and then we Move to next row once a missing value is found\n",
    "            # now we check for any leftovers\n",
    "            try:\n",
    "                # Convert to string, strip whitespace, convert to lowercase\n",
    "                item_str_lower = str(item).strip().lower()\n",
    "                if item_str_lower in missing_value_strings:\n",
    "                    print(f\"Info: Row position {i}, Column '{col_name}': Found custom missing value string. Original value: '{item}', Matched string: '{item_str_lower}'\") #here we print another missing item\n",
    "                    missing_row_indices.append(i) #if we find a missing value we also add the index to the list\n",
    "                    break # and lets move on again\n",
    "            except Exception:\n",
    "                # now lets add exceptions because it wont work otherwise and a friend told me that this is a good practice ;)\n",
    "                pass\n",
    "\n",
    "    final_sorted_list = sorted(list(set(missing_row_indices))) #now we sort the list and remove potential duplicates if break was removed\n",
    "\n",
    "    if not final_sorted_list:\n",
    "        print(\"Result: No missing values detected.\") # a bit more output to answer the questions\n",
    "    else:\n",
    "        print(f\"Result: Found {len(final_sorted_list)} rows with missing values.\")\n",
    "\n",
    "    # and finally we return the list\n",
    "    return final_sorted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25ac8efba1efdd23e9985baa4cf78d40",
     "grade": false,
     "grade_id": "cell-49e8fe2aeb51324f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for standard missing values (NaN, None) and specific strings: ['n/a', 'na', 'null', 'none', 'nan', 'inf', '-inf', '+inf', '']\n",
      "Info: Row position 2, Column 'company_name': Found custom missing value string. Original value: 'NaN', Matched string: 'nan'\n",
      "Info: Row position 57, Column 'automotive': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 131, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 159, Column 'color': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 165, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 184, Column 'coordinates': Found custom missing value string. Original value: '+inf', Matched string: '+inf'\n",
      "Info: Row position 201, Column 'color': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 211, Column 'coordinates': Found custom missing value string. Original value: '+inf', Matched string: '+inf'\n",
      "Info: Row position 286, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 295, Column 'company_name': Found custom missing value string. Original value: '+inf', Matched string: '+inf'\n",
      "Info: Row position 312, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 328, Column 'color': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 368, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 404, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 453, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 543, Column 'address': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 571, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 594, Column 'color': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 607, Column 'automotive': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 653, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 673, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 698, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 726, Column 'color': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 746, Column 'job': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 751, Column 'company_name': Found custom missing value string. Original value: 'NaN', Matched string: 'nan'\n",
      "Info: Row position 775, Column 'coordinates': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 845, Column 'coordinates': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 910, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 946, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 979, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 1021, Column 'job': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1023, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1036, Column 'address': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1110, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1118, Column 'coordinates': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1126, Column 'job': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1190, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1201, Column 'full_name': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 1233, Column 'company_name': Found custom missing value string. Original value: 'NaN', Matched string: 'nan'\n",
      "Info: Row position 1372, Column 'company_name': Found custom missing value string. Original value: '+inf', Matched string: '+inf'\n",
      "Info: Row position 1410, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 1500, Column 'company_name': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 1501, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 1577, Column 'coordinates': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 1595, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1596, Column 'color': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 1618, Column 'coordinates': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Result: Found 47 rows with missing values.\n",
      "Checking for standard missing values (NaN, None) and specific strings: ['n/a', 'na', 'null', 'none', 'nan', 'inf', '-inf', '+inf', '']\n",
      "Info: Row position 2, Column 'company_name': Found custom missing value string. Original value: 'NaN', Matched string: 'nan'\n",
      "Info: Row position 57, Column 'automotive': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 131, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 159, Column 'color': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 165, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 184, Column 'coordinates': Found custom missing value string. Original value: '+inf', Matched string: '+inf'\n",
      "Info: Row position 201, Column 'color': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 211, Column 'coordinates': Found custom missing value string. Original value: '+inf', Matched string: '+inf'\n",
      "Info: Row position 286, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 295, Column 'company_name': Found custom missing value string. Original value: '+inf', Matched string: '+inf'\n",
      "Info: Row position 312, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 328, Column 'color': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 368, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 404, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 453, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 543, Column 'address': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 571, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 594, Column 'color': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 607, Column 'automotive': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 653, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 673, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 698, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 726, Column 'color': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 746, Column 'job': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 751, Column 'company_name': Found custom missing value string. Original value: 'NaN', Matched string: 'nan'\n",
      "Info: Row position 775, Column 'coordinates': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 845, Column 'coordinates': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 910, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 946, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 979, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 1021, Column 'job': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1023, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1036, Column 'address': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1110, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1118, Column 'coordinates': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1126, Column 'job': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1190, Column 'automotive': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1201, Column 'full_name': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 1233, Column 'company_name': Found custom missing value string. Original value: 'NaN', Matched string: 'nan'\n",
      "Info: Row position 1372, Column 'company_name': Found custom missing value string. Original value: '+inf', Matched string: '+inf'\n",
      "Info: Row position 1410, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 1500, Column 'company_name': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 1501, Column 'company_name': Found custom missing value string. Original value: '', Matched string: ''\n",
      "Info: Row position 1577, Column 'coordinates': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 1595, Column 'full_name': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Info: Row position 1596, Column 'color': Found custom missing value string. Original value: '-inf', Matched string: '-inf'\n",
      "Info: Row position 1618, Column 'coordinates': Found standard missing value (pd.isna=True). Original value: nan\n",
      "Result: Found 47 rows with missing values.\n"
     ]
    }
   ],
   "source": [
    "assert type(missing_values(tidy(mn))) == list, \"T1.1\"\n",
    "assert all(isinstance(i, int) for i in missing_values(tidy(mn))), \"T1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce5f9cfea1f57588c27bc8d2a33bc37e",
     "grade": true,
     "grade_id": "cell-4c692eab5b638fc7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! üòº‚ö°Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1835150c80672823f016567532fe9856",
     "grade": false,
     "grade_id": "cell-82f316abfa9423e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2. Analytical part\n",
    "\n",
    "* Does the dataset contain missing values?\n",
    "* Explain your manual-inspection procedure and the Python helpers used!\n",
    "* If no, explain how you proved that this is actually the case. \n",
    "* If yes, describe the discovered missing values. What could be an explanation for their missingness?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "229a22e8b0e33f07eb7e3e7a6a652bf2",
     "grade": true,
     "grade_id": "cell-3ad38c6f2d1998f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "My dataset contains missing values, as shown by the output and a manual inspection. I started the manual inspection, by trying to open the file with \"numbers\", which gave me an error and told me the CSV was broken. Then I opened it with a text editor and saw multiple placeholder values via control+f. Things like \"nan\" etc. Code wise my python fucntion uses .iloc for indexing, pd.isna() for NaN/None detection, and string methods .lower() and .strip() to standardize text before checking against missing value indicators like 'n/a'. It returns a sorted list of affected row indices. \n",
    "\n",
    "The Normal are likely to be empty information, string NaNs are probably when someone typed it into the system like that, emptry strings might be fields left blank when collecting data. The +- inf might stem from miscalculations, or errors with numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "919ec0c1fbf3d46cfeacebf3789dc7b3",
     "grade": false,
     "grade_id": "cell-87126fc406d941ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## 2. Handling missing values\n",
    "### 2.1. Code part\n",
    "Apply a (simple) function called *handling_missing_values* for handling missing values using an adequate single-imputation technique (or, one of the alternatives to single imputation) of your choice per type of missing values. Make use of the techniques learned in Unit 4. The function should take as an input a dataframe and return the updated dataframe. Mind the following:\n",
    "- The objective is to apply single imputation on these synthetic data. Do not make up a background story (at this point)!\n",
    "- Do NOT simply drop the missing values. This is not an option.\n",
    "- The imputation technique must be adequate for a given variable type (quantitative, qualitative).\n",
    "- To establish whether a variable is quantitative or qualitative, it is *not* sufficient to only inspect on data types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0226044c2b66de952ca20bc64edae12b",
     "grade": false,
     "grade_id": "cell-5ba2dc8b5cbe1f8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def handling_missing_values(x):\n",
    "    df = x.copy() # We'll work on a copy, in case we still need the original\n",
    "\n",
    "    # lets take our missing value strings again (code doesnt seem to work, if i dont re define it in this cell). \n",
    "    missing_value_strings = [\"n/a\", \"na\", \"null\", \"none\", \"nan\", \"inf\", \"-inf\", \"+inf\", \"\", \"NaN\"]\n",
    "\n",
    "    df = df.replace(missing_value_strings, np.nan) # and now we replace the missing values with NaN\n",
    "\n",
    "    problematic_mode_strings = {s.lower() for s in missing_value_strings if isinstance(s, str)} # and now we convert all strings to lower case (to  help with the mode later on)\n",
    "\n",
    "    for col in df.columns: #now we go through each column and fill in values\n",
    "        if col == \"km_per_litre\": #for the km_per_litre column we use the mean\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce') # first we ensure it's numeric\n",
    "            mean_val = df[col].mean() # then we calculate the mean\n",
    "            if pd.notna(mean_val): # Only fill if mean could be calculated\n",
    "                df[col] = df[col].fillna(mean_val)\n",
    "\n",
    "        elif col == \"coordinates\": #now we look at the coordinates column\n",
    "            # For 'coordinates', use a specific placeholder, 0,0\n",
    "            df[col] = df[col].fillna(\"(0,0)\")\n",
    "\n",
    "        else:\n",
    "            # For 'date_time' and all other columns, try filling with the mode (most frequent value).\n",
    "            # If mode is problematic or doesn't exist, use a default.\n",
    "            default_fill = \"1970-01-01 00:00:00\" if col == \"date_time\" else \"Unknown\" #btw we use this time because a youtube video once told me that its the default time for unix systems. we could use any time here, just make sure that its recognizable as a default value\n",
    "            fill_value = default_fill # Start with the default\n",
    "\n",
    "            mode_result = df[col].mode() # now we calculate the mode\n",
    "            if not mode_result.empty: # if the mode is not empty\n",
    "                potential_mode = mode_result[0] # we take the first value\n",
    "                # Check if the mode itself is NaN or looks like one of our missing markers\n",
    "                is_problematic = pd.isna(potential_mode) or \\\n",
    "                                 (isinstance(potential_mode, str) and \\\n",
    "                                  potential_mode.strip().lower() in problematic_mode_strings) # and now we check if the mode is problematic (and we get to use some pretty booleans)\n",
    "\n",
    "                if not is_problematic:\n",
    "                    fill_value = potential_mode # Use the mode if it's valid\n",
    "\n",
    "            # Fill any remaining NaNs in the column with the defined value above (\"Unknown\" or \"1970-01-01 00:00:00\")\n",
    "            df[col] = df[col].fillna(fill_value)\n",
    "\n",
    "    return df # and now we return the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f4b2e3d0f0820ea95c3c5ffb9efe951",
     "grade": true,
     "grade_id": "cell-0edce052e98e2e95",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for standard missing values (NaN, None) and specific strings: ['n/a', 'na', 'null', 'none', 'nan', 'inf', '-inf', '+inf', '']\n",
      "Result: No missing values detected.\n"
     ]
    }
   ],
   "source": [
    "assert len(missing_values(handling_missing_values(tidy(mn)))) == 0, \"T2.1\"\n",
    "assert handling_missing_values(tidy(mn)).shape == tidy(mn).shape, \"T2.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "379b1e101131334e3262e96f723fe8e6",
     "grade": false,
     "grade_id": "cell-c697641b9d5a1c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2. Analytical part\n",
    "Discuss the implications. Answer the following:\n",
    "\n",
    "- How would you qualify the data-generating processes leading to different types of missing values, provided that the data was not synthetic?\n",
    "- What are the benefits and disadvantages of the chosen single-imputation technique?\n",
    "- How would you apply a multiple-imputation technique to one type of missing values, if applicable at all?\n",
    "- We asked you to test for/treat as missing values by checking certain field values, as well as empty fields or fields containing the numeric value 0... what are potential problems of this heuristics?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34f9ca681c722ebe151d6fac42d71b25",
     "grade": true,
     "grade_id": "cell-5c05456587f2ff17",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. Missing Data\n",
    "    - MCAR (Missing Completely At Random): Missingness is random & unrelated to any values (e.g., random data corruption).\n",
    "    - MAR (Missing At Random): Missingness depends only on other observed variables (e.g., older cars less likely to report fuel efficiency, but not based on the efficiency itself).\n",
    "    - MNAR (Missing Not At Random): Missingness depends on the missing value itself (e.g., people with very low fuel efficiency avoid reporting it). Determining the true type requires domain knowledge.\n",
    "\n",
    "2. Single-Imputation pros and cons\n",
    "    - Pro: Simple, fast, creates a complete dataset for algorithms.\n",
    "    - Con: Underestimates uncertainty (false precision), distorts relationships (correlations, variance), can bias results.\n",
    "    - Mean Imputation (km_per_litre): Pro: Preserves mean. Con: Reduces variance, sensitive to outliers, ignores other variables.\n",
    "    - Mode Imputation (Others): Pro: Simple for categorical data. Con: Distorts distribution (artificial spike), ignores other variables.\n",
    "\n",
    "3. Use of multiple-imputation \n",
    "    MI addresses the uncertainty ignored by single imputation.\n",
    "    3.1. Model: Build a model predicting the missing variable (e.g., km_per_litre) using other variables.\n",
    "    3.2. Impute Multiple Times: Generate several plausible values for each missing entry based on the model and its uncertainty, creating multiple complete datasets.\n",
    "    3.3. Analyze: Perform the analysis on each dataset separately.\n",
    "    3.4. Pool: Combine the results using specific rules (Rubin's Rules) to get a final estimate and correct uncertainty.\n",
    "\n",
    "4. Issues with testing for missing values\n",
    "    - False Positives: Treating valid data as missing.\n",
    "        - 0 is often a real measurement, not missing.\n",
    "        - \"\" (empty string) can be intentionally blank, not unknown.\n",
    "        - Specific strings (\"NA\") might be valid categories in some contexts.\n",
    "    - False Negatives: Failing to detect actual missing data.\n",
    "        - The list of missing strings (\"n/a\", etc.) might be incomplete (miss \"?, -999).\n",
    "    - Context Matters: The meaning of 0, \"\", or specific strings depends heavily on the variable. Universal rules without context are risky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21398d4af7c97ce06acb2f77675ce4d8",
     "grade": false,
     "grade_id": "cell-573d56d6699b84eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 3. Detecting duplicate entries\n",
    "Implement a function called `duplicates` that takes as an input a (tidy) dataframe `x` and a list of column labels (`VARIABLES`). Assume that `duplicates` receives a dataframe as returned from your Step 0 implementation of `tidy`. It then checks whether there are any duplicates in the dataset. Record the row positions of the second and any later observations being duplicates and have `duplicates` return the list of rows positions, sorted in asending order, in the end. An empty list indicates the absence of duplicated observations.\n",
    "\n",
    "Important:\n",
    "* The first observation that belongs to the detected duplicates is *not* considered a duplicate!\n",
    "* Mind the difference between row positions and row labels. `.index` of a dataframe returns row labels. `.iloc` takes row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7eb1977de42e36ad646b36acc6315d71",
     "grade": false,
     "grade_id": "cell-7954cffea933a812",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function duplicates at 0x1073befc0>\n"
     ]
    }
   ],
   "source": [
    "df = tidy(mn) #lets put our tidy function to work\n",
    "VARIABLES = df.columns.tolist(); #and lets get the columns and put them in a list\n",
    "\n",
    "def duplicates(data, vars):\n",
    "    if not isinstance(data, pd.DataFrame): #first we check if the input is a dataframe\n",
    "        raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "    \n",
    "    if not vars or not isinstance(vars, list) or not all(v in data.columns.tolist() for v in vars): # lets 1. check if vars list is empty 2. if it even is a list 3. if all the variables are in the dataframe (essentially, is the list correct, given the headers of our df)\n",
    "        return \"Name variables defining potential duplicates!\"\n",
    "    \n",
    "    mask_duplicates = data.duplicated(subset=vars, keep='first') #now we create a mask for the duplicates\n",
    "\n",
    "    duplicate_pos = [data.index.get_loc(i) for i in data[mask_duplicates].index] #then we get the positions of the duplicates\n",
    "\n",
    "    return sorted(duplicate_pos) #and now we return the sorted list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da0c90261f277ba2850eb06ae49cf99a",
     "grade": false,
     "grade_id": "cell-360763185fff8ba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = tidy(mn);\n",
    "assert len(VARIABLES) > 0 and all([v in df.columns.tolist() for v in VARIABLES]), \"T3.1\"\n",
    "assert duplicates(df, [list]) == \"Name variables defining potential duplicates!\", \"T3.2\"\n",
    "assert duplicates(df, None) == \"Name variables defining potential duplicates!\", \"T3.3\"\n",
    "assert type(duplicates(df, vars = df.columns.tolist())) == list, \"T3.4\"\n",
    "assert all(isinstance(i, int) for i in duplicates(df, df.columns.tolist())), \"T3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51e366349b5d24359135760030a2731c",
     "grade": true,
     "grade_id": "cell-583bc8ab4ba38aa6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! üòº‚ö°Ô∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5245abdf3b47f21f1fb0289ee8217bb",
     "grade": false,
     "grade_id": "cell-b04e3f6689a78a44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 4. Detecting outliers\n",
    "### 4.1. Code part\n",
    "Implement a function called `detecting_outliers` to detect outliers in one selected quantitative variable. Pick a suitable variable from the tidied dataset based on your characterisation and apply one suitable outlier-detection technique as covered in Unit 4. Justify your choice of this technique in the analytical part. Again, the function is assumed to receive a tidied data set from Step 0. The function returns the row positions (*not* row labels!) of the rows containing outliers on the selected variable, sorted in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c10ec065604cff85d708868fa0909ced",
     "grade": false,
     "grade_id": "cell-f593e79eae8f4227",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def detecting_outliers(df_input): #lets find some gas guzzlers!\n",
    "    col_name = \"km_per_litre\" #first we define the column we want to analyze\n",
    "    std_dev_multiplier = 2.0 # and then the sd multiplier\n",
    "\n",
    "    df = df_input.copy() # we will use a copy to leave the original untouched\n",
    "\n",
    "    if col_name not in df.columns: # first we check if the given col even exists\n",
    "        print(f\"Error: Required column '{col_name}' not found in DataFrame.\")\n",
    "        return []\n",
    "\n",
    "    original_non_null_count = df[col_name].notna().sum() #first we store the original count of NaNa\n",
    "    df[col_name] = pd.to_numeric(df[col_name], errors='coerce') # then we check if the column is numeric, coercing errors\n",
    "    numeric_non_null_count = df[col_name].notna().sum() # and we store the new count of NaNa\n",
    "\n",
    "    if original_non_null_count != numeric_non_null_count: # and then a little check to see if the coercion caused changes\n",
    "         print(f\"Note: Converted column to numeric. Valid entries changed from {original_non_null_count} to {numeric_non_null_count}.\")\n",
    "\n",
    "    # now that we know we only have numeric values left, we drop the Nans\n",
    "    numeric_data = df[col_name].dropna()\n",
    "    num_valid_points = len(numeric_data)\n",
    "    print(f\"Found {num_valid_points} valid numeric data points for analysis.\")\n",
    "\n",
    "    if num_valid_points < 2: # now lets see if we have enough data to calculate the standard deviation\n",
    "        print(f\"Not enough data ({num_valid_points}) to calculate standard deviation. Cannot detect outliers.\")\n",
    "        return [] # Return empty list if not enough data\n",
    "\n",
    "    mean_val = numeric_data.mean() #and now we can finally get to the stats part. lets calc the mean\n",
    "    std_dev_val = numeric_data.std() #adn the std dev\n",
    "\n",
    "    # --> Print the calculated statistics <--\n",
    "    print(f\"Calculated Mean for '{col_name}': {mean_val:.2f}\")\n",
    "    print(f\"Calculated Standard Deviation for '{col_name}': {std_dev_val:.2f}\")\n",
    "\n",
    "    if std_dev_val == 0 or pd.isna(std_dev_val): #now we check if the std dev is 0 or NaN\n",
    "        print(\"Standard deviation is zero or NaN. Outliers cannot be determined by this method.\")\n",
    "        return [] # Return empty list\n",
    "\n",
    "    lower_bound = mean_val - std_dev_multiplier * std_dev_val #now we get to the bounds part. first the lower bound\n",
    "    upper_bound = mean_val + std_dev_multiplier * std_dev_val #and then the upper bound\n",
    "\n",
    "    print(f\"Calculated Lower Bound ({std_dev_multiplier} std dev): {lower_bound:.2f}\") #and lets print them\n",
    "    print(f\"Calculated Upper Bound ({std_dev_multiplier} std dev): {upper_bound:.2f}\")\n",
    "\n",
    "    is_outlier = ((df[col_name] < lower_bound) | (df[col_name] > upper_bound)) & df[col_name].notna() # now we get the outliers witha  mask\n",
    "\n",
    "    outlier_positions = np.where(is_outlier)[0].tolist() # and use the mask to the get positions of the outliers\n",
    "    num_outliers = len(outlier_positions) #and then we get the number of outliers\n",
    "\n",
    "    print(f\"Number of outliers detected: {num_outliers}\") #and lets print the number of outliers\n",
    "\n",
    "    return sorted(outlier_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a687f420acb566a3dbe153acc0bfd16",
     "grade": true,
     "grade_id": "cell-231217b55b6ef843",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1671 valid numeric data points for analysis.\n",
      "Calculated Mean for 'km_per_litre': 27.68\n",
      "Calculated Standard Deviation for 'km_per_litre': 11.40\n",
      "Calculated Lower Bound (2.0 std dev): 4.87\n",
      "Calculated Upper Bound (2.0 std dev): 50.49\n",
      "Number of outliers detected: 57\n",
      "Found 1671 valid numeric data points for analysis.\n",
      "Calculated Mean for 'km_per_litre': 27.68\n",
      "Calculated Standard Deviation for 'km_per_litre': 11.40\n",
      "Calculated Lower Bound (2.0 std dev): 4.87\n",
      "Calculated Upper Bound (2.0 std dev): 50.49\n",
      "Number of outliers detected: 57\n",
      "Found 1671 valid numeric data points for analysis.\n",
      "Calculated Mean for 'km_per_litre': 27.68\n",
      "Calculated Standard Deviation for 'km_per_litre': 11.40\n",
      "Calculated Lower Bound (2.0 std dev): 4.87\n",
      "Calculated Upper Bound (2.0 std dev): 50.49\n",
      "Number of outliers detected: 57\n",
      "Found 1671 valid numeric data points for analysis.\n",
      "Calculated Mean for 'km_per_litre': 27.68\n",
      "Calculated Standard Deviation for 'km_per_litre': 11.40\n",
      "Calculated Lower Bound (2.0 std dev): 4.87\n",
      "Calculated Upper Bound (2.0 std dev): 50.49\n",
      "Number of outliers detected: 57\n"
     ]
    }
   ],
   "source": [
    "df = tidy(mn);\n",
    "assert type(detecting_outliers(df)) == list, \"T4.1\"\n",
    "assert all(isinstance(i, int) for i in detecting_outliers(df)), \"T4.2\"\n",
    "assert len(detecting_outliers(df)) > 0 and len(detecting_outliers(df)) < .05*df.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e5ca4b902e63d96e5376adc95c8302d",
     "grade": false,
     "grade_id": "cell-8c4530b128b5c186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2. Analytical part\n",
    "Discuss the implications. \n",
    "\n",
    "- What is the chosen outlier-detection technique? Explain it using your own words in 3-4 sentences.\n",
    "- Describe the outliers detected: How many? How do they relate to the typical, non-outlier values in the remaining dataset?\n",
    "- What could be one reason these outliers appear in the dataset? How would you treat them further?\n",
    "\n",
    "Write your answer in the markdown cell below. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e93305db24e2fc3800f252b797c2ad7",
     "grade": true,
     "grade_id": "cell-254db63097c3ed40",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "- The chosen outlier-detection technique is the 2 x Standard Deviation Method. This method operates by first calculating the average (mean) and the standard deviation of the selected variable (km_per_litre). It then establishes a range considered \"typical\" by going a specific number of standard deviations (in this case 2) below and above the mean. Any data point that falls outside this calculated range is classified as an outlier, indicating it's statistically far from the central tendency of the data.\n",
    "- The analysis detected 57 outliers in the km_per_litre variable. These outliers represent vehicles with fuel efficiency values that fall outside the calculated \"typical\" range. Specifically, they are vehicles with a km_per_litre value either less than 4.87 (exceptionally low fuel efficiency) or greater than 50.49 (exceptionally high fuel efficiency). These outlier values contrast significantly with the bulk of the dataset, where typical, non-outlier vehicles have fuel efficiencies falling between 4.87 and 50.49 km/litre, centered around the calculated mean of approximately 27.68 km/litre.\n",
    "- One possible reason for these outliers could be the presence of genuine extreme vehicle types within the dataset. For example, very low km_per_litre values (< 4.87) might correspond to heavy-duty vehicles, large trucks, or high-performance sports cars not representative of typical passenger cars. Conversely, very high values (> 50.49) could represent highly fuel-efficient vehicles like hybrids, potentially electric vehicles if measured on a comparable scale, motorcycles included erroneously, or data entry errors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
