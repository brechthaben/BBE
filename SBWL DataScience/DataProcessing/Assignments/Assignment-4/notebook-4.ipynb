{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f84edb5e70e5c8cbd2d3004c9b67ceaf",
     "grade": false,
     "grade_id": "cell-9e78272cc4af091a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4\n",
    "***\n",
    "*General hints:* <br>\n",
    "* You may use another notebook to test different approaches and ideas. When complete and mature, turn your code snippets into the requested functions in this notebook for submission. \n",
    "* Make sure the function implementations are generic and can be applied to any dataset (not just the one provided).\n",
    "* Add explanatory code comments in the code cells. Make sure that these comments improve our understanding of your implementation decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca384ab2b432240f2e75945bb1878249",
     "grade": false,
     "grade_id": "cell-9d52cb434e7f0910",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "* Create a variable holding your student id, as shown below. \n",
    "* Simply replace the example (`01234567`) with your actual student id having a total of 8 digits. \n",
    "* Maintain the variable as a string, do NOT change its type in this notebook!\n",
    "* *Note: If your student id has 7 digits, add a leading 0. The final student id MUST have 8 digits!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = '12318768'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95db3692d001bdd0ce4a073305285b20",
     "grade": false,
     "grade_id": "cell-98bb586658e7b54d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dce49140be8025de8894a25a2b824268",
     "grade": false,
     "grade_id": "cell-420b4c449379ffe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 0. Import\n",
    "\n",
    "Implement a function `tidy` which imports the data set assigned and provided to you as a CSV file into a `pandas` dataframe. Access the data set and establish whether your data set is tidy. If not, clean the data set before continuing with Step 1. Mind all rules of tidying data sets in this step. Make sure you comply to the following statements:\n",
    "* If there is an index column (row numbers) in your tidied dataset, keep it.\n",
    "* The following columns, once identified, correspond to variables 1:1 (no need for transformations):\n",
    "  * `full_name`\n",
    "  * `automotive`\n",
    "  * `color`\n",
    "  * `job`\n",
    "  * `address`\n",
    "  * `coordinates`\n",
    "  * `km_per_litre`\n",
    "* The tidied dataset should have a total of 9 columns (not including the index), the first column should be `full_name` and the last one `km_per_litre`.\n",
    "* Mind the intended content of each attribute (e.g. `full_name` should contain the full name of a person, no need to change that)\n",
    "* If tidy or done, have the function `tidy` return the ready data set as a dataframe.\n",
    "\n",
    "Note that `tidy` must take a single parameter that holds your student id (`mn`) as one part of the basename (according to the CoC) of the CSV file (i.e., the CoC file name without file extension). Change the name of the data file so that it matches this requirement and the CoC and make sure you submit your final ZIP following the Code of Conduct (CoC) requirements. Especially, make sure you put your data file in a folder called `data/` when submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e96fea9cc9f84652b592c3659339a48b",
     "grade": false,
     "grade_id": "cell-81e21dccd785e63d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tidy(x): #this function will tidy the data\n",
    "    file_path = f\"data/{x}.csv\" #first we fetch the file path\n",
    "    my_data_frame = pd.read_csv(file_path, header=None) #then we read the file\n",
    "\n",
    "    my_data_frame_indexed = my_data_frame.set_index(0) #we set the first column as the index\n",
    "    df = my_data_frame_indexed.T # and after looking at the data it seems to be transposed, so we transpose it again \n",
    "\n",
    "    first_col_name = df.columns[0] #now we can see that the first column is the full name of the person\n",
    "    if pd.isna(first_col_name): #if the first column name is missing, we drop it\n",
    "        df = df.drop(columns=[first_col_name])\n",
    "\n",
    "    combined_column = df[\"date_time/full_company_name\"] #now lets start splitting the name and date column\n",
    "\n",
    "    date_time_part = combined_column.str[:26] # after looking at the data we can see that the date and time are the first 26 characters, so we can split it after 26 characters\n",
    "    company_name_part = combined_column.str[26:] #and the rest is the company name\n",
    "\n",
    "    df[\"date_time\"] = date_time_part.str.strip() #now we can create the new columns, that are stripped\n",
    "    df[\"company_name\"] = company_name_part.str.strip() #and once again\n",
    "\n",
    "    df = df.drop(columns=[\"date_time/full_company_name\"]) #and now we remove the original column\n",
    "\n",
    "    desired_column_order = [ #lets reorder our headers, so they match the asignment\n",
    "        \"full_name\",\n",
    "        \"automotive\",\n",
    "        \"color\",\n",
    "        \"job\",\n",
    "        \"address\",\n",
    "        \"coordinates\",\n",
    "        \"date_time\",      # New column\n",
    "        \"company_name\",   # New column\n",
    "        \"km_per_litre\"    # and lets Ensure this is last\n",
    "    ]\n",
    "    df = df[desired_column_order] # and now we reorder the columns\n",
    "\n",
    "    # and finally we return the dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be6c2c8b745391ec6bf4e4482a6fa632",
     "grade": false,
     "grade_id": "cell-d538705e1bdde279",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(tidy(mn)) == pd.core.frame.DataFrame, \"T0.1\"\n",
    "assert len((tidy(mn)).columns) == 9, \"T0.2\"\n",
    "assert list((tidy(mn)).columns)[0] == \"full_name\", \"T0.3\"\n",
    "assert list((tidy(mn)).columns)[len((tidy(mn)).columns)-1] == \"km_per_litre\", \"T0.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb930bb6bf9a82301ea0e43b2e24ad91",
     "grade": true,
     "grade_id": "cell-9a75a2763c7bbe5d",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! üòº‚ö°Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1a8d139471d249eef3a881ab9502e3d",
     "grade": false,
     "grade_id": "cell-72c2573051ab8535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-------\n",
    "## 1. Missing values\n",
    "\n",
    "### 1.1 Code part\n",
    "Implement a function called `missing_values` which takes as an input a dataframe and check if there are any missing values in the dataset. Record the row positions (*not* the row labels!) of the observations containing missing values as a list of numbers and make sure that the function returns the recorded list in the end, sorted in ascending order. If there are no missing values, `missing_values` should return an empty list.\n",
    "\n",
    "**NOTE:** You shall find out how missing values are encoded in your datasest and which missing values occur in your dataset, you will ***need manual inspection*** by applying Python helpers. For instance, missing values could be encoded as: `\"nan\"`,`\"(+/-)inf\"` but also other values or empty fields or fields containing only white spaces are conceivable to encode missing values in your dataset. Do *not* rely on built-in Python or pandas functions alone!\n",
    "\n",
    "Important: Mind the difference between row positions and row labels. `.index` of a dataframe returns row labels. `.iloc` takes row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c924e80de70e8169cc282686502fb094",
     "grade": false,
     "grade_id": "cell-6d4fb7f2e44e7cfb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Note: numpy import removed as pd.isna handles np.nan\n",
    "\n",
    "# Finds rows in DataFrame 'x' containing missing values.\n",
    "# Checks for NaN/None, empty/whitespace strings, and common missing-value words.\n",
    "def missing_values(x):\n",
    "    # Store indices of rows with missing data\n",
    "    missing_row_indices = []\n",
    "\n",
    "    # Define common string representations of missing data (lowercase)\n",
    "    missing_value_strings = [\"n/a\", \"na\", \"null\", \"none\", \"nan\", \"inf\", \"-inf\", \"+inf\", \"\"]\n",
    "\n",
    "    # Iterate through each row index\n",
    "    for i in range(len(x)):\n",
    "        # Get the current row by position\n",
    "        row = x.iloc[i]\n",
    "\n",
    "        # Check each item in the row\n",
    "        for item in row:\n",
    "            # Check 1: Standard pandas check for NaN/None\n",
    "            if pd.isna(item):\n",
    "                missing_row_indices.append(i)\n",
    "                break # Move to next row once a missing value is found\n",
    "\n",
    "            # Check 2: Check for designated missing strings (or empty after stripping)\n",
    "            try:\n",
    "                # Convert to string, strip whitespace, convert to lowercase\n",
    "                item_str_lower = str(item).strip().lower()\n",
    "                if item_str_lower in missing_value_strings:\n",
    "                    missing_row_indices.append(i)\n",
    "                    break # Move to next row\n",
    "            except Exception:\n",
    "                # Handle potential errors during string conversion (optional, but safer)\n",
    "                # If conversion fails, might indicate an unusual data type we treat as non-missing here\n",
    "                pass\n",
    "\n",
    "    # Sort the indices for consistent output (and remove potential duplicates if break was removed)\n",
    "    # Using set() then list() is a common way to get unique sorted values\n",
    "    final_sorted_list = sorted(list(set(missing_row_indices)))\n",
    "\n",
    "    # Return the list of row indices with missing values\n",
    "    return final_sorted_list\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume 'tidy' function and 'mn' variable are defined already\n",
    "# df = tidy(mn)\n",
    "#\n",
    "# print(\"Running missing values check...\")\n",
    "# missing_indices = missing_values(df)\n",
    "#\n",
    "# # Inspect the results outside the function:\n",
    "# print(f\"\\nTotal rows checked: {len(df)}\")\n",
    "# print(f\"Indices of rows with missing values: {missing_indices}\")\n",
    "# print(f\"Count of rows with missing values: {len(missing_indices)}\")\n",
    "#\n",
    "# # Optional: View the actual rows with missing data\n",
    "# # if missing_indices:\n",
    "# #     print(\"\\nRows with missing data:\")\n",
    "# #     print(df.iloc[missing_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25ac8efba1efdd23e9985baa4cf78d40",
     "grade": false,
     "grade_id": "cell-49e8fe2aeb51324f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(missing_values(tidy(mn))) == list, \"T1.1\"\n",
    "assert all(isinstance(i, int) for i in missing_values(tidy(mn))), \"T1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce5f9cfea1f57588c27bc8d2a33bc37e",
     "grade": true,
     "grade_id": "cell-4c692eab5b638fc7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! üòº‚ö°Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1835150c80672823f016567532fe9856",
     "grade": false,
     "grade_id": "cell-82f316abfa9423e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2. Analytical part\n",
    "\n",
    "* Does the dataset contain missing values?\n",
    "* Explain your manual-inspection procedure and the Python helpers used!\n",
    "* If no, explain how you proved that this is actually the case. \n",
    "* If yes, describe the discovered missing values. What could be an explanation for their missingness?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "229a22e8b0e33f07eb7e3e7a6a652bf2",
     "grade": true,
     "grade_id": "cell-3ad38c6f2d1998f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "919ec0c1fbf3d46cfeacebf3789dc7b3",
     "grade": false,
     "grade_id": "cell-87126fc406d941ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## 2. Handling missing values\n",
    "### 2.1. Code part\n",
    "Apply a (simple) function called *handling_missing_values* for handling missing values using an adequate single-imputation technique (or, one of the alternatives to single imputation) of your choice per type of missing values. Make use of the techniques learned in Unit 4. The function should take as an input a dataframe and return the updated dataframe. Mind the following:\n",
    "- The objective is to apply single imputation on these synthetic data. Do not make up a background story (at this point)!\n",
    "- Do NOT simply drop the missing values. This is not an option.\n",
    "- The imputation technique must be adequate for a given variable type (quantitative, qualitative).\n",
    "- To establish whether a variable is quantitative or qualitative, it is *not* sufficient to only inspect on data types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0226044c2b66de952ca20bc64edae12b",
     "grade": false,
     "grade_id": "cell-5ba2dc8b5cbe1f8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume 'tidy' and 'missing_values' functions are defined elsewhere.\n",
    "\n",
    "def handling_missing_values(x):\n",
    "    \"\"\"\n",
    "    Cleans and fills missing values in a DataFrame using specific strategies\n",
    "    for different columns.\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with missing values handled.\n",
    "    \"\"\"\n",
    "    df = x.copy() # Work on a copy to avoid changing the original\n",
    "\n",
    "    # Define common ways missing data might be represented as text\n",
    "    missing_markers = [\"n/a\", \"na\", \"null\", \"none\", \"nan\", \"inf\", \"-inf\", \"+inf\", \"\", \"NaN\"]\n",
    "    # Convert all these markers to NumPy's standard NaN (Not a Number)\n",
    "    df = df.replace(missing_markers, np.nan)\n",
    "\n",
    "    # Define text values (lowercase) that might be mistaken for missing data,\n",
    "    # used later to check if a calculated 'mode' is suitable for filling.\n",
    "    problematic_mode_strings = {s.lower() for s in missing_markers if isinstance(s, str)}\n",
    "\n",
    "    # Go through each column and apply a filling strategy\n",
    "    for col in df.columns:\n",
    "        if col == \"km_per_litre\":\n",
    "            # For numerical 'km_per_litre', fill with the column's average (mean)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce') # Ensure it's numeric\n",
    "            mean_val = df[col].mean()\n",
    "            if pd.notna(mean_val): # Only fill if mean could be calculated\n",
    "                df[col] = df[col].fillna(mean_val)\n",
    "            # Optional: could add an else clause to fill with 0 if mean is NaN\n",
    "\n",
    "        elif col == \"coordinates\":\n",
    "            # For 'coordinates', use a specific placeholder\n",
    "            df[col] = df[col].fillna(\"(0,0)\")\n",
    "\n",
    "        else:\n",
    "            # For 'date_time' and all other columns, try filling with the mode (most frequent value).\n",
    "            # If mode is problematic or doesn't exist, use a default.\n",
    "            default_fill = \"1970-01-01 00:00:00\" if col == \"date_time\" else \"Unknown\"\n",
    "            fill_value = default_fill # Start with the default\n",
    "\n",
    "            mode_result = df[col].mode()\n",
    "            # Check if a mode exists and if it's suitable\n",
    "            if not mode_result.empty:\n",
    "                potential_mode = mode_result[0]\n",
    "                # Check if the mode itself is NaN or looks like one of our missing markers\n",
    "                is_problematic = pd.isna(potential_mode) or \\\n",
    "                                 (isinstance(potential_mode, str) and \\\n",
    "                                  potential_mode.strip().lower() in problematic_mode_strings)\n",
    "\n",
    "                if not is_problematic:\n",
    "                    fill_value = potential_mode # Use the mode if it's valid\n",
    "\n",
    "            # Fill any remaining NaNs in the column with the chosen value\n",
    "            df[col] = df[col].fillna(fill_value)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Example Usage & Assertions (Keep assertions for validation) ---\n",
    "# Assume 'mn' points to your data source and 'tidy'/'missing_values' are ready.\n",
    "# df_tidied = tidy(mn)\n",
    "# df_handled = handling_missing_values(df_tidied)\n",
    "# missing_after_handling = missing_values(df_handled)\n",
    "\n",
    "# # These assertions help confirm the function worked as expected.\n",
    "# assert len(missing_after_handling) == 0, f\"T2.1: Found {len(missing_after_handling)} missing values after handling!\"\n",
    "# assert df_handled.shape == df_tidied.shape, \"T2.2: DataFrame shape changed during handling!\"\n",
    "\n",
    "# print(\"Handling missing values complete. Assertions passed!\") # Optional confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f4b2e3d0f0820ea95c3c5ffb9efe951",
     "grade": true,
     "grade_id": "cell-0edce052e98e2e95",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(missing_values(handling_missing_values(tidy(mn)))) == 0, \"T2.1\"\n",
    "assert handling_missing_values(tidy(mn)).shape == tidy(mn).shape, \"T2.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "379b1e101131334e3262e96f723fe8e6",
     "grade": false,
     "grade_id": "cell-c697641b9d5a1c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2. Analytical part\n",
    "Discuss the implications. Answer the following:\n",
    "\n",
    "- How would you qualify the data-generating processes leading to different types of missing values, provided that the data was not synthetic?\n",
    "- What are the benefits and disadvantages of the chosen single-imputation technique?\n",
    "- How would you apply a multiple-imputation technique to one type of missing values, if applicable at all?\n",
    "- We asked you to test for/treat as missing values by checking certain field values, as well as empty fields or fields containing the numeric value 0... what are potential problems of this heuristics?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34f9ca681c722ebe151d6fac42d71b25",
     "grade": true,
     "grade_id": "cell-5c05456587f2ff17",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21398d4af7c97ce06acb2f77675ce4d8",
     "grade": false,
     "grade_id": "cell-573d56d6699b84eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 3. Detecting duplicate entries\n",
    "Implement a function called `duplicates` that takes as an input a (tidy) dataframe `x` and a list of column labels (`VARIABLES`). Assume that `duplicates` receives a dataframe as returned from your Step 0 implementation of `tidy`. It then checks whether there are any duplicates in the dataset. Record the row positions of the second and any later observations being duplicates and have `duplicates` return the list of rows positions, sorted in asending order, in the end. An empty list indicates the absence of duplicated observations.\n",
    "\n",
    "Important:\n",
    "* The first observation that belongs to the detected duplicates is *not* considered a duplicate!\n",
    "* Mind the difference between row positions and row labels. `.index` of a dataframe returns row labels. `.iloc` takes row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7eb1977de42e36ad646b36acc6315d71",
     "grade": false,
     "grade_id": "cell-7954cffea933a812",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Checking for duplicates using variables: ['full_name', 'automotive', 'color', 'job', 'address', 'coordinates', 'date_time', 'company_name', 'km_per_litre']\n",
      "üîÅ Duplicate row positions: [754]\n",
      "üî¢ Number of duplicates: 1\n",
      "\n",
      "üìã Duplicate row contents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>automotive</th>\n",
       "      <th>color</th>\n",
       "      <th>job</th>\n",
       "      <th>address</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>date_time</th>\n",
       "      <th>company_name</th>\n",
       "      <th>km_per_litre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>Chad Chase</td>\n",
       "      <td>2V TU960</td>\n",
       "      <td>SkyBlue</td>\n",
       "      <td>Environmental education officer</td>\n",
       "      <td>South Kimberlyberg</td>\n",
       "      <td>(Decimal('66.381533'), Decimal('-6.316750'))</td>\n",
       "      <td>2005-06-30 06:46:05.814620</td>\n",
       "      <td>Pena and Sons</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0     full_name automotive    color                              job  \\\n",
       "755  Chad Chase   2V TU960  SkyBlue  Environmental education officer   \n",
       "\n",
       "0               address                                   coordinates  \\\n",
       "755  South Kimberlyberg  (Decimal('66.381533'), Decimal('-6.316750'))   \n",
       "\n",
       "0                     date_time   company_name km_per_litre  \n",
       "755  2005-06-30 06:46:05.814620  Pena and Sons           23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = tidy(mn)\n",
    "VARIABLES = df.columns.tolist(); # Change value assignment!\n",
    "  # or manually define like [\"full_name\", \"automotive\", \"color\"]\n",
    "\n",
    "# ‚úÖ Function to find duplicate entries\n",
    "def duplicates(x, vars):\n",
    "    if not vars or not isinstance(vars, list) or not all(v in x.columns.tolist() for v in vars):\n",
    "        return \"Name variables defining potential duplicates!\"\n",
    "    \n",
    "    # Find duplicated rows (excluding the first occurrence)\n",
    "    duplicate_mask = x.duplicated(subset=vars, keep='first')\n",
    "\n",
    "    # Convert row labels to row **positions**\n",
    "    duplicate_positions = [x.index.get_loc(i) for i in x[duplicate_mask].index]\n",
    "\n",
    "    return sorted(duplicate_positions)\n",
    "\n",
    "# Run the function\n",
    "df = tidy(mn)\n",
    "duplicate_rows = duplicates(df, VARIABLES)\n",
    "\n",
    "# Print diagnostics\n",
    "print(\"üß† Checking for duplicates using variables:\", VARIABLES)\n",
    "print(\"üîÅ Duplicate row positions:\", duplicate_rows)\n",
    "print(\"üî¢ Number of duplicates:\", len(duplicate_rows))\n",
    "\n",
    "# Optionally show duplicate row content\n",
    "if duplicate_rows:\n",
    "    print(\"\\nüìã Duplicate row contents:\")\n",
    "    display(df.iloc[duplicate_rows])\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found!\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da0c90261f277ba2850eb06ae49cf99a",
     "grade": false,
     "grade_id": "cell-360763185fff8ba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = tidy(mn);\n",
    "assert len(VARIABLES) > 0 and all([v in df.columns.tolist() for v in VARIABLES]), \"T3.1\"\n",
    "assert duplicates(df, [list]) == \"Name variables defining potential duplicates!\", \"T3.2\"\n",
    "assert duplicates(df, None) == \"Name variables defining potential duplicates!\", \"T3.3\"\n",
    "assert type(duplicates(df, vars = df.columns.tolist())) == list, \"T3.4\"\n",
    "assert all(isinstance(i, int) for i in duplicates(df, df.columns.tolist())), \"T3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51e366349b5d24359135760030a2731c",
     "grade": true,
     "grade_id": "cell-583bc8ab4ba38aa6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! üòº‚ö°Ô∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5245abdf3b47f21f1fb0289ee8217bb",
     "grade": false,
     "grade_id": "cell-b04e3f6689a78a44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 4. Detecting outliers\n",
    "### 4.1. Code part\n",
    "Implement a function called `detecting_outliers` to detect outliers in one selected quantitative variable. Pick a suitable variable from the tidied dataset based on your characterisation and apply one suitable outlier-detection technique as covered in Unit 4. Justify your choice of this technique in the analytical part. Again, the function is assumed to receive a tidied data set from Step 0. The function returns the row positions (*not* row labels!) of the rows containing outliers on the selected variable, sorted in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c10ec065604cff85d708868fa0909ced",
     "grade": false,
     "grade_id": "cell-f593e79eae8f4227",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # Often used alongside pandas\n",
    "\n",
    "def detecting_outliers(x):\n",
    "    \"\"\"\n",
    "    Finds outliers in the 'km_per_litre' column of a DataFrame using the IQR method.\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of index labels for the outlier rows.\n",
    "    \"\"\"\n",
    "    col_name = \"km_per_litre\"\n",
    "    df = x.copy() # Work on a copy\n",
    "\n",
    "    # Ensure the column is numeric, converting errors to NaN\n",
    "    df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "\n",
    "    # Calculate IQR bounds\n",
    "    Q1 = df[col_name].quantile(0.25)\n",
    "    Q3 = df[col_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    is_outlier = (df[col_name] < lower_bound) | (df[col_name] > upper_bound)\n",
    "    outlier_labels = df.index[is_outlier].tolist()\n",
    "\n",
    "    return sorted(outlier_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a687f420acb566a3dbe153acc0bfd16",
     "grade": true,
     "grade_id": "cell-231217b55b6ef843",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = tidy(mn);\n",
    "assert type(detecting_outliers(df)) == list, \"T4.1\"\n",
    "assert all(isinstance(i, int) for i in detecting_outliers(df)), \"T4.2\"\n",
    "assert len(detecting_outliers(df)) > 0 and len(detecting_outliers(df)) < .05*df.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e5ca4b902e63d96e5376adc95c8302d",
     "grade": false,
     "grade_id": "cell-8c4530b128b5c186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2. Analytical part\n",
    "Discuss the implications. \n",
    "\n",
    "- What is the chosen outlier-detection technique? Explain it using your own words in 3-4 sentences.\n",
    "- Describe the outliers detected: How many? How do they relate to the typical, non-outlier values in the remaining dataset?\n",
    "- What could be one reason these outliers appear in the dataset? How would you treat them further?\n",
    "\n",
    "Write your answer in the markdown cell below. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e93305db24e2fc3800f252b797c2ad7",
     "grade": true,
     "grade_id": "cell-254db63097c3ed40",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
