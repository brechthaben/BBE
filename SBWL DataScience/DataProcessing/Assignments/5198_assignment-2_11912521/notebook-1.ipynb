{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3393057-ea06-4f2d-94c7-9697fccd3a41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9397067b9282bf77aa677b093849d83d",
     "grade": false,
     "grade_id": "cell-8f51681d0010e7ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***DISCLAIMER (Read this carefully)***\n",
    "\n",
    "Before you turn this assignment in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel\n",
    "Restart) and then run all cells (in the menubar, select Cell\n",
    "Run All). Do NOT add any cells to the notebook!\n",
    "\n",
    "Do not forget to submit both the notebook AND the files in the data/ subfolder according to the CoC!\n",
    "Make sure you fill in any place that says YOUR CODE HERE or YOUR ANSWER HERE , as well as your name and group below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981a73c8-4cde-4ea3-976a-8cce8ce3207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (8.3.5)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest) (1.2.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest) (22.0)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.10/site-packages (from pytest) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# These are the libraries you are allowed to use for this assignment\n",
    "# If any of those libraries are not present in your jupyter instance, install them by using !pip install [library-name]!\n",
    "!pip install xmltodict\n",
    "import xmltodict\n",
    "import csv\n",
    "import json\n",
    "!pip install pytest\n",
    "import pytest\n",
    "import requests\n",
    "import codecs\n",
    "import math\n",
    "import re\n",
    "import requests\n",
    "import urllib.request\n",
    "import pandas\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b4ced-9b69-4c8e-a7d0-12265cc28eea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccbfb02fdc67dc903abb412ab000bf72",
     "grade": false,
     "grade_id": "cell-01518f30645ea1ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Assignment 2 (Group)\n",
    "When carrying out a Data-Science project, screening and selecting appropriate data sources for the tasks at hand comes at the beginning. This assignment is about accessing and characterising potential data sources in teams of three. The teams have been randomly assigned. BEWARE! In Assignment 5, you will be asked to provide answers to those questions. Make sure that combining the two datasets makes sense from an analytical perspective!\n",
    "\n",
    "-----\n",
    "## Step 0 (2 points)\n",
    "\n",
    "Find two data sets online (from one or several sources) that would be interesting to combine and create ***data citations*** as Python dictionaries. \n",
    "\n",
    "The data sets should fulfill the following requirements:\n",
    "\n",
    "* Each data set must have a different file format (either CSV, XML, or JSON), please choose\n",
    "  - one CSV file (dataset1) \n",
    "  - and one JSON or XML file (dataset2)\n",
    "\n",
    "* The two datasets should not be two variations of each other (i.e. simply the same dataset for two different regions or timeframes or from the same source just in two different formats)\n",
    "* Workable data-set sizes: The selected or extracted data sets should have thousands of entries (>= 1000), but not more than (<=) 10000 entries. Be \"entries we mean rows or distinguishable key-value pairs). If larger, use an excerpt from the original data set. Justify in detail the extraction criteria in the markdown cell below and \n",
    "  1) add the code used for the extraction in the code cell or describe how you filtered the sample  \n",
    "  2) make the extracted dataset also available at a downloadable URL (for instance in a Github repository, [here](https://raw.githubusercontent.com/AxelPolleres/simple_dataset_sharing_repo/main/test.csv)'s an example)\n",
    "  3) name the new `resourceURL` in the data citation.\n",
    "* You may start from (but you are not limited to) the resource collections hinted at [in the Unit 2 slides](https://datascience.ai.wu.ac.at/ws21/dataprocessing1/unit2.html#slide-53).\n",
    "\n",
    "* Important: The use of datasets from kaggle.com and other curated collections of datasets with accompanying tutorials on processing and analysis (as highlighted to you in Unit 2) is **discouraged**. You are required to use **primary data sources**: This is mainly because we want you to work on data sets that have not been processed with some analysis in mind, so that you show that you can handle (messy) data sets harvested on the brownfields of Data Science. Besides, such curated datasets have been repeatedly used in ready-made case and tutorial work, which makes it basically impossible for us to establish whether your submissions are genuine contributions of yours. There is one viable option: Work backwards from the Kaggle data set to the original data source, obtain updated data from there, and start from there.\n",
    "\n",
    "\n",
    "* Please adhere to the CoC.\n",
    "\n",
    "[Data citations](http://blogs.nature.com/scientificdata/2016/07/14/data-citations-at-scientific-data/) must contain the following details:\n",
    "- creator: provider organisation / author(s) of the data set, e.g. \"Zentralanstalt für Meteorologie und Geodynamik (ZAMG)\"\n",
    "- catalogName: Names of the data repository and/or the Open Data portal used, e.g. Open Data Österreich\"\n",
    "- catalogURL: URL of th repository / portal, e.g. \"https://www.data.gv.at/\"\n",
    "- datasetID: (specific to the data repository), e.g. \"https://www.data.gv.at/katalog/dataset/zamg_meteorologischemessdatenderzamg\"\n",
    "- resourceURL: a URL where the CSV, XML or JSON file can be downloaded, e.g. \"https://www.football-data.co.uk/new/JPN.csv\"\n",
    "- pubYear: Dataset publication year, i.e. since when it is published, e.g. \"2012\"\n",
    "- lastAccessed: when have you last accessed the dataset (i.e. datetime of accessing, obtaining a copy of the data set) in ISO Format? e.g. \"2021-03-08T13:55:00\"\n",
    "\n",
    "One final note: as mentioned above, if you want to use a repository for your file download (e.g. github), you are allowed to do that. The most important part is that the URL can be accessed stably for each dataset you have chosen. \n",
    "\n",
    "Store the data citation in a dictionary for each of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ac2b05-1102-4cf7-96a3-f1ec47459ece",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b6ee8b15f725340d21f67892d57f1cf",
     "grade": true,
     "grade_id": "cell-cea2669d70d8d76a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We cut down the data, using a script that first delets the data which isnt in the correct time period (so that both datasets are in teh same time period) and then randomly selects 10000 samples. The code for both the CSV and JSON can be found in the respective Github Repos.\n",
    "dataset1 = {\n",
    "    \"creator\": \"U.S. Department of Housing and Urban Development (HUD)\",\n",
    "    \"catalogName\": \"Neighborhood Data for Social Change\",\n",
    "    \"catalogURL\": \"https://la.myneighborhooddata.org/\",\n",
    "    \"datasetID\": \"https://la.myneighborhooddata.org/2021/06/area-income-limits/\",\n",
    "    \"resourceURL\": \"https://raw.githubusercontent.com/anjatheanja/Assignment-2/main/final_processed_data.csv\",\n",
    "    \"pubYear\": \"2021\",\n",
    "    \"lastAccessed\": \"2025-03-22T19:28:34\",\n",
    "}\n",
    "\n",
    "dataset2 = {\n",
    "    \"creator\": \"Los Angeles Police Department\",\n",
    "    \"catalogName\": \"Crime Data from 2010 to 2019\",\n",
    "    \"catalogURL\": \"https://data.lacity.org\",\n",
    "    \"datasetID\": \"https://data.lacity.org/Public-Safety/Crime-Data-from-2010-to-2019/63jg-8b9z/about_data\",\n",
    "    \"resourceURL\": \"https://raw.githubusercontent.com/brechthaben/BBE/main/SBWL%20DataScience/DataProcessing/Assignments/Assignment2/processed_data.json\",\n",
    "    \"pubYear\": \"2019\",\n",
    "    \"lastAccessed\": \"18.03.2024\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852966b3-7ce6-47e0-a879-2de74b756861",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12168528df39434b1523d55f040752fe",
     "grade": true,
     "grade_id": "cell-d3cc293b6207d1c7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "\n",
    "assert type(dataset1) == dict\n",
    "assert type(dataset2) == dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c1bfa-decc-4711-83c1-87c761bda55e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12991e2a6ff07860ffd880d80927f39e",
     "grade": false,
     "grade_id": "cell-a34ab569805a650e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Please answer the questions in the cell below\n",
    "\n",
    "Use the following structure for your answer below:\n",
    "\n",
    "Data set 1\n",
    "\n",
    "(Describe the source and the general content of the dataset and why you chose it)\n",
    "\n",
    "Data set 2\n",
    "\n",
    "(Describe the source and the general content of the dataset and why you chose it)\n",
    "\n",
    "Project ideas\n",
    "\n",
    "(Describe in your own words, which kind of tasks could be addressed by combining the selected data sets, esp. how the two data sets fit together and what complementary information they contain; Formulate a question that could be potentially answered by combining data from both datasets; how could the data sets be combined exactly? 250 words max. BEWARE! In Assignment 5, you will be asked to provide answers to those questions. Make sure that combining the two datasets makes sense from an analytical perspective!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b04cd31-a41c-4d42-b756-87567ed08ee8",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25e589c21292c28ecbe9193e15daa7b9",
     "grade": true,
     "grade_id": "cell-9bc09f21e0c42050",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Data set 1\n",
    "This was the second data set that was found. Since we were looking for something that could coorelate to the first dataset in a way - it was very hard to find something this tailored. It is not optimal, especially because of the regions but it does provide us with information about household income which can be a great factor for analysis.\n",
    "\n",
    "Data set 2\n",
    "\n",
    "We chose this dataset after a long search. We wanted something with crime data and then compare them to socioeconomic factors. This was one of the few datasets we found which was detailed enough and where we could find a dataset with those factors. The source is the Los Angeles Police Department. It originally included 2.1 million individual crime reports from 2010 to 2019 but also got cut down to 10000. The code for this can be found in the same directory on github.\n",
    "\n",
    "Project ideas\n",
    "\n",
    "We hope to investigate the crime rates for poorer and richer areas in Los Angeles. We should be able to map the regions of both onto eachother. This would involve a third dataset which links streets to a specific area indicator, like the postal code. \n",
    "\n",
    "Potential questions could be: \n",
    "What crime is most common in different income brackets?\n",
    "Does crime happen more often in poorer neighborhoods?\n",
    "Is there is a proportion of increase in crime rate vs income decrease?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a43ce55-9c0b-44b2-a833-8e4e107175da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4c6631a22555c4c784b89e218ae1012",
     "grade": false,
     "grade_id": "cell-426859b26b9c84e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## Step 1 - File Access (3 points)\n",
    "\n",
    "Write a Python function `accessData` that takes the dataset dictionary created in step 0 as an input and returns an extended dictionary including following additions:\n",
    "\n",
    "* Write code that accesses the dataset from its `resourceURL` using the python `requests` package:\n",
    " * detects whether it's and XML, CSV or JSON file by\n",
    "     * checking whether the download URL **ends** with suffix \"xml\", \"json\", \"csv\" (in either upper- or lowercase)\n",
    "     * checking whether the \"Content-Type\" HTTP header field contains information about the format, hinting on XML, JSON or CSV, i.e., check whether the substring XML, JSON or CSV appears in the \"Content-Type\" header in either upper- or lowercase. \n",
    " * Detects the file size from the HTTP header (converted to KB) of each data set, clearly documenting your actions (e.g. through commented code).\n",
    "\n",
    "The result of the code below should extend your dictionaries `dataset1` and `dataset2` with two keys named \n",
    "* `\"detectedFormat\"` (which has one of the following values: `\"XML\"`, `\"JSON\"`, `\"CSV\"`, or `\"unknown\"`, if nothing could be detected from checking the suffix or HTTP header, or if the information in both was inconsistent)\n",
    "* and `\"filesizeKB\"` which contains the filesize in KB (Conversion should be done accordingly to decimal SI prefixes) from the number of bytes in the header-information. If there is no respective header information return 0.\n",
    "* If the detected format is `\"unknown\"`, the expected filesize to be returned is also 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06da511-204d-4316-bc92-507835acbe27",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a1ca537ab893786c9e466e4490d8a8b",
     "grade": false,
     "grade_id": "cell-87173edcb1445261",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "File size: 451.551 KB\n",
      "Detected format: CSV\n",
      "\n",
      "Dataset 2:\n",
      "File size: 764.137 KB\n",
      "Detected format: JSON\n"
     ]
    }
   ],
   "source": [
    "def accessData(datadict):    \n",
    "    \n",
    "    #Fetching resource URL from dictionary\n",
    "    URL = datadict.get(\"resourceURL\", \"\")\n",
    "    \n",
    "    #initial format and size\n",
    "    detectedFormat = \"\"\n",
    "    filesizeKB = 0\n",
    "\n",
    "    #Send Head request and save headers\n",
    "    response = requests.head(URL)\n",
    "    headers = response.headers\n",
    "\n",
    "    # Checking download URL suffix -> either small or capital\n",
    "    #for csv \n",
    "    if \".csv\" in URL or \".CSV\" in URL:\n",
    "        detectedFormat = \"CSV\"\n",
    "    #for json     \n",
    "    elif \".json\" in URL or \".JSON\" in URL:\n",
    "        detectedFormat = \"JSON\"\n",
    "    #for xml \n",
    "    elif \".xml\" in URL or \".XML\" in URL:\n",
    "        detectedFormat = \"XML\"\n",
    "\n",
    "    #Checking if headers have hint suffix in Content-Type\n",
    "    if detectedFormat == \"\" and \"Content-Type\" in headers:\n",
    "        \n",
    "        # Pick only Content-Type from headers\n",
    "        contentType = headers[\"Content-Type\"]\n",
    "        \n",
    "        #same as suffix from download URL\n",
    "        #for csv\n",
    "        if \"csv\" in contentType or \"CSV\" in contentType:\n",
    "            detectedFormat = \"CSV\"\n",
    "            \n",
    "        #for json\n",
    "        elif \"json\" in contentType or \"JSON\" in contentType:\n",
    "            detectedFormat = \"JSON\"\n",
    "        \n",
    "        #for xml\n",
    "        elif \"xml\" in contentType or \"XML\" in contentType:\n",
    "            detectedFormat = \"XML\"\n",
    "\n",
    "    #choose only content length from headers\n",
    "    if \"Content-Length\" in headers:\n",
    "        \n",
    "        #fetch content length as integer and translate bytes to kb -> ( 1000 bytes = 1kB )\n",
    "        filesizeKB = int(headers[\"Content-Length\"]) / 1000 \n",
    "\n",
    "    #add the variables to the dictionary\n",
    "    datadict[\"filesizeKB\"] = filesizeKB\n",
    "    datadict[\"detectedFormat\"] = detectedFormat\n",
    "    \n",
    "    return datadict  \n",
    "# Process both datasets\n",
    "accessData(dataset1)\n",
    "accessData(dataset2)\n",
    "\n",
    "print(\"Dataset 1:\")\n",
    "print(f\"File size: {dataset1['filesizeKB']} KB\")\n",
    "print(f\"Detected format: {dataset1['detectedFormat']}\")\n",
    "\n",
    "print(\"\\nDataset 2:\")\n",
    "print(f\"File size: {dataset2['filesizeKB']} KB\")\n",
    "print(f\"Detected format: {dataset2['detectedFormat']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba078d8-b700-4cb8-8a18-4a2fa86dc210",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fbfcaf8282114a8d61c3cae0cea5ee4",
     "grade": true,
     "grade_id": "cell-09b529ecf9606ac6",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic tests to see if your solution meets the foundational demands described in the task description\n",
    "dataset1 = accessData(dataset1)\n",
    "dataset2 = accessData(dataset2)\n",
    "if dataset1[\"detectedFormat\"] not in [\"XML\", \"JSON\", \"CSV\", \"unknown\"]:\n",
    "    raise AssertionError\n",
    "if dataset2[\"detectedFormat\"] not in [\"XML\", \"JSON\", \"CSV\", \"unknown\"]:\n",
    "    raise AssertionError\n",
    "\n",
    "assert isinstance(dataset1[\"filesizeKB\"],(int, float)) == True\n",
    "assert isinstance(dataset2[\"filesizeKB\"],(int, float)) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8134ce01-5e26-4fd9-b7be-c83135572eea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a790e5e6ab835612be43bf7d51d7033",
     "grade": true,
     "grade_id": "cell-1c560982a2f27ace",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! 😼⚡️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f60a646-215e-4b49-8c2d-fbccbef78af4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e2767aef997a735d1d876e1b23e6169",
     "grade": true,
     "grade_id": "cell-6f7cbd4186584572",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I know you are reading this, there is no help here, no hope. Also, do not delete me please :3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa69593-670f-4b2c-b5d0-1c70c9970b92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "612a790836aa21e3a63991348694e9b4",
     "grade": false,
     "grade_id": "cell-dcd3aa38b0b6d216",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Please answer the questions in the cell below\n",
    "\n",
    "#### Please explain your findings, using the following structure for your answer below (in \"other remarks\" you can explain, for instance, why you think your code did not detect the correct format, if needed)\n",
    "\n",
    "Data set 1\n",
    "\n",
    "(format, size, other remarks)\n",
    "\n",
    "Data set 2\n",
    "\n",
    "(format, size, other remarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ffd91-c334-43fd-aa68-ae9aeb9ae24b",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25ee6f9a55d5499eae679e146a2fb0c3",
     "grade": true,
     "grade_id": "cell-40248ad63aa2baea",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Data set 1 \n",
    "\n",
    "File size: 451.551 KB\n",
    "Detected format: CSV\n",
    "\n",
    "\n",
    "Data set 2\n",
    "File size: 764.137 KB\n",
    "Detected format: JSON\n",
    "\n",
    "This is a JSON which is 764137KB large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26a4ed-ce75-4a64-8c94-0eff3cb8de7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8ba2987efce440ec7e6f4d4857bd88e",
     "grade": false,
     "grade_id": "cell-c236c8ec72c4dded",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "## Step 2  (5 points) - Format Validation\n",
    "\n",
    "Establish that the two data files obtained are well-formed according to the detected data format (CSV, JSON, or XML). That is, the syntax used is valid according to accepted syntax definitions. Are there any violations of well-formedness?\n",
    "\n",
    "\n",
    "Proceed as follows (for each data file, in turn): according to the \"suspected\" data format from Step 1:\n",
    "\n",
    "  1. Use an _online validator_ for CSV, XML, and JSON, respectively, to confirm whether the files you downloaded in Step 1 are well-formed for the respective file format, document your findings and modify the file as described: \n",
    "\n",
    "   a. **Case 1**: no well-formedness errors were detected: \n",
    "    * Generally describe at least 3 well-formedness checks that your data sets, depending on its \"suspected\" format (against the background knowledge of Unit 2) should fulfill;\n",
    "    * Store a local copy of the file called `data_notebook-[notebook-nr.]_[name].[file extension]` in the `data/` subfolder\n",
    "    * Create another local copy of your data file called `data_notebook-[notebook-nr.]_[name]-invalid.[file extension]` and introduce a selected well-formedness violation (one occurrence) therein;\n",
    "    * document that the online validator you used finds the error you introduced\n",
    "\n",
    "   b. **Case 2**: well-formedness errors occurred:\n",
    "    * Document the occurrences by printing out the error message and describe the types of well-formedness violation that were reported to you.\n",
    "    * Store a local copy called `data_notebook-[notebook-nr.]_[name]-invalid.[file extension]`  in the `data/ subfolder`\n",
    "    * Create another local copy called `data_notebook-[notebook-nr.]_[name].[file extension]`, of your data file that fixes the well-formedness violations therein manually.  \n",
    "    \n",
    "**Please note that the datasets in the `data/` subfolder are for documentation only. Do not access those for subsequent steps!**\n",
    "    \n",
    "\n",
    "  2. Write a Python function `parseFile(datadict, format)` that that accesses the dataset from its `resourceURL`. The dataset should then be checked accordingly the given parser for the parameter `format` to check the following:\n",
    "     * CSV: Returns `True`, if a consistent delimiter out of `\",\",\";\",\"\\t\"` can be detected, such that each row has the same (> 1) number of elements, otherwise False\n",
    "     * JSON: Returns `True` if the file can be parsed with the `json` package, catching any parsing exceptions.\n",
    "     * XML: Returns `True` if the file can be parsed with the `xmltodict` package, catching any parsing exceptions.\n",
    "     * Returns `False` if any other format is supplied by the parameter.\n",
    "     \n",
    "In order to handle parsing exceptions and errors from the used packages, you can use [catching exceptions](https://docs.python.org/3/tutorial/errors.html), such that the program does not simply fail to check whether the file is parseable as the format specified in `format`    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d77f70-6543-4aab-9306-010a706fd09b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "356da61d941023bc467b54542264c09d",
     "grade": false,
     "grade_id": "cell-57dc295d9edd354d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Please answer the questions in this cell\n",
    "\n",
    "### Use the following structure for your answer in the cell below to document **Step 2.1**:\n",
    "\n",
    "***Data set 1***\n",
    "\n",
    "*(validator used, validation results, describe the modification to fix the file or to create an invalid version of it)*\n",
    "\n",
    "***Data set 2***\n",
    "\n",
    "*(validator used, validation results, describe the modification to fix the file or to create an invalid version of it)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec87d67-75e8-4bc7-9502-f24598959a5e",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdffe13535005452547b95c914f7071b",
     "grade": true,
     "grade_id": "cell-7326ff597819ce15",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Data set 1\n",
    "We used an online CSV validator. No errors were detected. To create an invalid version, we could have an inconsistent ammount of collumns. Adding special characters or creating inconsistent ammount of commas will also cause the file to be invalid. Well formedness can come from consistent collumns, consistent delimitors and consistent field quotation.\n",
    "\n",
    "\n",
    "\n",
    "Data set 2\n",
    "\n",
    "The JSON validator had no issues either. To create an invalid versison we could delete braces, use brackets and braces interchangeable or dont use quotation marks. Similarly the well formedness comes from consistent braces, delimiters and quotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b5bfb28-0a4f-4fbe-a2ed-4d8e9c3d52d8",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4732c1de34a7254c7e8f92d1a8d6f72",
     "grade": false,
     "grade_id": "cell-e72d44bc996d8aaf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import xmltodict\n",
    "import io  # Used to convert string content to a file-like object for CSV parsing\n",
    "\n",
    "def parseFile(datadict, format):\n",
    "   \n",
    "     # Get the URL from the dictionary. Use an empty string if not found.\n",
    "    resource_url = datadict.get(\"resourceURL\", \"\")\n",
    "    \n",
    "    try:\n",
    "        # Download the file content from the resourceURL\n",
    "        response = requests.get(resource_url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        content = response.text\n",
    "    except Exception as e:\n",
    "        \n",
    "        # In case of any error, return False.\n",
    "        return False\n",
    "    \n",
    "    # Convert the format to uppercase to avoid case issues.\n",
    "    format_upper = format.upper()\n",
    "    \n",
    "    if format_upper == \"CSV\":\n",
    "        # For CSV, check for a consistent delimiter among [\",\", \";\", \"\\t\"]\n",
    "        possible_delimiters = [\",\", \";\", \"\\t\"]\n",
    "        consistent = False\n",
    "        \n",
    "        # Try each possible delimiter.\n",
    "        for delim in possible_delimiters:\n",
    "            \n",
    "            # Create a file-like object from the downloaded text.\n",
    "            f = io.StringIO(content)\n",
    "            reader = csv.reader(f, delimiter=delim)\n",
    "            rows = list(reader)\n",
    "            \n",
    "            if not rows:\n",
    "                \n",
    "             # If no rows are found, try the next delimiter.\n",
    "                continue\n",
    "                \n",
    "            # Use the first row to determine the expected number of columns.\n",
    "            expected_columns = len(rows[0])\n",
    "            \n",
    "            # Only consider valid if there is more than one column\n",
    "            if expected_columns <= 1:\n",
    "                continue\n",
    "                \n",
    "            # Check that every row has the same number of columns\n",
    "            if all(len(row) == expected_columns for row in rows):\n",
    "                consistent = True\n",
    "                break # A good delimiter is found, so stop the loop.\n",
    "        return consistent\n",
    "    \n",
    "    elif format_upper == \"JSON\":\n",
    "        try:\n",
    "            # Try to parse the file content using the json package\n",
    "            json.loads(content)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    elif format_upper == \"XML\":\n",
    "        try:\n",
    "            # Try to parse the file content using the xmltodict package\n",
    "            xmltodict.parse(content)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return False\n",
    "    else:\n",
    "        # If the format is not recognized, return False.\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce5b307-c87b-4906-afc6-9c1febd645e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9281d46b50b1db2eb696cf556870930c",
     "grade": true,
     "grade_id": "cell-44a0730c406d4f1f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This checks if your code returns true twice for either xml, json or csv for your datasets specifically.\n",
    "\n",
    "assert([parseFile(dataset1, \"XML\"),\n",
    "    parseFile(dataset1, \"JSON\"),\n",
    "    parseFile(dataset1, \"CSV\"),\n",
    "    parseFile(dataset2, \"XML\"),\n",
    "    parseFile(dataset2, \"JSON\"),\n",
    "    parseFile(dataset2, \"CSV\")].count(True)) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749dd2d7-e366-4cd9-8186-a1ac278e9e9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f74ccd979eba3be80df1b3a134d0789a",
     "grade": true,
     "grade_id": "cell-c8199bfc375750df",
     "locked": true,
     "points": 3.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There will be consequences if you remove this cell, or even move this cell. Be wary of that <3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14967cf4-e8da-4e9a-90a9-752f1f441915",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a27a734c0b600b5516729b67f6e577d",
     "grade": false,
     "grade_id": "cell-c0772a5952f0b1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "## Step 3 - Content analysis (5 points)\n",
    "\n",
    "Similar to the Python function `parseFile(datadict,format)` above, now create a new Python function `describeFile(datadict)` that analyses the given file according to the respective format detected in Step 1 and returns a dictionary containing the following information:\n",
    "\n",
    "* for CSV files: number of numeric (float) columns, number of rows, column index (from 0 to n) and row index of the entry which contains the largrst float value which is a round number (modulo 2 with a remainder of 0). Ensure to try to transform any entry to a numeric type to ensure no number is accidentely seen as a string. The resulting dictionary should have the following form:\n",
    "\n",
    "    ```\n",
    "    { \"numberOfNumericColumns:\"  ...,\n",
    "       \"numberOfRows\":  ... ,\n",
    "       \"largestFloat\" : ... }\n",
    "    ```\n",
    "\n",
    "* for JSON files: number of distinct attribute names, nesting depth, length of the longest list appearing in an attribute value. That is, the resulting dictionary should have the following form:\n",
    "\n",
    "    ```\n",
    "    { \"numberOfAttributes:\" ... ,\n",
    "      \"nestingDepth\":  ... ,\n",
    "      \"longestListLength\" : ... }\n",
    "     ```\n",
    "\n",
    "  Here the `longestListLength` should be set to 0 if no list appears. [Nesting depth](https://www.tutorialspoint.com/find-depth-of-a-dictionary-in-python) is defined as follows: \n",
    "   * a flat list of atomic values has depth 0, a flat JSON object with only atomic attribute values has depth 1. \n",
    "   * a JSON attribute with another object as value (or another object as member of a list value!) increases the depth by 1\n",
    "   * and so on.\n",
    "\n",
    "\n",
    "* for XML files: number of different element and attribute a names (i.e. the sum of both), nesting depth, maximum numeric value in the dataset. That is, the resulting dictionary should have the following form:\n",
    "\n",
    "    ```\n",
    "    { \"numberOfElementsAttributes:\" ... ,\n",
    "      \"nestingDepth\":  ... ,\n",
    "      \"longestString\" : ... }\n",
    "     ```\n",
    "\n",
    "  Here the `longestString` should be set to \"\" if there are no String values present. Nesting depth is defined as the nesting depth of elements.\n",
    "  \n",
    "For files that cannot be parsed with respective given format, the function should simply return an empty dictionary (`{}`).\n",
    "\n",
    "##### Please ensure to keep the order of the elements as specified above! The test cases assume that your result has the same structure as the ones specified above. Basically this means for the csv e.g. that the first position should ALWAYS be the numberOfNumericColumns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc7a03d-610f-4591-94c5-b3fe6af3517a",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e3f7bd8fbced4b87d03372a0266b509",
     "grade": false,
     "grade_id": "cell-2123d8521820ec726",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def describeFile(datadict):\n",
    "\n",
    "    #getting all the needed info from the dictionary\n",
    "    URL = datadict.get(\"resourceURL\", \"\")\n",
    "    detectedFormat = datadict.get(\"detectedFormat\", \"\")\n",
    "\n",
    "    # Basic check to see if the URL or format is missing\n",
    "    if not URL or not detectedFormat:\n",
    "        return {}\n",
    "\n",
    "    #if the format -> CSV :\n",
    "    if detectedFormat == \"CSV\":\n",
    "        try:\n",
    "            #download csv from url as a string\n",
    "            # added timeout to be kind to the server :)\n",
    "            response = requests.get(URL, timeout=10)\n",
    "            response.raise_for_status() # Raise an exception for bad status codes\n",
    "            content = response.text\n",
    "\n",
    "            # now we Check for empty content after stripping whitespace\n",
    "            if not content.strip():\n",
    "                return {}\n",
    "\n",
    "            #remove spaces & split into rows\n",
    "            #Using splitlines() for better cross-platform line ending support\n",
    "            rows = content.strip().splitlines()\n",
    "\n",
    "            # not needed here, but for futureproofing we check if the file is empty or only a header\n",
    "            if not rows:\n",
    "                return {}\n",
    "            header_row = rows[0]\n",
    "            data_rows = rows[1:]\n",
    "\n",
    "            # here we check for the case, that there are no data rows\n",
    "            if not data_rows:\n",
    "                 return {\n",
    "                    \"numberOfNumericColumns\": 0,\n",
    "                    \"numberOfRows\": 0,\n",
    "                    # we use (-1, -1) to indicate not found, as requested index starts from 0\n",
    "                    \"largestFloat\": (-1, -1)\n",
    "                }\n",
    "\n",
    "            #try these delimeters for file separation\n",
    "            potential_delimiters = [\",\",\";\",\"\\t\", \"|\"]\n",
    "            best_delimiter = None\n",
    "            splitRows = []\n",
    "            num_cols_header = 0 #Store header column count for consistency check\n",
    "\n",
    "            # here we try to detec the delimiter being used\n",
    "            for delimiter in potential_delimiters:\n",
    "                # Check delimiter on header first\n",
    "                header_parts = header_row.split(delimiter)\n",
    "                current_num_cols = len(header_parts)\n",
    "                if current_num_cols <= 1: # Skip if delimiter doesn't split header meaningfully\n",
    "                    continue\n",
    "\n",
    "                temp_splitRows = []\n",
    "                consistent = True\n",
    "                for row_str in data_rows:\n",
    "                    parts = row_str.split(delimiter)\n",
    "                    # Check if row has more columns than header - this indicates wrong delimiter\n",
    "                    if len(parts) > current_num_cols:\n",
    "                        consistent = False\n",
    "                        break\n",
    "                    # Pad rows that are shorter than the header\n",
    "                    parts.extend([''] * (current_num_cols - len(parts)))\n",
    "                    temp_splitRows.append(parts)\n",
    "\n",
    "                if consistent:\n",
    "                    best_delimiter = delimiter\n",
    "                    splitRows = temp_splitRows\n",
    "                    num_cols_header = current_num_cols\n",
    "                    break # Found a consistent delimiter\n",
    "\n",
    "            # now for the case, that our delimiters arent enough: If no consistent delimiter worked based on header length, return empty\n",
    "            if best_delimiter is None:\n",
    "                # Maybe add a fallback here if needed, but for now stick to consistency check\n",
    "                return {}\n",
    "\n",
    "\n",
    "            # Get number of rows excluding header\n",
    "            numberOfRows = len(splitRows)\n",
    "            # num_cols = len(splitRows[0]) if numberOfRows > 0 else num_cols_header\n",
    "            num_cols = num_cols_header # Use the number of columns derived from the header and consistent rows\n",
    "\n",
    "            numericColumnIndices = []\n",
    "\n",
    "            # Go through each row by the index based on rows legth \n",
    "            for i in range(num_cols):\n",
    "                #assume the column is numeric\n",
    "                isNumeric = True\n",
    "                # Loop through the each row, skip header and only include non-empty rows, convert into float if can\n",
    "                for row in splitRows:\n",
    "                    # now we Check index bounds explicitly although padding should handle it\n",
    "                    if i < len(row):\n",
    "                        value_str = row[i].strip() # here we Strip whitespace from value\n",
    "                        if value_str != \"\": # and now we test non-empty strings to really be bulletproof\n",
    "                            try:\n",
    "                                float(value_str)\n",
    "                            except (ValueError, TypeError): # Catch TypeError as well\n",
    "                                isNumeric = False\n",
    "                                break # Once a non-numeric is found, stop checking this column\n",
    "\n",
    "                if isNumeric:\n",
    "                    # here we Store the index of the numeric column\n",
    "                    numericColumnIndices.append(i)\n",
    "\n",
    "            #how many numeric columns were found\n",
    "            numberOfNumericColumns = len(numericColumnIndices)\n",
    "\n",
    "            #looking for the largest even float ( + the position)\n",
    "            largestValue = None\n",
    "            # explanation: (columnIndex, rowIndex)\n",
    "            # we Initialize position to indicate \"not found\" clearly. see above for context\n",
    "            position = (-1, -1)\n",
    "\n",
    "            #loop through every row and cell -> looking for even float\n",
    "            for rowIndex, row in enumerate(splitRows):\n",
    "                #only in confirmed numeric columns -> avoids string based IDs and all of that\n",
    "                for columnIndex in numericColumnIndices:\n",
    "                    try:\n",
    "                        #get the value from the num column\n",
    "                        value = row[columnIndex]\n",
    "                        # now we again Strip whitespace before converting\n",
    "                        value_str = value.strip()\n",
    "                        if value_str: # Check if string is not empty after stripping\n",
    "                             #if the num can be divided by 2 -> even, number is numeric (converted into float)\n",
    "                            number = float(value_str)\n",
    "                            # Check if it's effectively an integer (no fractional part) and even\n",
    "                            # Also check number == number to exclude NaN/inf which cause errors with modulo\n",
    "                            if number == number and number % 1 == 0 and number % 2 == 0:\n",
    "                                # Store the found current largest value and get its position\n",
    "                                if largestValue is None or number > largestValue:\n",
    "                                    largestValue = number\n",
    "                                    # keep in mind: rowIndex is 0-based index within splitRows (data rows)\n",
    "                                    position = (columnIndex, rowIndex)\n",
    "                    except (ValueError, TypeError): # Catch TypeError again \n",
    "                        #ignore if it is not a number\n",
    "                        continue\n",
    "\n",
    "            # here we want to ensure the key order matches the required specification\n",
    "            return {\n",
    "                \"numberOfNumericColumns\": numberOfNumericColumns,\n",
    "                \"numberOfRows\": numberOfRows,\n",
    "                \"largestFloat\": position\n",
    "            }\n",
    "\n",
    "        except requests.exceptions.RequestException as e: # we try to catch specific request errors\n",
    "            print(f\"TODO - CHANGE COMMENT: CSV Request failed: {e}\") # helps with debugging\n",
    "            return {}\n",
    "        except Exception as e: # and now we (dont) want to Catch other potential processing errors\n",
    "            print(f\"TODO - CHANGE COMMENT: CSV Processing failed: {e}\") # helps with debugginh\n",
    "            return {}\n",
    "\n",
    "    #if format -> JSON then:\n",
    "    elif detectedFormat == \"JSON\":\n",
    "        try:\n",
    "            #fetching JSON data from URL\n",
    "            # again a timeout to ensure we dont upset the robot overlords\n",
    "            response = requests.get(URL, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            # we again try to Handle potential empty response body before JSON parsing\n",
    "            if not response.content:\n",
    "                 return {}\n",
    "            data = response.json()\n",
    "\n",
    "            #counting the unique attribute names\n",
    "            def collectAttributes(obj, names_set):\n",
    "                 #if we have a dictionary -> (example: {\"name\": \"John\", \"age\": 25})\n",
    "                if isinstance(obj, dict):\n",
    "                    #add keys from current dict\n",
    "                    for key in obj.keys():\n",
    "                        names_set.add(key)\n",
    "                    #explore the values (check for nested dict/ or lists)\n",
    "                    for value in obj.values():\n",
    "                        collectAttributes(value, names_set)\n",
    "                #if list -> go through every item & find more keys\n",
    "                elif isinstance(obj, list):\n",
    "                    for item in obj:\n",
    "                        collectAttributes(item, names_set)\n",
    "\n",
    "            attributeNames = set() #lets give our attributes a nice place to live in <3\n",
    "            collectAttributes(data, attributeNames) \n",
    "\n",
    "\n",
    "            #function for finding the longest nesting depth\n",
    "            def findDepth(obj):\n",
    "                #if obj is a dictionary -> {\"name\": \"John\", \"age\": 25}\n",
    "                if isinstance(obj, dict):\n",
    "                    # the base depth for an object is 1\n",
    "                    max_child_depth = 0\n",
    "                    # If dict is empty, depth is 1. Otherwise, 1 + max depth of children.\n",
    "                    if not obj:\n",
    "                        return 1\n",
    "                    #go through each value in the dictionary\n",
    "                    for value in obj.values():\n",
    "                        # now we recursively find depth of each value\n",
    "                        depth = findDepth(value)\n",
    "                        if depth > max_child_depth:\n",
    "                            max_child_depth = depth\n",
    "                    # this gives us the max depth +1 because of the base depth\n",
    "                    return 1 + max_child_depth\n",
    "\n",
    "                #if obj is a list\n",
    "                elif isinstance(obj, list):\n",
    "                    # the Base depth for a list is 0, depth is max depth of items.\n",
    "                    max_item_depth = 0\n",
    "                    # If list is empty, depth is 0. Otherwise, max depth of items.\n",
    "                    if not obj:\n",
    "                        return 0\n",
    "                    # Go through each item in the list\n",
    "                    for item in obj:\n",
    "                         # we again find the depth of each item \n",
    "                        depth = findDepth(item)\n",
    "                        if depth > max_item_depth:\n",
    "                            max_item_depth = depth\n",
    "                    # and now we Return the max depth found among items (list itself doesn't add depth per definition)\n",
    "                    return max_item_depth\n",
    "\n",
    "                #if obj is a number or a string\n",
    "                else:\n",
    "                    # Atomic values have depth 0 according to definition\n",
    "                    return 0\n",
    "\n",
    "            #function to find longest list anywhere in the JSON file\n",
    "            def findLongestList(obj):\n",
    "                longest = 0\n",
    "\n",
    "                # If we have a dictionary -> checking for lists / nested structures\n",
    "                if isinstance(obj, dict):\n",
    "                    for value in obj.values():\n",
    "                        if isinstance(value, list):\n",
    "                             # we want to Compare current list length with longest found so far\n",
    "                            longest = max(longest, len(value))\n",
    "                        # and now we Recurse into nested dicts/lists\n",
    "                        if isinstance(value, (dict, list)):\n",
    "                            longest = max(longest, findLongestList(value))\n",
    "\n",
    "                #if we have a list -> check length / nested structures\n",
    "                elif isinstance(obj, list):\n",
    "                     # we also Consider the length of the current list itself\n",
    "                    longest = max(longest, len(obj))\n",
    "                    for item in obj:\n",
    "                         # again Recurse! into nested dicts or lists (again)\n",
    "                        if isinstance(item, (dict, list)):\n",
    "                            longest = max(longest, findLongestList(item))\n",
    "\n",
    "                return longest\n",
    "\n",
    "            #create result dictionary\n",
    "            result = {\n",
    "                \"numberOfAttributes\": len(attributeNames), \n",
    "                \"nestingDepth\": findDepth(data),\n",
    "                \"longestListLength\": findLongestList(data)\n",
    "            }\n",
    "\n",
    "            return result\n",
    "\n",
    "        except requests.exceptions.RequestException as e: # again error handling to make this even more powerful (unicode U+1F4AA for the emoji enjoyers;) )\n",
    "            print(f\"TODO - CHANGE COMMENT: JSON Request failed: {e}\") \n",
    "            return {}\n",
    "        except json.JSONDecodeError as e: \n",
    "            print(f\"TODO - CHANGE COMMENT: JSON Parsing failed: {e}\") \n",
    "            return {}\n",
    "        except Exception as e: \n",
    "            print(f\"TODO - CHANGE COMMENT: JSON Processing failed: {e}\") \n",
    "            return {}\n",
    "\n",
    "    #if the file is xml\n",
    "    elif detectedFormat == \"XML\":\n",
    "        # now we Implement the XML analysis, eventhough we dont have an XML. we werent quite sure if this is necessary but dont want points deducted\n",
    "        try:\n",
    "            # now we Fetch XML data from URL like a cowboy\n",
    "            response = requests.get(URL, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            # we Use response.content, because Dr. Google said it handles encoding better ¯\\_(ツ)_/¯\n",
    "            xml_content = response.content\n",
    "            # now we want to handle empty responses again\n",
    "            if not xml_content.strip():\n",
    "                 return {}\n",
    "\n",
    "            # finally we get to the processing: we parse the XML content\n",
    "            root = ET.fromstring(xml_content)\n",
    "\n",
    "            # our little helper function will collect element and attribute names\n",
    "            def collect_xml_names(element, names_set):\n",
    "                if element is None:\n",
    "                    return\n",
    "                # we add the tag name of the current element\n",
    "                names_set.add(element.tag)\n",
    "                # then we add all attribute names of the current element\n",
    "                for attr_name in element.attrib:\n",
    "                    names_set.add(attr_name)\n",
    "                # now we recursively call for all child elements\n",
    "                for child in element:\n",
    "                    collect_xml_names(child, names_set)\n",
    "\n",
    "            # this little guy will calculate the XML nesting depth \n",
    "            def get_xml_depth(element):\n",
    "                # in XML an element itself has depth 1\n",
    "                if not list(element): # Check if the element has any child elements\n",
    "                    return 1\n",
    "                else:\n",
    "                    # and now we calculate depth recursively for children\n",
    "                    max_child_depth = 0\n",
    "                    for child in element:\n",
    "                       child_depth = get_xml_depth(child)\n",
    "                       if child_depth > max_child_depth:\n",
    "                           max_child_depth = child_depth\n",
    "                    # the depth is 1 (for current element) + max depth of children\n",
    "                    return 1 + max_child_depth\n",
    "\n",
    "            # the last helper function to find the longest string in text or attributes\n",
    "            def find_longest_xml_string(element, current_longest):\n",
    "                 # Tfirst we check the element's text content\n",
    "                 if element.text:\n",
    "                     text = element.text.strip()\n",
    "                     if len(text) > len(current_longest):\n",
    "                         current_longest = text\n",
    "\n",
    "                 # then we check the element's attribute values\n",
    "                 for attr_value in element.attrib.values():\n",
    "                     # Ensure value is a string before checking length\n",
    "                     if isinstance(attr_value, str):\n",
    "                         if len(attr_value) > len(current_longest):\n",
    "                             current_longest = attr_value\n",
    "\n",
    "                 # and now we recursively check children, passing the longest found so far\n",
    "                 for child in element:\n",
    "                     current_longest = find_longest_xml_string(child, current_longest)\n",
    "\n",
    "                 return current_longest # Return the longest string found in this branch\n",
    "\n",
    "\n",
    "            # Home stretch! We Calculate the required values\n",
    "            xml_names = set()\n",
    "            collect_xml_names(root, xml_names)\n",
    "            numberOfElementsAttributes = len(xml_names)\n",
    "\n",
    "            # then we calculate depth starting from the root\n",
    "            nestingDepth = get_xml_depth(root) if root is not None else 0\n",
    "\n",
    "            # and now we will find the longest string, initialize with empty string\n",
    "            longestString = find_longest_xml_string(root, \"\") if root is not None else \"\"\n",
    "\n",
    "            # done at last! we create the result dictionary in the specified order\n",
    "            return {\n",
    "                \"numberOfElementsAttributes\": numberOfElementsAttributes,\n",
    "                \"nestingDepth\": nestingDepth,\n",
    "                \"longestString\": longestString\n",
    "            }\n",
    "\n",
    "        except requests.exceptions.RequestException as e: # One last time - to help with errors and to make this as strong as schwarzenegger\n",
    "            print(f\"TODO - CHANGE COMMENT: XML Request failed: {e}\") \n",
    "            return {}\n",
    "        except ET.ParseError as e: \n",
    "            print(f\"TODO - CHANGE COMMENT: XML Parsing failed: {e}\") \n",
    "            return {}\n",
    "        except Exception as e: \n",
    "            print(f\"TODO - CHANGE COMMENT: XML Processing failed: {e}\") \n",
    "            return {}\n",
    "\n",
    "    # now to deal with files that arent CSV, JSON or XML!\n",
    "    else:\n",
    "        return {} \n",
    "results1 = describeFile(dataset1)\n",
    "results2 = describeFile(dataset2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e58d31-e57e-4ac6-84d3-48809e6b0d04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f1366f100e7999f4c277405ab2794ad",
     "grade": true,
     "grade_id": "cell-eba7f348525a45dd",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'numberOfNumericColumns': 13, 'numberOfRows': 10000, 'largestFloat': (1, 529)}\n",
      "{'numberOfAttributes': 27, 'nestingDepth': 1, 'longestListLength': 10000}\n"
     ]
    }
   ],
   "source": [
    "assert(len(describeFile(dataset1))) == 3\n",
    "assert(len(describeFile(dataset2))) == 3\n",
    "\n",
    "# Check your output\n",
    "print(describeFile(dataset1))\n",
    "print(describeFile(dataset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3964a3f4-838b-48db-b70a-1aa8dc045be4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f64ab1df5aa90e5d77c647cefa6cfbb1",
     "grade": true,
     "grade_id": "cell-179ad8edcdce23b21d",
     "locked": true,
     "points": 1.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! 😼⚡️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f794d334-83a6-43b4-9a2d-f5c58f8ef2f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c23eb3b1d4f662bd6ddcf8d21e5c2e3",
     "grade": true,
     "grade_id": "cell-17969cee89805c39",
     "locked": true,
     "points": 1.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit this cell or remove it, and you shall perish, meow! 😼⚡️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e1e37-e968-436f-ac91-a22154e6f32b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50254397eddaa1abb3f42df09cf17cd8",
     "grade": false,
     "grade_id": "cell-66aada55df321ea7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Please answer the questions\n",
    "\n",
    "***Final check:***\n",
    "\n",
    "Be sure to cross-check the results by\n",
    "1) manually inspecting your chosen dataset and\n",
    "2) comparing the results for plausibility against the results of your code... \n",
    "\n",
    "Describe your findings and use the following structure for your answer below:\n",
    "\n",
    "*Data set 1*\n",
    "\n",
    "(number and types of items etc., describe your findings)\n",
    "\n",
    "*Data set 2*\n",
    "\n",
    "(number and types of items etc., describe your findings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3bce1c-e495-4733-baa8-ddc2962f077f",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df18cdf1882599d295da8465485a5b1c",
     "grade": true,
     "grade_id": "cell-4709b3248c430b6a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Data set 1\n",
    "\n",
    "This dataset contains 13 numeric columns , therefore it is mainly quantitative. It was artificially cut down to 10000 rows previously and the code was able to detect that. The largest even float that was found is in column 1 in row 529. I manually checked the data to make sure it is the correct output.\n",
    "\n",
    "Data set 2\n",
    "\n",
    "Based on a careful visual inspection, the dataset comprises 27 columns, exhibits a nesting depth of 1, and contains exactly 10,000 artificially generated rows, confirming the initial analysis. Additionally, this manual verification directly addresses the question by ensuring that our code's output is both plausible and accurate. This data was also checked and matches the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806a284-a86d-42c1-add2-48e6472068c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
